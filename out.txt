<root path="~/Desktop/notebooklm/backend">
<file path="./.github/workflows/build-and-push.yaml">
  name: Build and Push Services
  
  on:
    push:
      tags:
        - "v*.*"
  
  jobs:
    # Common setup job to get version
    setup:
      runs-on: ubuntu-latest
      outputs:
        version: ${{ steps.get_version.outputs.version }}
      steps:
        - name: Get version from tag
          id: get_version
          run: echo "version=${GITHUB_REF#refs/tags/v}" >> $GITHUB_OUTPUT
  
    api-service:
      needs: setup
      runs-on: ubuntu-latest
      permissions:
        contents: read
        packages: write
      steps:
        - name: Checkout code
          uses: actions/checkout@v3
        - name: Login to Container Registry
          uses: docker/login-action@v3
          with:
            registry: nvcr.io
            username: ${{ secrets.NVCR_USERNAME }}
            password: ${{ secrets.NVCR_TOKEN }}
        - name: Build and push api-service
          uses: docker/build-push-action@v6
          with:
            context: .
            file: services/APIService/Dockerfile
            push: true
            tags: nvcr.io/pfteb4cqjzrs/playground/api-service:${{ needs.setup.outputs.version }}
  
    agent-service:
      needs: setup
      runs-on: ubuntu-latest
      permissions:
        contents: read
        packages: write
      steps:
        - name: Checkout code
          uses: actions/checkout@v3
        - name: Login to Container Registry
          uses: docker/login-action@v3
          with:
            registry: nvcr.io
            username: ${{ secrets.NVCR_USERNAME }}
            password: ${{ secrets.NVCR_TOKEN }}
        - name: Build and push agent-service
          uses: docker/build-push-action@v6
          with:
            context: .
            file: services/AgentService/Dockerfile
            push: true
            tags: nvcr.io/pfteb4cqjzrs/playground/agent-service:${{ needs.setup.outputs.version }}
  
    pdf-service:
      needs: setup
      runs-on: ubuntu-latest
      permissions:
        contents: read
        packages: write
      steps:
        - name: Checkout code
          uses: actions/checkout@v3
        - name: Login to Container Registry
          uses: docker/login-action@v3
          with:
            registry: nvcr.io
            username: ${{ secrets.NVCR_USERNAME }}
            password: ${{ secrets.NVCR_TOKEN }}
        - name: Build and push pdf-service
          uses: docker/build-push-action@v6
          with:
            context: .
            file: services/PDFService/Dockerfile
            push: true
            tags: nvcr.io/pfteb4cqjzrs/playground/pdf-service:${{ needs.setup.outputs.version }}
  
    tts-service:
      needs: setup
      runs-on: ubuntu-latest
      permissions:
        contents: read
        packages: write
      steps:
        - name: Checkout code
          uses: actions/checkout@v3
        - name: Login to Container Registry
          uses: docker/login-action@v3
          with:
            registry: nvcr.io
            username: ${{ secrets.NVCR_USERNAME }}
            password: ${{ secrets.NVCR_TOKEN }}
        - name: Build and push tts-service
          uses: docker/build-push-action@v6
          with:
            context: .
            file: services/TTSService/Dockerfile
            push: true
            tags: nvcr.io/pfteb4cqjzrs/playground/tts-service:${{ needs.setup.outputs.version }}
</file>
<file path="./.github/workflows/ruff.yaml">
  name: Ruff
  on:
    push:
      branches: [main]
    pull_request:
  jobs:
    ruff:
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v4
        - uses: astral-sh/ruff-action@v1
          with:
            args: "format --check"
        - uses: astral-sh/ruff-action@v1
          with:
            args: "check --diff"
</file>
<file path="./.gitignore">
  .pytest_cache
  __pycache__
  tmp
  .venv
  **/output.mp3
  kompose
  get_helm.sh
  .env
  .ruff_cache
  data/minio
  eval.txt</file>
<file path="./Makefile">
  # Env vars
  include .env
  export
  
  # Version for production deployment
  VERSION := 1.8
  
  # Docker registry and project
  REGISTRY := nvcr.io/pfteb4cqjzrs/playground
  
  # List of services to build
  SERVICES := api-service agent-service pdf-service tts-service
  
  # Required environment variables
  REQUIRED_ENV_VARS := ELEVENLABS_API_KEY NIM_KEY MAX_CONCURRENT_REQUESTS
  
  # Colors for terminal output
  RED := \033[0;31m
  GREEN := \033[0;32m
  NC := \033[0m  # No Color
  
  # Explicitly use bash
  SHELL := /bin/bash
  
  # Check if environment variables are set
  check_env:
  	@for var in $(REQUIRED_ENV_VARS); do \
  		if [ -z "$$(eval echo "\$$$$var")" ]; then \
  			echo "$(RED)Error: $$var is not set$(NC)"; \
  			echo "Please set required environment variables:"; \
  			echo "  export $$var=<value>"; \
  			exit 1; \
  		else \
  			echo "$(GREEN)✓ $$var is set$(NC)"; \
  		fi \
  	done
  
  # Development target
  dev: check_env
  	@if [ ! -d "data/minio" ]; then \
  		echo "$(GREEN)Creating data/minio directory...$(NC)"; \
  		mkdir -p data/minio; \
  	fi
  	docker compose down
  	@echo "$(GREEN)Starting development environment...$(NC)"
  	docker compose -f docker-compose.yaml --env-file .env up --build
  
  # Production target
  prod: check_env
  	@if [ ! -d "data/minio" ]; then \
  		echo "$(GREEN)Creating data/minio directory...$(NC)"; \
  		mkdir -p data/minio; \
  	fi
  	docker compose down
  	@echo "$(GREEN)Starting production environment with version $(VERSION)...$(NC)"
  	VERSION=$(VERSION) docker compose -f docker-compose-remote.yaml --env-file .env up
  
  # Version bump and release target
  version-bump:
  	@echo "Current version: $(VERSION)"
  	@new_version=$$(echo $(VERSION) | awk -F. '{$$NF = $$NF + 1;} 1' | sed 's/ /./g'); \
  	sed -i.bak "s/VERSION := $(VERSION)/VERSION := $$new_version/" Makefile; \
  	rm Makefile.bak; \
  	echo "$(GREEN)Version bumped to: $$new_version$(NC)"; \
  	git add Makefile; \
  	git commit -m "chore: bump version to $$new_version"; \
  	git tag -a "v$$new_version" -m "Release v$$new_version"; \
  	git push origin main; \
  	git push origin "v$$new_version"
  
  # Clean up containers and volumes
  clean:
  	docker compose -f docker-compose.yaml down -v
  
  lint:
  	ruff check
  
  format:
  	ruff format
  
  ruff: lint format
  
  .PHONY: check_env dev clean ruff prod version-bump</file>
<file path="./README.md">
  # Podcast Generation System
  
  This project implements a sophisticated podcast generation system using a combination of microservices and Large Language Models (LLMs). The system is designed to process PDF inputs and generate audio outputs through a series of interconnected services.
  
  ## Microservice Workflow
  
  The microservice architecture is designed to process PDFs and generate audio files in a sequential manner. Here's an overview of the workflow:
  
  ![Microservice Workflow](static/Microservice_flow.svg)
  
  1. **User Uploads PDF**: The process begins when a user uploads a PDF file to the system.
  2. **Validate PDF File Type**: The system checks if the uploaded file is a valid PDF.
  3. **Convert PDF to Markdown Service**: This service extracts the content from the PDF and converts it into a markdown format.
  4. **Process Markdown with Agent Service**: An intelligent agent processes the markdown content, potentially enriching or structuring it further.
  5. **Generate TTS Service**: The processed content is then converted into speech using a Text-to-Speech (TTS) service.
  6. **Return Audio File to User**: Finally, the system provides the generated audio file back to the user.
  
  ## LLM Flow
  
  The LLM (Large Language Model) flow is a crucial part of the content generation process. It takes the input text and transforms it into a structured dialogue suitable for a podcast. Here's a detailed explanation of each step:
  
  ![LLM Flow](static/LLM_flow.svg)
  
  1. **Input Text**: The initial input provided for generating the podcast content.
  2. **Raw Outline Generation**: Creates a basic structure from the input text, outlining the key points.
  3. **Structured Outline Conversion**: Converts the raw outline into a structured format that adheres to a specific schema for downstream processing.
  4. **Segment Transcript Generation**: Generates detailed transcripts for each segment based on the structured outline, focusing on specific topics for in-depth coverage.
  5. **Transcript Optimization**: Combines and refines individual segment transcripts to ensure a smooth and coherent flow across the entire content.
  6. **Podcast Dialogue Creation**: Transforms the optimized transcript into a dynamic dialogue, incorporating conversational elements.
  7. **Dialogue Revision**: Reviews and enriches the dialogue, adding any missing details or exchanges for completeness.
  8. **Structured Dialogue Conversion**: Converts the final dialogue into a structured JSON format for further use.
  9. **Output JSON**: The completed structured dialogue, ready for various applications.
  
  ## Key Features
  
  - PDF to Audio conversion
  - Intelligent content processing using LLMs
  - Microservice architecture for scalability and maintainability
  - Dynamic and parallel execution of services
  - Structured dialogue generation for podcast-like content
  
  ## Development
  
  In order to run this project locally, you can simply run the following command:
  
  ```bash
  make dev
  ```
  This will connect to the production LLM and PDF endpoint. The rest of the stack will run locally. From there, simply run 
  
  ```bash
  python test.py
  ```
  
  ## CI/CD
  
  We use GitHub Actions to run the CI/CD pipeline. We use `ruff` for linting and formatting. To run locally, simply run `make ruff`.
</file>
<file path="./docker-compose-remote.yaml">
  services:
    redis:
      image: redis:latest
      ports:
        - "6379:6379"
      volumes:
        - redis_data:/data
      command: redis-server --appendonly no
      networks:
        - app-network
    minio:
      image: minio/minio:latest
      ports:
        - "9000:9000"
        - "9001:9001"
      environment:
        - MINIO_ROOT_USER=minioadmin
        - MINIO_ROOT_PASSWORD=minioadmin
      volumes:
        - ./data/minio:/data
      command: minio server /data --console-address ":9001"
      networks:
        - app-network
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
        interval: 30s
        timeout: 20s
        retries: 5
    api-service:
      image: nvcr.io/pfteb4cqjzrs/playground/api-service:${VERSION:-1.1}
      ports:
        - "8002:8002"
      environment:
        - PDF_SERVICE_URL=http://pdf-service:8003
        - AGENT_SERVICE_URL=http://agent-service:8964
        - TTS_SERVICE_URL=http://tts-service:8889
        - REDIS_URL=redis://redis:6379
      depends_on:
        - redis
        - pdf-service
        - agent-service
        - tts-service
      networks:
        - app-network
    agent-service:
      image: nvcr.io/pfteb4cqjzrs/playground/agent-service:${VERSION:-1.1}
      ports:
        - "8964:8964"
      environment:
        - NIM_KEY=${NIM_KEY}
        - REDIS_URL=redis://redis:6379
        - MODEL_CONFIG_PATH=/app/config/models.json
      volumes:
        - ./models.json:/app/config/models.json
      depends_on:
        - redis
      networks:
        - app-network
    pdf-service:
      image: nvcr.io/pfteb4cqjzrs/playground/pdf-service:${VERSION:-1.1}
      ports:
        - "8003:8003"
      environment:
        - REDIS_URL=redis://redis:6379
        - MODEL_API_URL=${MODEL_API_URL:-https://pdf-gyrdps568.brevlab.com}
      depends_on:
        - redis
      networks:
        - app-network
    tts-service:
      image: nvcr.io/pfteb4cqjzrs/playground/tts-service:${VERSION:-1.1}
      ports:
        - "8889:8889"
      environment:
        - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
        - ELEVENLABS_API_KEY=${ELEVENLABS_API_KEY}
        - REDIS_URL=redis://redis:6379
      depends_on:
        - redis
      networks:
        - app-network
  volumes:
    redis_data:
  networks:
    app-network:
      driver: bridge
</file>
<file path="./docker-compose.yaml">
  services:
    redis:
      image: redis:latest
      ports:
        - "6379:6379"
      volumes:
        - redis_data:/data
      command: redis-server --appendonly no
      networks:
        - app-network
  
    minio:
      image: minio/minio:latest
      ports:
        - "9000:9000"
        - "9001:9001"
      environment:
        - MINIO_ROOT_USER=minioadmin
        - MINIO_ROOT_PASSWORD=minioadmin
      volumes:
        - ./data/minio:/data
      command: minio server /data --console-address ":9001"
      networks:
        - app-network
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
        interval: 30s
        timeout: 20s
        retries: 5
  
    api-service:
      build:
        context: .
        dockerfile: services/APIService/Dockerfile
      ports:
        - "8002:8002"
      environment:
        - PDF_SERVICE_URL=http://pdf-service:8003
        - AGENT_SERVICE_URL=http://agent-service:8964
        - TTS_SERVICE_URL=http://tts-service:8889
        - REDIS_URL=redis://redis:6379
      depends_on:
        - redis
        - pdf-service
        - agent-service
        - tts-service
      networks:
        - app-network
  
    agent-service:
      build:
        context: .
        dockerfile: services/AgentService/Dockerfile
      ports:
        - "8964:8964"
      environment:
        - NIM_KEY=${NIM_KEY}
        - REDIS_URL=redis://redis:6379
        - MODEL_CONFIG_PATH=/app/config/models.json
      volumes:
        - ./models.json:/app/config/models.json
      depends_on:
        - redis
      networks:
        - app-network
  
    pdf-service:
      build:
        context: .
        dockerfile: services/PDFService/Dockerfile
      ports:
        - "8003:8003"
      environment:
        - REDIS_URL=redis://redis:6379
        - MODEL_API_URL=${MODEL_API_URL:-https://pdf-gyrdps568.brevlab.com}
      depends_on:
        - redis
      networks:
        - app-network
  
    tts-service:
      build:
        context: .
        dockerfile: services/TTSService/Dockerfile
      ports:
        - "8889:8889"
      environment:
        - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
        - ELEVENLABS_API_KEY=${ELEVENLABS_API_KEY}
        - REDIS_URL=redis://redis:6379
      depends_on:
        - redis
      networks:
        - app-network
  
  volumes:
    redis_data:
  
  networks:
    app-network:
      driver: bridge
</file>
<file path="./out.txt">
  <root path="~/Desktop/notebooklm/backend">
  <file path="./.github/workflows/build-and-push.yaml">
    name: Build and Push Services
    
    on:
      push:
        tags:
          - "v*.*"
    
    jobs:
      # Common setup job to get version
      setup:
        runs-on: ubuntu-latest
        outputs:
          version: ${{ steps.get_version.outputs.version }}
        steps:
          - name: Get version from tag
            id: get_version
            run: echo "version=${GITHUB_REF#refs/tags/v}" >> $GITHUB_OUTPUT
    
      api-service:
        needs: setup
        runs-on: ubuntu-latest
        permissions:
          contents: read
          packages: write
        steps:
          - name: Checkout code
            uses: actions/checkout@v3
          - name: Login to Container Registry
            uses: docker/login-action@v3
            with:
              registry: nvcr.io
              username: ${{ secrets.NVCR_USERNAME }}
              password: ${{ secrets.NVCR_TOKEN }}
          - name: Build and push api-service
            uses: docker/build-push-action@v6
            with:
              context: .
              file: services/APIService/Dockerfile
              push: true
              tags: nvcr.io/pfteb4cqjzrs/playground/api-service:${{ needs.setup.outputs.version }}
    
      agent-service:
        needs: setup
        runs-on: ubuntu-latest
        permissions:
          contents: read
          packages: write
        steps:
          - name: Checkout code
            uses: actions/checkout@v3
          - name: Login to Container Registry
            uses: docker/login-action@v3
            with:
              registry: nvcr.io
              username: ${{ secrets.NVCR_USERNAME }}
              password: ${{ secrets.NVCR_TOKEN }}
          - name: Build and push agent-service
            uses: docker/build-push-action@v6
            with:
              context: .
              file: services/AgentService/Dockerfile
              push: true
              tags: nvcr.io/pfteb4cqjzrs/playground/agent-service:${{ needs.setup.outputs.version }}
    
      pdf-service:
        needs: setup
        runs-on: ubuntu-latest
        permissions:
          contents: read
          packages: write
        steps:
          - name: Checkout code
            uses: actions/checkout@v3
          - name: Login to Container Registry
            uses: docker/login-action@v3
            with:
              registry: nvcr.io
              username: ${{ secrets.NVCR_USERNAME }}
              password: ${{ secrets.NVCR_TOKEN }}
          - name: Build and push pdf-service
            uses: docker/build-push-action@v6
            with:
              context: .
              file: services/PDFService/Dockerfile
              push: true
              tags: nvcr.io/pfteb4cqjzrs/playground/pdf-service:${{ needs.setup.outputs.version }}
    
      tts-service:
        needs: setup
        runs-on: ubuntu-latest
        permissions:
          contents: read
          packages: write
        steps:
          - name: Checkout code
            uses: actions/checkout@v3
          - name: Login to Container Registry
            uses: docker/login-action@v3
            with:
              registry: nvcr.io
              username: ${{ secrets.NVCR_USERNAME }}
              password: ${{ secrets.NVCR_TOKEN }}
          - name: Build and push tts-service
            uses: docker/build-push-action@v6
            with:
              context: .
              file: services/TTSService/Dockerfile
              push: true
              tags: nvcr.io/pfteb4cqjzrs/playground/tts-service:${{ needs.setup.outputs.version }}
  </file>
  <file path="./.github/workflows/ruff.yaml">
    name: Ruff
    on:
      push:
        branches: [main]
      pull_request:
    jobs:
      ruff:
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v4
          - uses: astral-sh/ruff-action@v1
            with:
              args: "format --check"
          - uses: astral-sh/ruff-action@v1
            with:
              args: "check --diff"
  </file>
  <file path="./.gitignore">
    .pytest_cache
    __pycache__
    tmp
    .venv
    **/output.mp3
    kompose
    get_helm.sh
    .env
    .ruff_cache
    data/minio
    eval.txt</file>
  <file path="./Makefile">
    # Env vars
    include .env
    export
    
    # Version for production deployment
    VERSION := 1.8
    
    # Docker registry and project
    REGISTRY := nvcr.io/pfteb4cqjzrs/playground
    
    # List of services to build
    SERVICES := api-service agent-service pdf-service tts-service
    
    # Required environment variables
    REQUIRED_ENV_VARS := ELEVENLABS_API_KEY NIM_KEY MAX_CONCURRENT_REQUESTS
    
    # Colors for terminal output
    RED := \033[0;31m
    GREEN := \033[0;32m
    NC := \033[0m  # No Color
    
    # Explicitly use bash
    SHELL := /bin/bash
    
    # Check if environment variables are set
    check_env:
    	@for var in $(REQUIRED_ENV_VARS); do \
    		if [ -z "$$(eval echo "\$$$$var")" ]; then \
    			echo "$(RED)Error: $$var is not set$(NC)"; \
    			echo "Please set required environment variables:"; \
    			echo "  export $$var=<value>"; \
    			exit 1; \
    		else \
    			echo "$(GREEN)✓ $$var is set$(NC)"; \
    		fi \
    	done
    
    # Development target
    dev: check_env
    	@if [ ! -d "data/minio" ]; then \
    		echo "$(GREEN)Creating data/minio directory...$(NC)"; \
    		mkdir -p data/minio; \
    	fi
    	docker compose down
    	@echo "$(GREEN)Starting development environment...$(NC)"
    	docker compose -f docker-compose.yaml --env-file .env up --build
    
    # Production target
    prod: check_env
    	@if [ ! -d "data/minio" ]; then \
    		echo "$(GREEN)Creating data/minio directory...$(NC)"; \
    		mkdir -p data/minio; \
    	fi
    	docker compose down
    	@echo "$(GREEN)Starting production environment with version $(VERSION)...$(NC)"
    	VERSION=$(VERSION) docker compose -f docker-compose-remote.yaml --env-file .env up
    
    # Version bump and release target
    version-bump:
    	@echo "Current version: $(VERSION)"
    	@new_version=$$(echo $(VERSION) | awk -F. '{$$NF = $$NF + 1;} 1' | sed 's/ /./g'); \
    	sed -i.bak "s/VERSION := $(VERSION)/VERSION := $$new_version/" Makefile; \
    	rm Makefile.bak; \
    	echo "$(GREEN)Version bumped to: $$new_version$(NC)"; \
    	git add Makefile; \
    	git commit -m "chore: bump version to $$new_version"; \
    	git tag -a "v$$new_version" -m "Release v$$new_version"; \
    	git push origin main; \
    	git push origin "v$$new_version"
    
    # Clean up containers and volumes
    clean:
    	docker compose -f docker-compose.yaml down -v
    
    lint:
    	ruff check
    
    format:
    	ruff format
    
    ruff: lint format
    
    .PHONY: check_env dev clean ruff prod version-bump</file>
  <file path="./README.md">
    # Podcast Generation System
    
    This project implements a sophisticated podcast generation system using a combination of microservices and Large Language Models (LLMs). The system is designed to process PDF inputs and generate audio outputs through a series of interconnected services.
    
    ## Microservice Workflow
    
    The microservice architecture is designed to process PDFs and generate audio files in a sequential manner. Here's an overview of the workflow:
    
    ![Microservice Workflow](static/Microservice_flow.svg)
    
    1. **User Uploads PDF**: The process begins when a user uploads a PDF file to the system.
    2. **Validate PDF File Type**: The system checks if the uploaded file is a valid PDF.
    3. **Convert PDF to Markdown Service**: This service extracts the content from the PDF and converts it into a markdown format.
    4. **Process Markdown with Agent Service**: An intelligent agent processes the markdown content, potentially enriching or structuring it further.
    5. **Generate TTS Service**: The processed content is then converted into speech using a Text-to-Speech (TTS) service.
    6. **Return Audio File to User**: Finally, the system provides the generated audio file back to the user.
    
    ## LLM Flow
    
    The LLM (Large Language Model) flow is a crucial part of the content generation process. It takes the input text and transforms it into a structured dialogue suitable for a podcast. Here's a detailed explanation of each step:
    
    ![LLM Flow](static/LLM_flow.svg)
    
    1. **Input Text**: The initial input provided for generating the podcast content.
    2. **Raw Outline Generation**: Creates a basic structure from the input text, outlining the key points.
    3. **Structured Outline Conversion**: Converts the raw outline into a structured format that adheres to a specific schema for downstream processing.
    4. **Segment Transcript Generation**: Generates detailed transcripts for each segment based on the structured outline, focusing on specific topics for in-depth coverage.
    5. **Transcript Optimization**: Combines and refines individual segment transcripts to ensure a smooth and coherent flow across the entire content.
    6. **Podcast Dialogue Creation**: Transforms the optimized transcript into a dynamic dialogue, incorporating conversational elements.
    7. **Dialogue Revision**: Reviews and enriches the dialogue, adding any missing details or exchanges for completeness.
    8. **Structured Dialogue Conversion**: Converts the final dialogue into a structured JSON format for further use.
    9. **Output JSON**: The completed structured dialogue, ready for various applications.
    
    ## Key Features
    
    - PDF to Audio conversion
    - Intelligent content processing using LLMs
    - Microservice architecture for scalability and maintainability
    - Dynamic and parallel execution of services
    - Structured dialogue generation for podcast-like content
    
    ## Development
    
    In order to run this project locally, you can simply run the following command:
    
    ```bash
    make dev
    ```
    This will connect to the production LLM and PDF endpoint. The rest of the stack will run locally. From there, simply run 
    
    ```bash
    python test.py
    ```
    
    ## CI/CD
    
    We use GitHub Actions to run the CI/CD pipeline. We use `ruff` for linting and formatting. To run locally, simply run `make ruff`.
  </file>
  <file path="./docker-compose-remote.yaml">
    services:
      redis:
        image: redis:latest
        ports:
          - "6379:6379"
        volumes:
          - redis_data:/data
        command: redis-server --appendonly no
        networks:
          - app-network
      minio:
        image: minio/minio:latest
        ports:
          - "9000:9000"
          - "9001:9001"
        environment:
          - MINIO_ROOT_USER=minioadmin
          - MINIO_ROOT_PASSWORD=minioadmin
        volumes:
          - ./data/minio:/data
        command: minio server /data --console-address ":9001"
        networks:
          - app-network
        healthcheck:
          test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
          interval: 30s
          timeout: 20s
          retries: 5
      api-service:
        image: nvcr.io/pfteb4cqjzrs/playground/api-service:${VERSION:-1.1}
        ports:
          - "8002:8002"
        environment:
          - PDF_SERVICE_URL=http://pdf-service:8003
          - AGENT_SERVICE_URL=http://agent-service:8964
          - TTS_SERVICE_URL=http://tts-service:8889
          - REDIS_URL=redis://redis:6379
        depends_on:
          - redis
          - pdf-service
          - agent-service
          - tts-service
        networks:
          - app-network
      agent-service:
        image: nvcr.io/pfteb4cqjzrs/playground/agent-service:${VERSION:-1.1}
        ports:
          - "8964:8964"
        environment:
          - NIM_KEY=${NIM_KEY}
          - REDIS_URL=redis://redis:6379
          - MODEL_CONFIG_PATH=/app/config/models.json
        volumes:
          - ./models.json:/app/config/models.json
        depends_on:
          - redis
        networks:
          - app-network
      pdf-service:
        image: nvcr.io/pfteb4cqjzrs/playground/pdf-service:${VERSION:-1.1}
        ports:
          - "8003:8003"
        environment:
          - REDIS_URL=redis://redis:6379
          - MODEL_API_URL=${MODEL_API_URL:-https://pdf-gyrdps568.brevlab.com}
        depends_on:
          - redis
        networks:
          - app-network
      tts-service:
        image: nvcr.io/pfteb4cqjzrs/playground/tts-service:${VERSION:-1.1}
        ports:
          - "8889:8889"
        environment:
          - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
          - ELEVENLABS_API_KEY=${ELEVENLABS_API_KEY}
          - REDIS_URL=redis://redis:6379
        depends_on:
          - redis
        networks:
          - app-network
    volumes:
      redis_data:
    networks:
      app-network:
        driver: bridge
  </file>
  <file path="./docker-compose.yaml">
    services:
      redis:
        image: redis:latest
        ports:
          - "6379:6379"
        volumes:
          - redis_data:/data
        command: redis-server --appendonly no
        networks:
          - app-network
    
      minio:
        image: minio/minio:latest
        ports:
          - "9000:9000"
          - "9001:9001"
        environment:
          - MINIO_ROOT_USER=minioadmin
          - MINIO_ROOT_PASSWORD=minioadmin
        volumes:
          - ./data/minio:/data
        command: minio server /data --console-address ":9001"
        networks:
          - app-network
        healthcheck:
          test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
          interval: 30s
          timeout: 20s
          retries: 5
    
      api-service:
        build:
          context: .
          dockerfile: services/APIService/Dockerfile
        ports:
          - "8002:8002"
        environment:
          - PDF_SERVICE_URL=http://pdf-service:8003
          - AGENT_SERVICE_URL=http://agent-service:8964
          - TTS_SERVICE_URL=http://tts-service:8889
          - REDIS_URL=redis://redis:6379
        depends_on:
          - redis
          - pdf-service
          - agent-service
          - tts-service
        networks:
          - app-network
    
      agent-service:
        build:
          context: .
          dockerfile: services/AgentService/Dockerfile
        ports:
          - "8964:8964"
        environment:
          - NIM_KEY=${NIM_KEY}
          - REDIS_URL=redis://redis:6379
          - MODEL_CONFIG_PATH=/app/config/models.json
        volumes:
          - ./models.json:/app/config/models.json
        depends_on:
          - redis
        networks:
          - app-network
    
      pdf-service:
        build:
          context: .
          dockerfile: services/PDFService/Dockerfile
        ports:
          - "8003:8003"
        environment:
          - REDIS_URL=redis://redis:6379
          - MODEL_API_URL=${MODEL_API_URL:-https://pdf-gyrdps568.brevlab.com}
        depends_on:
          - redis
        networks:
          - app-network
    
      tts-service:
        build:
          context: .
          dockerfile: services/TTSService/Dockerfile
        ports:
          - "8889:8889"
        environment:
          - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
          - ELEVENLABS_API_KEY=${ELEVENLABS_API_KEY}
          - REDIS_URL=redis://redis:6379
        depends_on:
          - redis
        networks:
          - app-network
    
    volumes:
      redis_data:
    
    networks:
      app-network:
        driver: bridge
  </file>
  <file path="./out.txt">
</file>
<file path="./ruff.toml">
  exclude = [
      "services/AgentService/flexagent"
  ]
</file>
<file path="./services/APIService/Dockerfile">
  FROM python:3.9-slim
  
  WORKDIR /app
  
  # Install system dependencies if needed
  RUN apt-get update && apt-get install -y --no-install-recommends \
      build-essential \
      && rm -rf /var/lib/apt/lists/*
  
  # Install Python packages
  RUN pip install --no-cache-dir \
      fastapi \
      uvicorn[standard] \
      python-multipart \
      requests \
      pydantic \
      redis \
      websockets \
      asyncio \
      minio
  
  # Copy shared package first
  COPY shared /shared
  RUN pip install /shared
  
  # Copy service files
  COPY services/APIService/main.py ./
  
  # Use uvicorn with websocket support enabled
  CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8002", "--ws", "auto"]</file>
<file path="./services/APIService/main.py">
  from fastapi import (
      HTTPException,
      FastAPI,
      File,
      UploadFile,
      Form,
      BackgroundTasks,
      Response,
      WebSocket,
      WebSocketDisconnect,
  )
  from shared.shared_types import (
      ServiceType,
      JobStatus,
      StatusUpdate,
      TranscriptionParams,
      SavedPodcast,
      SavedPodcastWithAudio,
      Conversation,
      PromptTracker,
  )
  from shared.connection import ConnectionManager
  from shared.storage import StorageManager
  from fastapi.middleware.cors import CORSMiddleware
  from pydantic import ValidationError
  import redis
  import requests
  import json
  import os
  import logging
  import time
  import asyncio
  from typing import Dict, List
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  app = FastAPI(debug=True)
  redis_client = redis.Redis.from_url(
      os.getenv("REDIS_URL", "redis://redis:6379"), decode_responses=False
  )
  
  # Initialize the connection manager
  manager = ConnectionManager(redis_client=redis_client)
  storage_manager = StorageManager()
  
  # Service URLs
  PDF_SERVICE_URL = os.getenv("PDF_SERVICE_URL", "http://localhost:8003")
  AGENT_SERVICE_URL = os.getenv("AGENT_SERVICE_URL", "http://localhost:8964")
  TTS_SERVICE_URL = os.getenv("TTS_SERVICE_URL", "http://localhost:8889")
  
  # MP3 Cache TTL
  MP3_CACHE_TTL = 60 * 60 * 4  # 4 hours
  
  # CORS setup
  CORS_ORIGINS = os.getenv("CORS_ORIGINS", "http://localhost:3000")
  allowed_origins = [origin.strip() for origin in CORS_ORIGINS.split(",")]
  logger.info(f"Configuring CORS with allowed origins: {allowed_origins}")
  app.add_middleware(
      CORSMiddleware,
      allow_origins=allowed_origins,
      allow_credentials=True,
      allow_methods=["*"],
      allow_headers=["*"],
      expose_headers=["Content-Disposition"],
      max_age=3600,
  )
  
  
  @app.websocket("/ws/status/{job_id}")
  async def websocket_endpoint(websocket: WebSocket, job_id: str):
      try:
          # Accept the WebSocket connection
          await manager.connect(websocket, job_id)
  
          # Send initial status for all services
          for service in ServiceType:
              status_data = redis_client.hgetall(f"status:{job_id}:{service}")
              if status_data:
                  status_msg = {
                      "service": service.value,
                      "status": status_data.get(b"status", b"").decode(),
                      "message": status_data.get(b"message", b"").decode(),
                  }
                  await websocket.send_json(status_msg)
                  logger.info(f"Sent initial status for {job_id} {service}: {status_msg}")
  
          # Keep connection alive and handle client messages
          while True:
              try:
                  # Wait for client messages (ping/pong handled automatically by FastAPI)
                  data = await websocket.receive_text()
                  if data == "ping":
                      await websocket.send_text("pong")
              except WebSocketDisconnect:
                  break
  
              await asyncio.sleep(0.1)
  
      except Exception as e:
          logger.error(f"WebSocket error for job {job_id}: {e}")
      finally:
          manager.disconnect(websocket, job_id)
  
  
  def process_pdf_task(
      job_id: str, file_content: bytes, transcription_params: TranscriptionParams
  ):
      try:
          pubsub = redis_client.pubsub()
          pubsub.subscribe("status_updates:all")
  
          # Start PDF Service
          requests.post(
              f"{PDF_SERVICE_URL}/convert",
              files={"file": ("file.pdf", file_content, "application/pdf")},
              data={"job_id": job_id},
          )
  
          storage_manager.store_file(
              job_id,
              file_content,
              f"{job_id}.pdf",
              "application/pdf",
              transcription_params,
          )
          logger.info(f"Stored original PDF for {job_id} in storage")
  
          # Monitor services
          current_service = ServiceType.PDF
          while True:
              message = pubsub.get_message()
              if message and message["type"] == "message":
                  update = StatusUpdate.model_validate_json(message["data"].decode())
  
                  if update.job_id == job_id:
                      logger.info(f"Received update for job {job_id}: {update}")
  
                      if update.status == JobStatus.FAILED:
                          raise Exception(f"{update.service}: {update.message}")
  
                      if update.status == JobStatus.COMPLETED:
                          if current_service == ServiceType.PDF:
                              # Start Agent Service
                              markdown_content = requests.get(
                                  f"{PDF_SERVICE_URL}/output/{job_id}"
                              ).text
                              requests.post(
                                  f"{AGENT_SERVICE_URL}/transcribe",
                                  json={
                                      "markdown": markdown_content,
                                      "job_id": job_id,
                                      **transcription_params.model_dump(),
                                  },
                              )
                              current_service = ServiceType.AGENT
  
                          elif current_service == ServiceType.AGENT:
                              # Start TTS Service
                              agent_result = requests.get(
                                  f"{AGENT_SERVICE_URL}/output/{job_id}"
                              ).json()
  
                              # Store script result in minio
                              storage_manager.store_file(
                                  job_id,
                                  json.dumps(agent_result).encode(),
                                  f"{job_id}_agent_result.json",
                                  "application/json",
                                  transcription_params,
                              )
                              logger.info(
                                  f"Stored agent result for {job_id} in minio, size: {len(json.dumps(agent_result).encode())} bytes"
                              )
  
                              requests.post(
                                  f"{TTS_SERVICE_URL}/generate_tts",
                                  json={
                                      "dialogue": agent_result["dialogue"],
                                      "job_id": job_id,
                                      "voice_mapping": transcription_params.voice_mapping,  # Forward the voice mapping
                                  },
                              )
                              current_service = ServiceType.TTS
  
                          elif current_service == ServiceType.TTS:
                              # Get final output and store it
                              logger.info(
                                  f"TTS completed for {job_id}, fetching and storing result"
                              )
                              audio_content = requests.get(
                                  f"{TTS_SERVICE_URL}/output/{job_id}"
                              ).content
  
                              # Store both the content and the ready flag
                              redis_client.set(
                                  f"result:{job_id}:{ServiceType.TTS}",
                                  audio_content,
                                  ex=MP3_CACHE_TTL,
                              )
                              redis_client.set(
                                  f"final_status:{job_id}", "ready", ex=MP3_CACHE_TTL
                              )
  
                              # Store in DB
                              storage_manager.store_audio(
                                  job_id,
                                  audio_content,
                                  f"{job_id}.mp3",
                                  transcription_params,
                              )
  
                              logger.info(
                                  f"Stored TTS result for {job_id}, size: {len(audio_content)} bytes, with TTL: {MP3_CACHE_TTL} seconds"
                              )
                              return audio_content
  
              time.sleep(0.01)
  
      except Exception as e:
          logger.error(f"Job {job_id} failed: {str(e)}")
          raise
  
  
  @app.post("/process_pdf", status_code=202)
  async def process_pdf(
      background_tasks: BackgroundTasks,
      file: UploadFile = File(...),
      transcription_params: str = Form(...),
  ):
      if file.content_type != "application/pdf":
          raise HTTPException(status_code=400, detail="Only PDF files are allowed")
  
      try:
          params_dict = json.loads(transcription_params)
          params = TranscriptionParams.model_validate(params_dict)
      except json.JSONDecodeError:
          raise HTTPException(
              status_code=400, detail="Invalid JSON in transcription_params"
          )
      except ValidationError as e:
          raise HTTPException(status_code=400, detail=str(e))
  
      # Create job
      job_id = str(int(time.time()))
  
      # Start processing
      file_content = await file.read()
      background_tasks.add_task(process_pdf_task, job_id, file_content, params)
  
      return {"job_id": job_id}
  
  
  @app.get("/status/{job_id}")
  async def get_status(job_id: str):
      """Get aggregated status from all services"""
      statuses = {}
      for service in ServiceType:
          status = redis_client.hgetall(f"status:{job_id}:{service}")
          if status:
              # Decode the bytes to strings
              statuses[service] = {k.decode(): v.decode() for k, v in status.items()}
  
      if not statuses:
          raise HTTPException(status_code=404, detail="Job not found")
  
      return statuses
  
  
  # This needs to also interact with our db as well. Check cache first if job running. If nothing there, check db
  @app.get("/output/{job_id}")
  async def get_output(job_id: str):
      """Get the final TTS output"""
      # First check if the final result is ready
      is_ready = redis_client.get(f"final_status:{job_id}")
      if not is_ready:
          # Check if TTS service reports completion
          tts_status = redis_client.hgetall(f"status:{job_id}:{ServiceType.TTS}")
          if not tts_status or tts_status.get(b"status", b"").decode() != "completed":
              raise HTTPException(status_code=404, detail="Result not found")
  
          # If TTS reports complete but result not ready, it's still being fetched
          raise HTTPException(
              status_code=425,  # Too Early
              detail="Result is being prepared",
          )
  
      result = redis_client.get(f"result:{job_id}:{ServiceType.TTS}")
      if not result:
          logger.info(f"Final result not found in cache for {job_id}. Checking DB...")
          result = storage_manager.get_podcast_audio(job_id)
          if not result:
              raise HTTPException(status_code=404, detail="Result not found")
  
      return Response(
          content=result,
          media_type="audio/mpeg",
          headers={"Content-Disposition": f"attachment; filename={job_id}.mp3"},
      )
  
  
  @app.post("/cleanup")
  async def cleanup_jobs():
      """Clean up old jobs across all services"""
      removed = 0
      for service in ServiceType:
          pattern = f"status:*:{service}"
          for key in redis_client.scan_iter(match=pattern):
              job_id = key.split(b":")[1].decode()  # Handle bytes key
              redis_client.delete(key)
              redis_client.delete(f"result:{job_id}:{service}")
              removed += 1
      return {"message": f"Removed {removed} old jobs"}
  
  
  @app.get("/saved_podcasts", response_model=Dict[str, List[SavedPodcast]])
  async def get_saved_podcasts():
      """Get a list of all saved podcasts from storage with their audio data"""
      try:
          saved_files = storage_manager.list_files_metadata()
          return {
              "podcasts": [
                  SavedPodcast(
                      job_id=file["job_id"],
                      filename=file["filename"],
                      created_at=file["created_at"],
                      size=file["size"],
                      transcription_params=file.get("transcription_params", {}),
                  )
                  for file in saved_files
              ]
          }
      except Exception as e:
          logger.error(f"Failed to list saved podcasts: {str(e)}")
          raise HTTPException(
              status_code=500, detail=f"Failed to retrieve saved podcasts: {str(e)}"
          )
  
  
  @app.get("/saved_podcast/{job_id}/metadata", response_model=SavedPodcast)
  async def get_saved_podcast_metadata(job_id: str):
      """Get a specific saved podcast metadata without audio data"""
      try:
          saved_files = storage_manager.list_files_metadata()
          podcast_metadata = next(
              (file for file in saved_files if file["job_id"] == job_id), None
          )
          if not podcast_metadata:
              raise HTTPException(
                  status_code=404, detail=f"Podcast with job_id {job_id} not found"
              )
          return SavedPodcast(
              job_id=podcast_metadata["job_id"],
              filename=podcast_metadata["filename"],
              created_at=podcast_metadata["created_at"],
              size=podcast_metadata["size"],
              transcription_params=podcast_metadata.get("transcription_params", {}),
          )
      except Exception as e:
          logger.error(f"Failed to get podcast metadata {job_id}: {str(e)}")
          raise HTTPException(
              status_code=500, detail=f"Failed to retrieve podcast metadata: {str(e)}"
          )
  
  
  @app.get("/saved_podcast/{job_id}/audio", response_model=SavedPodcastWithAudio)
  async def get_saved_podcast(job_id: str):
      """Get a specific saved podcast with its audio data"""
      try:
          # Get metadata first
          saved_files = storage_manager.list_files_metadata()
          podcast_metadata = next(
              (file for file in saved_files if file["job_id"] == job_id), None
          )
  
          if not podcast_metadata:
              raise HTTPException(
                  status_code=404, detail=f"Podcast with job_id {job_id} not found"
              )
  
          # Get audio data
          audio_data = storage_manager.get_podcast_audio(job_id)
          if not audio_data:
              raise HTTPException(
                  status_code=404, detail=f"Audio data for podcast {job_id} not found"
              )
  
          return SavedPodcastWithAudio(
              job_id=podcast_metadata["job_id"],
              filename=podcast_metadata["filename"],
              created_at=podcast_metadata["created_at"],
              size=podcast_metadata["size"],
              transcription_params=podcast_metadata.get("transcription_params", {}),
              audio_data=audio_data,
          )
  
      except HTTPException:
          raise
      except Exception as e:
          logger.error(f"Failed to get podcast {job_id}: {str(e)}")
          raise HTTPException(
              status_code=500, detail=f"Failed to retrieve podcast: {str(e)}"
          )
  
  
  @app.get("/saved_podcast/{job_id}/transcript", response_model=Conversation)
  async def get_saved_podcast_transcript(job_id: str):
      """Get a specific saved podcast transcript"""
      try:
          filename = f"{job_id}_agent_result.json"
          raw_data = storage_manager.get_file(job_id, filename)
  
          if not raw_data:
              raise HTTPException(
                  status_code=404, detail=f"Transcript for {job_id} not found"
              )
  
          agent_result = json.loads(raw_data)
          return Conversation.model_validate(agent_result)
  
      except ValidationError as e:
          logger.error(f"Validation error for transcript {job_id}: {str(e)}")
          raise HTTPException(
              status_code=500, detail=f"Invalid transcript format: {str(e)}"
          )
      except Exception as e:
          logger.error(f"Failed to get transcript for {job_id}: {str(e)}")
          raise HTTPException(
              status_code=500, detail=f"Failed to retrieve transcript: {str(e)}"
          )
  
  
  @app.get("/saved_podcast/{job_id}/history")
  async def get_saved_podcast_agent_workflow(job_id: str):
      """Get a specific saved podcast agent workflow"""
      try:
          filename = f"{job_id}_prompt_tracker.json"
          raw_data = storage_manager.get_file(job_id, filename)
  
          if not raw_data:
              raise HTTPException(
                  status_code=404, detail=f"History for {job_id} not found"
              )
  
          return PromptTracker.model_validate_json(raw_data)
  
      except Exception as e:
          logger.error(f"Failed to get history for {job_id}: {str(e)}")
          raise HTTPException(
              status_code=500, detail=f"Failed to retrieve history: {str(e)}"
          )
  
  
  @app.get("/saved_podcast/{job_id}/pdf")
  async def get_saved_podcast_pdf(job_id: str):
      """Get the original PDF file for a specific podcast"""
      try:
          pdf_data = storage_manager.get_file(job_id, f"{job_id}.pdf")
  
          if not pdf_data:
              raise HTTPException(
                  status_code=404, detail=f"PDF for podcast {job_id} not found"
              )
  
          return Response(
              content=pdf_data,
              media_type="application/pdf",
              headers={"Content-Disposition": f"attachment; filename={job_id}.pdf"},
          )
  
      except Exception as e:
          logger.error(f"Failed to get PDF for {job_id}: {str(e)}")
          raise HTTPException(status_code=500, detail=f"Failed to retrieve PDF: {str(e)}")
  
  
  @app.delete("/saved_podcast/{job_id}")
  async def delete_saved_podcast(job_id: str):
      """Delete a specific saved podcast and all its associated files"""
      try:
          saved_files = storage_manager.list_files_metadata()
          podcast_metadata = next(
              (file for file in saved_files if file["job_id"] == job_id), None
          )
  
          if not podcast_metadata:
              raise HTTPException(
                  status_code=404, detail=f"Podcast with job_id {job_id} not found"
              )
  
          success = storage_manager.delete_job_files(job_id)
  
          if not success:
              raise HTTPException(
                  status_code=500, detail=f"Failed to delete podcast {job_id}"
              )
  
          # Also clean up any Redis entries
          for service in ServiceType:
              redis_client.delete(f"status:{job_id}:{service}")
              redis_client.delete(f"result:{job_id}:{service}")
          redis_client.delete(f"final_status:{job_id}")
  
          return {"message": f"Successfully deleted podcast {job_id}"}
  
      except HTTPException:
          raise
      except Exception as e:
          logger.error(f"Failed to delete podcast {job_id}: {str(e)}")
          raise HTTPException(
              status_code=500, detail=f"Failed to delete podcast: {str(e)}"
          )
  
  @app.get("/health")
  async def health():
      return {"status": "healthy"}
</file>
<file path="./services/AgentService/Dockerfile">
  FROM python:3.9-slim
  
  WORKDIR /app
  
  # Install system dependencies
  RUN apt-get update && apt-get install -y \
      build-essential \
      && rm -rf /var/lib/apt/lists/*
  
  # Create config directory
  RUN mkdir -p /app/config
  
  # Install Python dependencies
  RUN pip install fastapi uvicorn pydantic python-dotenv jinja2 redis minio
  
  # Install flexagent
  COPY services/AgentService/flexagent /app/flexagent
  WORKDIR /app/flexagent/python
  RUN pip install --use-pep517 -e .
  WORKDIR /app
  
  COPY shared /shared
  RUN pip install /shared
  
  # Copy service files
  COPY services/AgentService/main.py ./
  COPY services/AgentService/prompts.py ./
  
  EXPOSE 8964
  
  CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8964"]</file>
<file path="./services/AgentService/README.md">
   docker build -t agent-service .
  
  
   docker run -p 8964:8964 -e NIM_KEY=$NIM_KEY  agent-service</file>
<file path="./services/AgentService/flexagent/README.md">
  # FlexAgent
  
  
  
  ## Getting started
  
  ```
  cd python
  pip install --use-pep517 -e .
  ```
</file>
<file path="./services/AgentService/flexagent/python/flexagent/__init__.py">
  # Copyright (c) 2024 NVIDIA Corporation
  
  """
  FlexAgent: A flexible agent framework for AI applications.
  
  This package provides tools and utilities for building and deploying
  AI agents with customizable behaviors and capabilities.
  """
  
  __version__ = "0.1.0"
  __author__ = "NVIDIA Corporation"
  __license__ = "Proprietary"
  
  # Import main components
  import atexit
  
  from . import engine, ops, utils
  
  # Convenience imports
  # from .utils import logger
  
  # executor.TaskQueue.start_engine()
  atexit.register(engine.shutdown_engine)
  
  # Define what's available when using `from flexagent import *`
  __all__ = ["engine", "ops", "utils"]
</file>
<file path="./services/AgentService/flexagent/python/flexagent/backend/__init__.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  # flake8: noqa
  
  from .base_backend import BackendConfig
  from .registry import get
  from .openai import *
  from .anthropic import *
  from .nim import *
  
  
  __all__ = ["get", "BackendConfig"]
</file>
<file path="./services/AgentService/flexagent/python/flexagent/backend/anthropic.py">
  from typing import Any, Dict, List, Optional
  
  from .base_backend import BaseBackend
  from .registry import reg
  from .sampling_params import SamplingParams
  
  
  class AnthropicBackend(BaseBackend):
      """
      A backend class for interacting with Anthropic's language models.
  
      This class provides an interface to generate text using Anthropic's API.
      It inherits from BaseBackend and implements the necessary methods for text generation.
  
      :param model_name: The name of the Anthropic model to use.
      :param api_key: The API key for authenticating with Anthropic's API.
      :param api_base: The base URL for the Anthropic API (optional).
      """
  
      def __init__(self, model_name: str, api_key: str, api_base: Optional[str] = None):
          super().__init__(model_name)
          import anthropic
  
          self.client: anthropic.Anthropic = anthropic.Anthropic(
              api_key=api_key, base_url=api_base
          )
  
      def generate(self, messages: List[Dict[str, str]], *args, **kwargs) -> str:
          """
          Generate text using the Anthropic model.
  
          This method takes a system prompt and a user prompt, combines them into a message format
          expected by Anthropic's API, and generates a response.
  
          :param messages: The messages to generate text from.
          :param args: Additional positional arguments (not used in this implementation).
          :param kwargs: Additional keyword arguments. Can include 'sampling_params'.
          :return: The generated text response from the model.
  
          :example:
  
          >>> backend = AnthropicBackend("claude-2", "your-api-key")
          >>> system_prompt = "You are a helpful assistant."
          >>> user_prompt = "What is the capital of France?"
          >>> response = backend.generate(system_prompt, user_prompt)
          >>> print(response)
          The capital of France is Paris.
          """
  
          sampling_params = kwargs.get("sampling_params", SamplingParams())
          anthropic_params = sampling_params.to_anthropic_params()
  
          response = self.client.messages.create(
              model=self.model_name, messages=messages, **anthropic_params
          )
  
          return response.content[0].text.strip()
  
      def __call__(self, *args: Any, **kwds: Any) -> Dict[str, Any]:
          """
          Make the AnthropicBackend instance callable.
  
          This method allows the instance to be called directly, which in turn calls the generate method.
  
          :param args: Positional arguments to pass to the generate method.
          :param kwds: Keyword arguments to pass to the generate method.
          :return: A dictionary containing the generated text.
  
          :example:
  
          >>> backend = AnthropicBackend("claude-2", "your-api-key")
          >>> result = backend("You are a helpful assistant.", "What is the capital of France?")
          >>> print(result)
          {'generated_text': 'The capital of France is Paris.'}
          """
          generated_text = self.generate(*args, **kwds)
          return {"generated_text": generated_text}
  
  
  @reg("anthropic")
  def get_anthropic_backend(
      model_name: str, api_key: str, api_base: Optional[str] = None
  ):
      """
      Factory function to create an AnthropicBackend instance.
  
      This function is registered with the 'anthropic' key and can be used to create
      AnthropicBackend instances dynamically.
  
      :param model_name: The name of the Anthropic model to use.
      :param api_key: The API key for authenticating with Anthropic's API.
      :param api_base: The base URL for the Anthropic API (optional).
      :return: An instance of AnthropicBackend.
  
      :example:
  
      >>> backend = get_anthropic_backend("claude-2", "your-api-key")
      >>> response = backend.generate("You are a helpful assistant.", "What is the capital of France?")
      >>> print(response)
      The capital of France is Paris.
      """
      return AnthropicBackend(model_name, api_key, api_base)
</file>
<file path="./services/AgentService/flexagent/python/flexagent/backend/base_backend.py">
  # Copyright (c) 2024 NVIDIA Corporation
  
  from abc import ABC, abstractmethod
  from dataclasses import dataclass
  from typing import Dict, List
  
  
  @dataclass
  class BackendConfig:
      backend_type: str
      model_name: str
      api_key: str = None
      api_base: str = None
  
  
  class BaseBackend(ABC):
      """
      An abstract base class for implementing backend interfaces for different language models.
  
      This class provides a common structure for various backend implementations,
      allowing for consistent interaction with different language models.
  
      Attributes:
          _model_name (str): The name of the language model being used.
          client: The client object for interacting with the language model API (if applicable).
  
      Example:
          class MyBackend(BaseBackend):
              def __init__(self, model_name: str):
                  super().__init__(model_name)
                  self.client = MyAPIClient()
  
              def generate(self, prompt: str) -> str:
                  return self.client.generate_text(prompt)
      """
  
      def __init__(self, model_name: str, *args, **kwargs):
          """
          Initialize the BaseBackend.
  
          Args:
              model_name (str): The name of the language model to be used.
              *args: Variable length argument list.
              **kwargs: Arbitrary keyword arguments.
          """
          super().__init__()
          self._model_name = model_name
          self.client = None
  
      @property
      def model_name(self) -> str:
          """
          Get the name of the current language model.
  
          Returns:
              str: The name of the language model.
          """
          return self._model_name
  
      @model_name.setter
      def model_name(self, model_name: str):
          """
          Set the name of the language model.
  
          Args:
              model_name (str): The new name for the language model.
          """
          self._model_name = model_name
  
      @abstractmethod
      def generate(self, messages: List[Dict[str, str]], *args, **kwargs) -> str:
          """
          Generate text based on the given prompt using the language model.
  
          This method must be implemented by subclasses to define the specific
          text generation logic for the chosen language model.
  
          Args:
              prompt (str): The input prompt for text generation.
              *args: Variable length argument list.
              **kwargs: Arbitrary keyword arguments.
  
          Returns:
              str: The generated text response.
  
          Raises:
              NotImplementedError: If the method is not implemented by a subclass.
  
          Example:
              def generate(self, prompt: str, max_tokens: int = 100) -> str:
                  response = self.client.complete(prompt, max_tokens=max_tokens)
                  return response.text
          """
          pass
</file>
<file path="./services/AgentService/flexagent/python/flexagent/backend/nim.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  import os
  from typing import Dict, List, Optional, Any
  
  from .openai import OpenAIBackend
  from .registry import reg
  from .sampling_params import SamplingParams
  import logging
  logging.basicConfig(level=logging.INFO)
  
  class NIMBackend(OpenAIBackend):
      """
      A backend class for interacting with NVIDIA Inference Microservices (NIM) API.
  
      This class inherits from OpenAIBackend and customizes it for use with NIM.
      It provides an interface for generating text using NIM's API, which is
      compatible with the OpenAI API format.
  
      :param model_name: The name of the NIM model to use.
      :param api_key: The API key for authenticating with NIM. If not provided,
                      it will attempt to use the NIM_KEY environment variable.
      :param api_base: The base URL for the NIM API. Defaults to the standard NIM endpoint.
  
      :type model_name: str
      :type api_key: Optional[str]
      :type api_base: str
  
      :raises ValueError: If no API key is provided or found in the environment.
  
      Example:
          >>> from flexagent.backend.nim import NIMBackend
          >>> from flexagent.backend.sampling_params import SamplingParams
          >>>
          >>> # Initialize the NIM backend
          >>> nim_backend = NIMBackend(model_name="meta/llama-3.1-8b-instruct")
          >>>
          >>> # Set up prompts and parameters
          >>> system_prompt = "You are a helpful assistant."
          >>> user_prompt = "What is the capital of France?"
          >>> sampling_params = SamplingParams(max_new_tokens=50, temperature=0.7)
          >>>
          >>> # Generate a response
          >>> response = nim_backend.generate(system_prompt, user_prompt, sampling_params)
          >>> print(response)
          The capital of France is Paris.
      """
  
      def __init__(
          self,
          model_name: str,
          api_base: str,
          api_key: Optional[str] = None,
      ):
          api_key = api_key or os.getenv("NIM_KEY")
          self.max_retry = 5
          if not api_key:
              raise ValueError(
                  "NIM API key is required. Set it as NIM_KEY environment variable or pass it explicitly."
              )
  
          super().__init__(model_name, api_key=api_key, api_base=api_base)
  
      def generate(
          self,
          messages: List[Dict[str, str]],
          sampling_params: Optional[SamplingParams] = None,
          extra_body: Optional[Dict[str, Any]] = None,
      ) -> str:
          """
          Generate text using the NIM API based on the given prompts and parameters.
  
          This method overrides the parent class method to allow for any NIM-specific
          adjustments that might be needed in the future.
  
          :param messages: The messages to generate text from.
          :param sampling_params: Optional parameters to control the text generation.
  
          :type messages: List[Dict[str, str]]
          :type sampling_params: Optional[SamplingParams]
  
          :return: The generated text response.
          :rtype: str
  
          Example:
              >>> nim_backend = NIMBackend(model_name="meta/llama-3.1-8b-instruct")
              >>> system_prompt = "You are a helpful assistant."
              >>> user_prompt = "Explain the concept of machine learning in one sentence."
              >>> params = SamplingParams(max_new_tokens=30, temperature=0.8)
              >>> response = nim_backend.generate(system_prompt, user_prompt, params)
              >>> print(response)
              Machine learning is the process of training algorithms to learn patterns
              from data and make predictions or decisions without explicit programming.
          """
          # NIM-specific adjustments can be made here if needed in the future
          for _ in range(self.max_retry):
              try:
                  return super().generate(messages, sampling_params, extra_body=extra_body)
              except Exception as e:
                  logging.error(f"Error generating text: {e}")
          raise Exception("Failed to generate text after multiple retries")
  
  
  @reg("nim")
  def get_nim_backend(
      model_name: str,
      api_key: Optional[str] = None,
      api_base: Optional[str] = None,
  ) -> NIMBackend:
      """
      Factory function to create an NIMBackend instance.
  
      This function is registered with the "nim" key and can be used to create
      OpenAIBackend instances.
  
      :param model_name: The name of the NIM model to use.
      :param api_key: The API key for authentication with NIM.
      :param api_base: Optional custom API base URL.
      :return: An instance of NIMBackend.
  
      :example:
  
      >>> backend = get_nim_backend("meta/llama-3.1-8b-instruct", "your-api-key")
      >>> response = backend("You are a helpful assistant.", "What is the capital of France?")
      >>> print(response)
      The capital of France is Paris.
      """
      return NIMBackend(model_name=model_name, api_key=api_key, api_base=api_base)
</file>
<file path="./services/AgentService/flexagent/python/flexagent/backend/openai.py">
  from typing import Any, Dict, List, Optional
  
  from .base_backend import BaseBackend
  from .registry import reg
  from .sampling_params import SamplingParams
  
  
  class OpenAIBackend(BaseBackend):
      """
      A backend class for interacting with OpenAI's API.
  
      This class provides methods to generate text using OpenAI's language models.
  
      :param model_name: The name of the OpenAI model to use.
      :param api_key: The API key for authentication with OpenAI.
      :param org_id: Optional organization ID for OpenAI API access.
      :param api_base: Optional custom API base URL.
      """
  
      def __init__(
          self,
          model_name: str,
          api_key: str,
          org_id: Optional[str] = None,
          api_base: Optional[str] = None,
      ):
          super().__init__(model_name)
          import openai
  
          self.client = openai.OpenAI(
              api_key=api_key, organization=org_id, base_url=api_base
          )
  
      def generate(
          self,
          messages: List[Dict[str, str]],
          sampling_params: Optional[SamplingParams] = None,
          *args: Any,
          **kwargs: Any
      ) -> str:
          """
          Generate text using the OpenAI API.
  
          :param messages: The list of messages to generate text from.
          :param sampling_params: Optional sampling parameters to control text generation.
          :param args: Additional positional arguments to pass to the API call.
          :param kwargs: Additional keyword arguments to pass to the API call.
          :return: The generated text response.
  
          :example:
  
          >>> backend = OpenAIBackend("gpt-3.5-turbo", "your-api-key")
          >>> response = backend.generate(
          ...     messages=[
          ...         {"role": "system", "content": "You are a helpful assistant."},
          ...         {"role": "user", "content": "What is the capital of France?"}
          ...     ],
          ...     sampling_params=SamplingParams(temperature=0.7, max_new_tokens=50)
          ... )
          >>> print(response)
          The capital of France is Paris.
          """
  
          if sampling_params:
              kwargs.update(
                  {
                      "temperature": sampling_params.temperature,
                      "max_tokens": sampling_params.max_new_tokens,
                      "top_p": sampling_params.top_p,
                      "frequency_penalty": sampling_params.frequency_penalty,
                      "presence_penalty": sampling_params.presence_penalty,
                  }
              )
  
          if "tools" in kwargs:
              tool_msg = {
                  "role": "system",
                  "content": kwargs["tools"]
              }
              messages = [tool_msg, *messages]
          response = self.client.chat.completions.create(
              model=self.model_name, stream=True, messages=messages, *args, **kwargs
          )
  
          accumulated_content = ""
          for chunk in response:
              if chunk.choices[0].delta.content is not None:
                  accumulated_content += chunk.choices[0].delta.content
  
          return accumulated_content
  
      def __call__(self, *args: Any, **kwds: Any) -> str:
          """
          Allow the class instance to be called as a function.
  
          This method simply calls the `generate` method with the provided arguments.
  
          :param args: Positional arguments to pass to the `generate` method.
          :param kwds: Keyword arguments to pass to the `generate` method.
          :return: The generated text response.
          """
          return self.generate(*args, **kwds)
  
  
  @reg("openai")
  def get_openai_backend(
      model_name: str,
      api_key: str,
      org_id: Optional[str] = None,
      api_base: Optional[str] = None,
  ) -> OpenAIBackend:
      """
      Factory function to create an OpenAIBackend instance.
  
      This function is registered with the "openai" key and can be used to create
      OpenAIBackend instances.
  
      :param model_name: The name of the OpenAI model to use.
      :param api_key: The API key for authentication with OpenAI.
      :param org_id: Optional organization ID for OpenAI API access.
      :param api_base: Optional custom API base URL.
      :return: An instance of OpenAIBackend.
  
      :example:
  
      >>> backend = get_openai_backend("gpt-3.5-turbo", "your-api-key")
      >>> response = backend("You are a helpful assistant.", "What is the capital of France?")
      >>> print(response)
      The capital of France is Paris.
      """
      return OpenAIBackend(model_name, api_key, org_id, api_base)
</file>
<file path="./services/AgentService/flexagent/python/flexagent/backend/registry.py">
  # Copyright (c) 2024 NVIDIA Corporation
  
  from typing import Callable, Optional
  
  
  BACKEND_REGISTRY: dict[str, Callable[[], None]] = {}
  
  
  def reg(func_name: str, func: Optional[Callable[[], None]] = None) -> Callable:
      """
      Register a backend function in the BACKEND_REGISTRY.
  
      This decorator can be used in two ways:
      1. As a decorator without arguments: @reg("func_name")
      2. As a function call: reg("func_name", func)
  
      :param func_name: A string identifier for the backend function
      :param func: The function to be registered (optional)
      :return: The registered function
      :raises ValueError: If a backend with the same name is already registered
  
      Examples:
      ---------
      >>> @reg("my_backend")
      ... def my_backend_func():
      ...     pass
  
      >>> def another_backend():
      ...     pass
      >>> reg("another_backend", another_backend)
      """
      if func_name in BACKEND_REGISTRY:
          raise ValueError(f"Backend {func_name} already registered")
  
      def _do_reg(func: Callable[[], None]) -> Callable[[], None]:
          BACKEND_REGISTRY[func_name] = func
          return func
  
      return _do_reg if func is None else _do_reg(func)
  
  
  def get(func_name: str) -> Callable:
      """
      Retrieve a registered backend function from the BACKEND_REGISTRY.
  
      :param func_name: The string identifier of the backend function to retrieve
      :return: The registered backend function
      :raises ValueError: If the requested backend is not found in the registry
  
      Example:
      --------
      >>> registered_func = get("my_backend")
      >>> registered_func()
      """
      try:
          return BACKEND_REGISTRY[func_name]
      except KeyError:
          raise ValueError(f"Backend {func_name} not registered")
</file>
<file path="./services/AgentService/flexagent/python/flexagent/backend/sampling_params.py">
  import dataclasses
  from typing import Any, Dict, List, Union
  
  
  @dataclasses.dataclass
  class SamplingParams:
      """
      A dataclass representing sampling parameters for language models.
  
      This class encapsulates various parameters used in text generation tasks,
      such as temperature, top-p sampling, and maximum token limits.
  
      :param max_new_tokens: Maximum number of new tokens to generate.
      :param temperature: Controls randomness in generation. Higher values increase randomness.
      :param top_p: Cumulative probability for top-p sampling.
      :param top_k: Number of highest probability vocabulary tokens to keep for top-k sampling.
      :param repetition_penalty: Penalty for repeating tokens.
      :param stop: Token(s) at which to stop generation.
      :param frequency_penalty: Penalty for frequently used tokens.
      :param presence_penalty: Penalty for new tokens.
  
      :type max_new_tokens: int
      :type temperature: float
      :type top_p: float
      :type top_k: int
      :type repetition_penalty: float
      :type stop: Union[str, List[str]]
      :type frequency_penalty: float
      :type presence_penalty: float
  
      Example:
          >>> params = SamplingParams(max_new_tokens=100, temperature=0.8, stop=["END"])
          >>> print(params.temperature)
          0.8
      """
  
      max_new_tokens: int = 8192
      temperature: float = 0.7
      top_p: float = 0.9
      top_k: int = 50
      repetition_penalty: float = 1.1
      stop: Union[str, List[str]] = ()
      frequency_penalty: float = 0.0
      presence_penalty: float = 0.0
  
      def to_openai_params(self) -> Dict[str, Any]:
          """
          Convert sampling parameters to OpenAI API format.
  
          :return: A dictionary of parameters compatible with OpenAI API.
          :rtype: Dict[str, Any]
  
          Example:
              >>> params = SamplingParams(max_new_tokens=100, temperature=0.8, stop="END")
              >>> openai_params = params.to_openai_params()
              >>> print(openai_params)
              {'max_tokens': 100, 'temperature': 0.8, 'top_p': 0.9, 'repetition_penalty': 1.1, 'stop': 'END'}
          """
          return {
              "max_tokens": self.max_new_tokens,
              "temperature": self.temperature,
              "top_p": self.top_p,
              "repetition_penalty": self.repetition_penalty,
              "stop": self.stop or None,
          }
  
      def to_anthropic_params(self) -> Dict[str, Any]:
          """
          Convert sampling parameters to Anthropic API format.
  
          :return: A dictionary of parameters compatible with Anthropic API.
          :rtype: Dict[str, Any]
  
          Example:
              >>> params = SamplingParams(max_new_tokens=100, temperature=0.8, stop=["END"])
              >>> anthropic_params = params.to_anthropic_params()
              >>> print(anthropic_params)
              {'max_tokens': 100, 'temperature': 0.8, 'top_p': 0.9, 'top_k': 50, 'stop_sequences': ['END']}
          """
          return {
              "max_tokens": self.max_new_tokens,
              "temperature": self.temperature,
              "top_p": self.top_p,
              "top_k": self.top_k,
              "stop_sequences": (
                  self.stop if isinstance(self.stop, (list, tuple)) else [self.stop]
              ),
          }
</file>
<file path="./services/AgentService/flexagent/python/flexagent/engine/__init__.py">
  # depengine/__init__.py
  
  from .engine import get_engine, Resource, shutdown_engine
  from .operator import Operator
  from .operator_schema import OperatorInputsSchema, OperatorSchema
  from .value import Value
  
  
  __all__ = [
      "get_engine",
      "shutdown_engine",
      "Operator",
      "Value",
      "Resource",
      "OperatorSchema",
      "OperatorInputsSchema",
  ]
</file>
<file path="./services/AgentService/flexagent/python/flexagent/engine/dill_executor.py">
  import multiprocessing
  from concurrent.futures import Future, ProcessPoolExecutor
  from typing import Any, Callable, Optional, Tuple
  
  import dill
  
  
  class DillProcessPoolExecutor(ProcessPoolExecutor):
      """
      A subclass of ProcessPoolExecutor that uses dill for serialization.
  
      This class extends the functionality of ProcessPoolExecutor by using dill
      to serialize and deserialize functions and arguments. This allows for
      more complex objects and functions to be passed to the executor.
  
      :param max_workers: The maximum number of processes to use for execution.
                          If None, it will default to the number of processors on the machine.
      :param mp_context: A multiprocessing context to use for multiprocessing.
      :param initializer: An optional callable used to initialize worker processes.
      :param initargs: Arguments to pass to the initializer.
      """
  
      def __init__(
          self,
          max_workers: Optional[int] = None,
          mp_context: Optional[multiprocessing.context.BaseContext] = None,
          initializer: Optional[Callable[..., None]] = None,
          initargs: Tuple[Any, ...] = (),
      ) -> None:
          super().__init__(
              max_workers=max_workers,
              mp_context=mp_context,
              initializer=initializer,
              initargs=initargs,
          )
  
      def submit(self, fn: Callable[..., Any], *args: Any, **kwargs: Any) -> Future:
          """
          Submit a callable to be executed with the given arguments.
  
          :param fn: The callable to be executed.
          :param args: Positional arguments for the callable.
          :param kwargs: Keyword arguments for the callable.
          :return: A Future representing the execution of the callable.
          """
          # Serialize the function and arguments using dill
          fn_pkl = dill.dumps(fn)
          args_pkl = dill.dumps(args)
          kwargs_pkl = dill.dumps(kwargs)
          # Submit the wrapper function to the executor
          return super().submit(self._dill_wrapper, fn_pkl, args_pkl, kwargs_pkl)
  
      @staticmethod
      def _dill_wrapper(fn_pkl: bytes, args_pkl: bytes, kwargs_pkl: bytes) -> Any:
          """
          Internal method to deserialize and execute the function.
  
          :param fn_pkl: Pickled function.
          :param args_pkl: Pickled positional arguments.
          :param kwargs_pkl: Pickled keyword arguments.
          :return: The result of the function execution.
          """
          # Deserialize the function and arguments
          fn = dill.loads(fn_pkl)
          args = dill.loads(args_pkl)
          kwargs = dill.loads(kwargs_pkl)
          # Execute the function
          return fn(*args, **kwargs)
</file>
<file path="./services/AgentService/flexagent/python/flexagent/engine/engine.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  
  import multiprocessing
  import os
  import threading
  from collections import defaultdict, deque
  from concurrent.futures import Future
  from dataclasses import dataclass
  from typing import Any, Callable, Deque, Dict, List, Optional, Set, Tuple
  
  from .dill_executor import DillProcessPoolExecutor
  
  from .value import Value
  
  
  @dataclass(frozen=True)
  class Resource:
      """
      Represents a resource that can be used by tasks in the dependency engine.
  
      :param name: The name of the resource.
      :type name: str
      """
  
      name: str
  
  
  class DepEngine:
      """
      A dependency engine that manages task execution with resource constraints.
  
      This class handles task submission, dependency tracking, and resource management
      for concurrent task execution using a process pool.
  
      Examples:
          >>> engine = DepEngine()
          >>> def task_function(x, y):
          ...     return x + y
          >>> input_value = Value()
          >>> output_value = Value()
          >>> engine.submit_task(task_function, (5, 3), {}, [input_value], [output_value])
          >>> engine.shutdown()
      """
  
      def __init__(self) -> None:
          """
          Initialize the DepEngine with a process pool executor and necessary data structures.
  
          The engine uses a ProcessPoolExecutor with a number of workers equal to the number
          of CPU cores available on the system.
  
          Examples:
              >>> engine = DepEngine()
              >>> isinstance(engine.executor, ProcessPoolExecutor)
              True
          """
          max_workers = int(os.environ.get("MAX_WORKERS", os.cpu_count()))
          # self.executor: ProcessPoolExecutor = ProcessPoolExecutor(
          #     max_workers=max_workers, mp_context=multiprocessing.get_context("spawn")
          # )
          self.executor: DillProcessPoolExecutor = DillProcessPoolExecutor(
              max_workers=max_workers, mp_context=multiprocessing.get_context("spawn")
          )
          self.tasks: Dict[int, Dict[str, Any]] = {}
          self.task_counter: int = 0
          self.lock: threading.RLock = threading.RLock()
          self.active_resources: Set[Resource] = set()
          self.resource_queues: Dict[Resource, Deque[int]] = defaultdict(deque)
  
      def submit_task(
          self,
          func: Callable,
          args: Tuple,
          kwargs: Dict[str, Any],
          input_values: List[Value],
          output_values: List[Value],
          resources: Optional[Set[Resource]] = None,
      ) -> None:
          """
          Submit a task to the dependency engine for execution.
  
          This method adds the task to the engine's queue, considering its dependencies
          and resource requirements. If all dependencies are met and required resources
          are available, the task is immediately submitted for execution.
  
          Args:
              func (Callable): The function to be executed.
              args (Tuple): Positional arguments for the function.
              kwargs (Dict[str, Any]): Keyword arguments for the function.
              input_values (List[Value]): List of input Value objects.
              output_values (List[Value]): List of output Value objects.
              resources (Optional[Set[Resource]]): Set of Resource objects required by the task.
  
          Examples:
              >>> engine = DepEngine()
              >>> def add(x, y):
              ...     return x + y
              >>> input_val1, input_val2 = Value(), Value()
              >>> output_val = Value()
              >>> input_val1.data, input_val2.data = 5, 3
              >>> input_val1.event.set(), input_val2.event.set()
              >>> engine.submit_task(add, (input_val1, input_val2), {}, [input_val1, input_val2], [output_val])
              >>> # The task will be executed and the result will be stored in output_val
          """
          with self.lock:
              task_id = self.task_counter
              self.task_counter += 1
              task = {
                  "func": func,
                  "args": args,
                  "kwargs": kwargs,
                  "input_values": input_values,
                  "output_values": output_values,
                  "dependencies": set(),
                  "resources": resources or set(),
              }
              self.tasks[task_id] = task  # Store the task
              for val in input_values:
                  if not val.event.is_set():
                      task["dependencies"].add(val)
                      val.dependents.add(task_id)
              if not task["dependencies"]:
                  # All inputs are ready, try to submit the task
                  self._try_submit_task(task_id, task)
  
      def _try_submit_task(self, task_id: int, task: Dict[str, Any]) -> None:
          """
          Attempt to submit a task for execution if resources are available.
  
          This internal method checks if the required resources for the task are available.
          If so, it marks the resources as in use and submits the task for execution.
          Otherwise, it enqueues the task for later execution.
  
          Args:
              task_id (int): The unique identifier of the task.
              task (Dict[str, Any]): The task dictionary containing all necessary information.
  
          Note:
              This method assumes that the engine's lock is held when it's called.
          """
          # Assumes lock is held
          if self._resources_available(task["resources"]):
              # Mark resources as in use
              self.active_resources.update(task["resources"])
              # Proceed to submit the task
              func = task["func"]
              args = []
              for arg in task["args"]:
                  if isinstance(arg, Value):
                      args.append(arg.data)
                  else:
                      args.append(arg)
              future = self.executor.submit(func, *args, **task["kwargs"])
              future.task_id = task_id
              future.add_done_callback(self._task_done)
          else:
              # Resources are not available, enqueue the task per resource
              for resource in task["resources"]:
                  self.resource_queues[resource].append(task_id)
  
      def _resources_available(self, resources: Set[Resource]) -> bool:
          """
          Check if all required resources are available.
  
          This method determines whether the set of required resources for a task
          is disjoint from the set of currently active (in-use) resources.
  
          Args:
              resources (Set[Resource]): Set of resources to check.
  
          Returns:
              bool: True if all resources are available, False otherwise.
  
          Examples:
              >>> engine = DepEngine()
              >>> res1, res2 = Resource("CPU"), Resource("GPU")
              >>> engine.active_resources = {res1}
              >>> engine._resources_available({res2})
              True
              >>> engine._resources_available({res1, res2})
              False
          """
          return resources.isdisjoint(self.active_resources)
  
      def _task_done(self, future: Future) -> None:
          """
          Callback function to handle task completion.
  
          This method is called when a task completes execution. It processes the task's
          result, updates the output values, releases resources, and checks if any
          waiting tasks can now proceed.
  
          Args:
              future (Future): The completed Future object.
  
          Note:
              This method handles both successful task completions and exceptions.
          """
          task_id = future.task_id  # type: ignore
          try:
              result = future.result()
          except Exception as e:
              result = e
          with self.lock:
              task = self.tasks.pop(task_id, None)
              if task is None:
                  return
              # Release resources
              self.active_resources.difference_update(task["resources"])
              # Check if any waiting tasks can now proceed
              self._release_waiting_tasks(task["resources"])
              if isinstance(result, Exception):
                  # Handle exception
                  result = (result,)
              else:
                  if not isinstance(result, tuple):
                      result = (result,)
              for val, res in zip(task["output_values"], result):
                  val.data = res
                  val.event.set()
                  # Notify dependents
                  for dep_task_id in val.dependents.copy():
                      dep_task = self.tasks.get(dep_task_id)
                      if dep_task:
                          dep_task["dependencies"].remove(val)
                          if not dep_task["dependencies"]:
                              # All dependencies are resolved, try to submit the task
                              self._try_submit_task(dep_task_id, dep_task)
  
      def _release_waiting_tasks(self, released_resources: Set[Resource]) -> None:
          """
          Attempt to submit waiting tasks after resources have been released.
  
          This method checks if any tasks waiting for the released resources can now
          be submitted for execution.
  
          Args:
              released_resources (Set[Resource]): Set of resources that have been released.
  
          Note:
              This method is called internally after a task completes and releases its resources.
          """
          # Try to submit tasks waiting for the released resources
          for resource in released_resources:
              queue = self.resource_queues[resource]
              tasks_to_try = []
              while queue:
                  task_id = queue.popleft()
                  task = self.tasks.get(task_id)
                  if (
                      task
                      and self._resources_available(task["resources"])
                      and not task["dependencies"]
                  ):
                      tasks_to_try.append((task_id, task))
                  else:
                      # If resources are still not available or task has dependencies, re-enqueue
                      queue.appendleft(task_id)
                      break  # Exit to prevent infinite loop
              for task_id, task in tasks_to_try:
                  self.active_resources.update(task["resources"])
                  func = task["func"]
                  args = []
                  for arg in task["args"]:
                      if isinstance(arg, Value):
                          args.append(arg.data)
                      else:
                          args.append(arg)
                  future = self.executor.submit(func, *args, **task["kwargs"])
                  future.task_id = task_id
                  future.add_done_callback(self._task_done)
  
      def shutdown(self) -> None:
          """
          Shutdown the dependency engine and its executor.
  
          This method waits for all pending tasks to complete before shutting down
          the ProcessPoolExecutor.
  
          Examples:
              >>> engine = DepEngine()
              >>> # ... submit some tasks ...
              >>> engine.shutdown()
              >>> # All tasks are completed and resources are released
          """
          self.executor.shutdown(wait=True)
  
  
  # Singleton engine instance
  _engine: Optional[DepEngine] = None
  _engine_lock: threading.Lock = threading.Lock()
  _engine_thread: Optional[threading.Thread] = None
  
  
  def initialize_engine() -> DepEngine:
      """
      Initialize and return the singleton DepEngine instance.
  
      If called from a non-main process, it starts the engine in a new thread.
  
      :return: The initialized DepEngine instance.
      :rtype: DepEngine
      """
      global _engine
      global _engine_thread
      with _engine_lock:
          if _engine is None:
              if multiprocessing.current_process().name == "MainProcess":
                  _engine = DepEngine()
              else:
                  # pop out warning
                  print("__main__ is not detected, starting the engine in a new thread")
                  assert _engine_thread is None, "Engine thread already exists"
                  _engine_thread = threading.Thread(target=initialize_engine)
                  _engine_thread.start()
  
      return _engine
  
  
  def get_engine() -> DepEngine:
      """
      Get the singleton DepEngine instance, initializing it if necessary.
  
      :return: The DepEngine instance.
      :rtype: DepEngine
      """
      global _engine
      if _engine is None:
          initialize_engine()
      with _engine_lock:
          return _engine
  
  
  def shutdown_engine() -> None:
      """
      Shutdown the singleton DepEngine instance and join any associated threads.
      """
      global _engine
      global _engine_thread
      with _engine_lock:
          if _engine is not None:
              _engine.shutdown()
              _engine = None
          if _engine_thread is not None:
              _engine_thread.join()
              _engine_thread = None
</file>
<file path="./services/AgentService/flexagent/python/flexagent/engine/operator.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  
  from abc import ABC, abstractmethod
  from typing import Any, Callable, Dict, List, Optional
  
  from .engine import get_engine, Resource
  from .value import Value
  
  
  class Operator(ABC):
      """
      A class that wraps a function to be executed as an operation in the engine.
  
      The Operator class is responsible for submitting tasks to the engine and
      managing the input and output values for the wrapped function.
  
      :param func: The function to be wrapped and executed as an operation.
      :type func: Callable[..., Any]
  
      Attributes:
          engine: The engine instance used for task submission.
          func: The wrapped function to be executed.
  
      Warning:
          The wrapped function (func) must be picklable. Non-picklable functions
          may cause deadlocks when submitted to the engine.
  
      Example:
          >>> def add(a, b):
          ...     return a + b
          >>> op = Operator(add)
          >>> result = op(3, 4)
          >>> print(result)  # This will print a Value object
      """
  
      def __init__(self, func: Callable[..., Any]):
          self.engine = get_engine()
          self.func = func
  
      @staticmethod
      @abstractmethod
      def get_function_schema(cls) -> Dict[str, Any]:
          pass
  
      def __call__(
          self, *args: Any, resources: Optional[List[Resource]] = None, **kwargs: Any
      ) -> Value:
          """
          Execute the wrapped function as an operation in the engine.
  
          This method prepares the input values, submits the task to the engine,
          and returns a Value object representing the output.
  
          :param args: Positional arguments to be passed to the wrapped function.
          :type args: Any
          :param resources: Optional list of resources required for the operation.
          :type resources: Optional[List[Resource]]
          :param kwargs: Keyword arguments to be passed to the wrapped function.
          :type kwargs: Any
          :return: A Value object representing the output of the operation.
          :rtype: Value
  
          :raises: Any exception that may be raised by the wrapped function or the engine.
  
          Note:
              - If an argument is a Value object, it will be treated as both an input value
                and a function argument.
              - All other arguments are passed directly to the wrapped function.
              - The method returns immediately with a Value object, which will be populated
                with the result once the task is completed by the engine.
          """
          input_values: List[Value] = []
          func_args: List[Any] = []
          for arg in args:
              if isinstance(arg, Value):
                  input_values.append(arg)
                  func_args.append(arg)
              else:
                  func_args.append(arg)
          out = Value()
          output_values = [out]
          self.engine.submit_task(
              self.func, func_args, kwargs, input_values, output_values, resources
          )
          return out
</file>
<file path="./services/AgentService/flexagent/python/flexagent/engine/operator_schema.py">
  # Copyright 2024 NVIDIA Corporation. All rights reserved.
  
  from typing import Any, ClassVar, Dict, get_args, get_origin, Type, Union
  
  from pydantic import BaseModel
  
  
  class OperatorSchema(BaseModel):
      """
      A base class for defining operator schemas.
  
      This class provides a structure for defining operator schemas with name, description,
      and parameters. It also includes methods for generating function schemas.
  
      Attributes:
          name (ClassVar[str]): The name of the operator.
          description (ClassVar[str]): A description of the operator.
          parameters (ClassVar[Type[BaseModel]]): A Pydantic model defining the operator's parameters.
      """
  
      name: ClassVar[str]
      description: ClassVar[str]
      parameters: ClassVar[Type[BaseModel]]
  
      class Config:
          extra = "forbid"
  
          @classmethod
          def model_modify_json_schema(cls, json_schema):
              """
              Modify the JSON schema by removing the 'title' field.
  
              Args:
                  json_schema (dict): The original JSON schema.
  
              Returns:
                  dict: The modified JSON schema.
              """
              # Remove 'title' from the JSON schema
              json_schema.pop("title", None)
              return json_schema
  
      @classmethod
      def get_function_schema(cls) -> Dict[str, Any]:
          """
          Generate a function schema based on the operator's parameters.
  
          This method creates a JSON-compatible schema that describes the operator's
          parameters, including their types and descriptions.
  
          Returns:
              Dict[str, Any]: A dictionary representing the function schema.
          """
          # Generate the parameters schema
          parameters_schema = {
              "type": "object",
              "properties": {},
              "required": [],
              "additionalProperties": False,
          }
  
          # Access the fields of the parameters model
          for field_name, field_info in cls.parameters.model_fields.items():
              # Get the annotation (type) of the field
              field_type = field_info.annotation
              field_schema = {}
  
              # Handle typing.Optional
              if get_origin(field_type) is Union:
                  field_types = [t for t in get_args(field_type) if t is not type(None)]
                  field_type = field_types[0] if field_types else Any
                  is_optional = True
              else:
                  is_optional = False
  
              # Map Python types to JSON Schema types
              origin_type = get_origin(field_type)
              if origin_type is list:
                  # Handle list types
                  item_type = get_args(field_type)[0]
                  item_type_name = cls._map_python_type_to_json_type(item_type)
                  field_schema = {
                      "type": "array",
                      "items": {"type": item_type_name},
                      "description": field_info.description or "",
                  }
              else:
                  field_type_name = cls._map_python_type_to_json_type(field_type)
                  field_schema = {
                      "type": field_type_name,
                      "description": field_info.description or "",
                  }
  
              parameters_schema["properties"][field_name] = field_schema
              if not is_optional:
                  parameters_schema["required"].append(field_name)
  
          # Construct the final function schema
          function_schema = {
              "name": cls.name,
              "description": cls.description,
              "parameters": parameters_schema,
          }
          return function_schema
  
      @staticmethod
      def _map_python_type_to_json_type(py_type):
          """
          Map Python types to corresponding JSON schema types.
  
          Args:
              py_type: The Python type to be mapped.
  
          Returns:
              str: The corresponding JSON schema type.
          """
          if py_type is str:
              return "string"
          elif py_type is int:
              return "integer"
          elif py_type is float:
              return "number"
          elif py_type is bool:
              return "boolean"
          elif isinstance(py_type, type) and issubclass(py_type, BaseModel):
              return "object"
          elif get_origin(py_type) is list:
              return "array"
          else:
              return "string"  # Default to string for simplicity
  
  
  class OperatorInputsSchema(BaseModel):
      """
      A base class for defining operator input schemas.
  
      This class is used to create specific input schemas for operators.
      """
  
      class Config:
          extra = "forbid"
</file>
<file path="./services/AgentService/flexagent/python/flexagent/engine/value.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  
  import threading
  from typing import Any, Set
  
  
  class Value:
      """
      A thread-safe container for a value that can be set asynchronously.
  
      This class is designed to hold a value that may not be immediately available,
      allowing other parts of the code to wait for the value to be set. It also
      supports dependency tracking and exception handling.
  
      Attributes:
          data (Any): The stored value or exception.
          event (threading.Event): An event that is set when the value is available.
          dependents (Set[Any]): A set of objects that depend on this value.
  
      Args:
          data (Any, optional): The initial value to store. If provided, the event
              will be set immediately. Defaults to None.
  
      Example:
          >>> v = Value()
          >>> v.set(42)
          >>> print(v.get())
          42
      """
  
      def __init__(self, data: Any = None) -> None:
          self.data: Any = data
          self.event: threading.Event = threading.Event()
          self.dependents: Set[Any] = set()
          if data is not None:
              self.event.set()
  
      def get(self) -> Any:
          """
          Retrieve the stored value.
  
          This method blocks until the value is available. If the stored value
          is an exception, it will be raised instead of returned.
  
          Returns:
              Any: The stored value.
  
          Raises:
              Exception: If the stored value is an exception, it will be raised.
  
          Example:
              >>> v = Value()
              >>> v.set(10)
              >>> v.get()
              10
          """
          self.event.wait()
          if isinstance(self.data, Exception):
              raise self.data
          return self.data
  
      def __str__(self) -> str:
          """
          Return a string representation of the stored value.
  
          This method automatically calls get() to retrieve the value.
  
          Returns:
              str: String representation of the stored value.
  
          Raises:
              Exception: If the stored value is an exception, it will be raised.
          """
          return str(self.get())
</file>
<file path="./services/AgentService/flexagent/python/flexagent/ops/__init__.py">
  #  Copyright (c) 2024 NVIDIA Corporation.
  
  from . import pdf, search
  from .llm import LLM
  from .python import Python
  
  __all__ = ["Python", "LLM", "search", "pdf"]
</file>
<file path="./services/AgentService/flexagent/python/flexagent/ops/llm/__init__.py">
  from .llm import LLM
  
  __all__ = ["LLM"]
</file>
<file path="./services/AgentService/flexagent/python/flexagent/ops/llm/llm.py">
  # # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  
  from typing import Any, Dict, List, Optional
  import jinja2
  import json
  from flexagent.backend import BackendConfig, get
  from flexagent.engine import Operator, Resource, Value
  
  
  def compute(messages: List[Dict[str, str]], **kwargs):
      config = kwargs.pop("backend")
      backend_config = {
          k: v
          for k, v in config.__dict__.items()
          if k != "backend_type" and v is not None
      }
      backend = get(config.backend_type)(**backend_config)
      return backend.generate(messages, **kwargs)
  
  TOOL_USE_TEMPLATE = jinja2.Template("""
  You have the following tools available to you: [{{tool_names}}]
  
                                      
  {{tool_schemas}}
  
  When you think you should use a tool, you should generate a JSON object following the schema of the tool.
  ```tool
  {
      "name": "tool_name",
      "parameters": {
          "param_name": "param_value",
          ...
      }
  }
  ```
  If you don't have any tool to use, just answer the question and let the user know that you don't have any tool to use.                      
  """)
  
  class LLM(Operator):
      """
      A class representing a Language Model (LLM) operator.
  
      This class extends the Operator class and provides functionality to interact
      with various LLM backends.
  
      :param backend: An optional BackendConfig object specifying the LLM backend configuration.
      :type backend: Optional[BackendConfig]
  
      :example:
      >>> from flexagent.backend import BackendConfig
      >>> backend_config = BackendConfig(backend_type="openai", api_key="your-api-key")
      >>> llm = LLM(backend=backend_config)
      """
  
      def __init__(self, backend: Optional[BackendConfig] = None):
          super().__init__(compute)
          self.backend = backend
          self.tools = []
  
      def to(self, backend: BackendConfig):
          """
          Set or change the LLM backend configuration.
  
          :param backend: A BackendConfig object specifying the new LLM backend configuration.
          :type backend: BackendConfig
          :return: The LLM instance with updated backend configuration.
          :rtype: LLM
          :raises ValueError: If the provided backend is not a subclass of BaseBackend.
  
          :example:
          >>> new_backend_config = BackendConfig(backend_type="anthropic", api_key="your-api-key")
          >>> llm.to(new_backend_config)
          """
          if not isinstance(backend, BackendConfig):
              raise ValueError("Backend must be a subclass of BaseBackend")
          self.backend = backend
          return self
  
      def __call__(
          self, *args: Any, resources: Optional[List[Resource]] = None, **kwargs: Any
      ) -> Value:
          """
          Call the LLM operator with the specified arguments and resources.
  
          :param args: Positional arguments to be passed to the compute function.
          :param resources: Optional list of Resource objects.
          :type resources: Optional[List[Resource]]
          :param kwargs: Keyword arguments to be passed to the compute function.
          :return: The result of the LLM computation.
          :rtype: Value
          :raises ValueError: If the LLM backend is not set.
  
          :example:
          >>> result = llm("Hello, how are you?", max_tokens=50)
          >>> print(result)
          """
          # Include backend in kwargs
          kwargs["backend"] = self.backend
          if self.backend is None:
              raise ValueError("LLM backend is not set")
          if len(self.tools) > 0:
              kwargs["tools"] = self.tools
          return super().__call__(*args, resources=resources, **kwargs)
  
      @staticmethod
      def get_function_schema() -> Dict[str, Any]:
          return {}
  
      def enable_tools(self, tools: List[Operator]):
          tool_schemas = []
          tool_names = []
          for tool in tools:
              tool_schemas.append(tool.get_function_schema())
              tool_names.append(json.loads(tool.get_function_schema())["name"])
          tool_schemas = "\n".join(tool_schemas)
          tool_names = ", ".join(tool_names)
          self.tools = TOOL_USE_TEMPLATE.render(tool_schemas=tool_schemas, tool_names=tool_names)
  
</file>
<file path="./services/AgentService/flexagent/python/flexagent/ops/pdf/__init__.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  # flake8: noqa
  
  from .pdf2markdown import Pdf2Markdown
  
  __all__ = ["Pdf2Markdown"]
</file>
<file path="./services/AgentService/flexagent/python/flexagent/ops/pdf/pdf2markdown.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  
  from typing import Any, ClassVar, Dict, Type
  
  from pydantic import BaseModel, Field
  
  from flexagent.engine import Operator, OperatorInputsSchema, OperatorSchema
  
  
  def compute(pdf_uri: str, **kwargs: Any) -> str:
      """
      Convert a PDF file to Markdown.
  
      :param pdf_uri: The URI of the PDF file to convert.
      :param kwargs: Additional keyword arguments.
      :return: The converted Markdown content as a string.
      """
      try:
          from docling.document_converter import DocumentConverter  # Lazy import
      except ImportError:
          raise ImportError(
              "docling is not installed. Please install it by `pip install flexagent[all]`"
          )
      converter = DocumentConverter()
      result = converter.convert_single(pdf_uri)
      return result.render_as_markdown()
  
  
  class Pdf2MarkdownOpInputsSchema(OperatorInputsSchema):
      pdf_uri: str = Field(..., description="The URI of the PDF file to convert.")
  
  
  class Pdf2MarkdownOpSchema(OperatorSchema):
      name: ClassVar[str] = "pdf2markdown"
      description: ClassVar[str] = "Convert a PDF file to Markdown."
      parameters: ClassVar[Type[BaseModel]] = Pdf2MarkdownOpInputsSchema
  
  
  class Pdf2Markdown(Operator):
      """
      An Operator class for converting PDF to Markdown.
  
      .. warning::
         This operator should be run in a Docker container for safety.
      """
  
      def __init__(self) -> None:
          """
          Initialize the Pdf2Markdown operator.
  
          :param resource_dependencies: Optional list of resource dependencies.
          """
          super().__init__(compute)
  
      def __call__(self, pdf_uri: str, **kwargs: Any) -> str:
          """
          Convert a PDF file to Markdown.
  
          :param pdf_uri: The URI of the PDF file to convert.
          :param kwargs: Additional keyword arguments.
          :return: The converted Markdown content as a string.
          """
          return super().__call__(pdf_uri, **kwargs)
  
      @staticmethod
      def get_function_schema() -> Dict[str, Any]:
          return Pdf2MarkdownOpSchema.get_function_schema()
</file>
<file path="./services/AgentService/flexagent/python/flexagent/ops/python/__init__.py">
  from .python import Python
  
  __all__ = ["Python"]
</file>
<file path="./services/AgentService/flexagent/python/flexagent/ops/python/python.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  
  import os
  import subprocess
  import sys
  import tempfile
  
  from typing import Any, ClassVar, Dict, List, Optional, Type
  
  from pydantic import BaseModel, Field
  
  from flexagent.engine import (
      Operator,
      OperatorInputsSchema,
      OperatorSchema,
      Resource,
      Value,
  )
  
  
  def compute(code: str, **kwargs: Any):
      """
      Execute a Python code snippet in a temporary file and return the results.
  
      This function creates a temporary file with the given Python code, executes it
      using the current Python interpreter, captures the output, and then removes the
      temporary file.
  
      Parameters
      ----------
      code : str
          The Python code to be executed.
  
      Returns
      -------
      dict
          A dictionary containing the following keys:
          - 'stdout': str, the standard output of the executed code.
          - 'stderr': str, the standard error output of the executed code.
          - 'exit_code': int, the exit code of the process.
  
      Examples
      --------
      >>> result = compute("print('Hello, World!')")
      >>> print(result['stdout'])
      Hello, World!
  
      >>> result = compute("import sys; print(sys.version)")
      >>> print(result['stdout'])
      3.8.10 (default, May 26 2023, 14:05:08)
      [GCC 9.4.0]
  
      >>> result = compute("1/0")
      >>> print(result['stderr'])
      Traceback (most recent call last):
        File "<string>", line 1, in <module>
      ZeroDivisionError: division by zero
      """
      timeout = kwargs.pop("timeout", 360)
      # create a temp file to write the program
      with tempfile.NamedTemporaryFile(delete=False, suffix=".py") as temp_file:
          temp_file.write(code.encode("utf-8"))
          temp_file_path = temp_file.name
      # run the program
      process = subprocess.Popen(
          [sys.executable, temp_file_path],
          stdout=subprocess.PIPE,
          stderr=subprocess.PIPE,
      )
      try:
          stdout, stderr = process.communicate(timeout=timeout)
      except subprocess.TimeoutExpired:
          process.kill()
          stdout, stderr = process.communicate()
          stderr = "Timeout error: The code execution exceeded the time limit.".encode(
              "utf-8"
          )
      # check the exit code
      exit_code = process.returncode
      # remove the temp file
      os.remove(temp_file_path)
      return {
          "stdout": stdout.decode("utf-8"),
          "stderr": stderr.decode("utf-8"),
          "exit_code": exit_code,
      }
  
  
  class PythonOpInputSchema(OperatorInputsSchema):
      code: str = Field(..., description="The Python code to be executed.")
  
  
  class PythonOpSchema(OperatorSchema):
      name: ClassVar[str] = "python"
      description: ClassVar[str] = (
          "Execute a Python code snippet in a temporary file and return the results."
      )
      parameters: ClassVar[Type[BaseModel]] = PythonOpInputSchema
  
  
  class Python(Operator):
      """
      An Operator class for executing Python code.
  
      This class extends the Operator class to provide functionality for
      executing Python code snippets.
  
      Parameters
      ----------
      resource_dependencies : Optional[List[Resource]], optional
          A list of Resource objects that this operator depends on.
  
      Examples
      --------
      >>> python_op = Python()
      >>> result = python_op("print('Hello from Python Operator!')")
      >>> print(result['stdout'])
      Hello from Python Operator!
  
      >>> python_op_with_deps = Python(resource_dependencies=[some_resource])
      >>> result = python_op_with_deps("import some_module; print(some_module.version)")
      >>> print(result['stdout'])
      1.2.3
      """
  
      def __init__(self, timeout: int = 360):
          super().__init__(compute)
          self.timeout = timeout
  
      def __call__(
          self, *args: Any, resources: Optional[List[Resource]] = None, **kwargs: Any
      ) -> Value:
          return super().__call__(
              *args, timeout=self.timeout, resources=resources, **kwargs
          )
  
      @staticmethod
      def get_function_schema() -> Dict[str, Any]:
          return PythonOpSchema.get_function_schema()
</file>
<file path="./services/AgentService/flexagent/python/flexagent/ops/search/__init__.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  # flake8: noqa
  from .google import GoogleSearch
  
  __all__ = ["GoogleSearch"]
</file>
<file path="./services/AgentService/flexagent/python/flexagent/ops/search/google.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  import logging
  import os
  from typing import Any, ClassVar, Dict, List, Optional, Type
  
  import requests
  from pydantic import BaseModel, Field
  from requests.exceptions import RequestException
  
  from flexagent.engine import (
      Operator,
      OperatorInputsSchema,
      OperatorSchema,
      Resource,
      Value,
  )
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  
  def google_custom_search_compute(
      query: str, subscription_key: str, cx: str, **kwargs: Any
  ) -> Dict[str, Any]:
      """
      Perform a Google Custom Search and return the search contexts.
  
      Parameters
      ----------
      query : str
          The search query string.
      subscription_key : str
          Your Google Custom Search API subscription key.
      cx : str
          The Custom Search Engine ID to scope this search.
      timeout : int, optional
          The request timeout in seconds (default is 5).
  
      Returns
      -------
      dict
          A dictionary containing the search results with keys:
          - 'contexts': List of search results with 'name', 'url', and 'snippet'.
  
      Raises
      ------
      ValueError
          If there's an error during the search.
      """
      if subscription_key == "" or cx == "":
          raise ValueError("Subscription key and Custom Search Engine ID are required.")
      timeout = kwargs.get("timeout", 10)
      num = kwargs.get("num", 8)  # REFERENCE_COUNT
      search_endpoint = "https://customsearch.googleapis.com/customsearch/v1"
      params = {
          "key": subscription_key,
          "cx": cx,
          "q": query,
          "num": num,
      }
      logger.info(f"Searching for {query}")
  
      try:
          response = requests.get(search_endpoint, params=params, timeout=timeout)
          if not response.ok:
              logger.error(
                  f"Google Search API Error {response.status_code}: {response.text}"
              )
              raise RequestException(f"Google Search API error: {response.status_code}")
  
          json_content = response.json()
          contexts = json_content.get("items", [])[:num]
  
          formatted_contexts = [
              {
                  "name": item.get("title", ""),
                  "url": item.get("link", ""),
                  "snippet": item.get("snippet", ""),
              }
              for item in contexts
          ]
  
          return {"contexts": formatted_contexts}
  
      except RequestException as e:
          logger.error(f"Request exception during Google Custom Search: {e}")
          raise ValueError(f"Error during search: {str(e)}")
      except KeyError as e:
          logger.error(f"Key error processing Google Custom Search response: {e}")
          return {"contexts": []}
  
  
  class GoogleSearchOpInputsSchema(OperatorInputsSchema):
      query: str = Field(..., description="The search query string.")
  
  
  class GoogleSearchOpSchema(OperatorSchema):
      name: ClassVar[str] = "google_search"
      description: ClassVar[str] = (
          "Perform a Google Custom Search and return the search contexts."
      )
      parameters: ClassVar[Type[BaseModel]] = GoogleSearchOpInputsSchema
  
  
  class GoogleSearch(Operator):
      """
      An Operator class for executing Google Custom Search.
  
      This class extends the Operator class to provide functionality for
      performing Google Custom Searches.
  
      Parameters
      ----------
      subscription_key : str
          Your Google Custom Search API subscription key.
      cx : str
          The Custom Search Engine ID to scope the search.
      timeout : int, optional
          The request timeout in seconds (default is 5).
  
      Examples
      --------
      >>> google_search_op = GoogleCustomSearchOperator(
      ...     subscription_key="your_subscription_key",
      ...     cx="your_search_engine_id"
      ... )
      >>> result = google_search_op(query="OpenAI GPT-4")
      >>> print(result['contexts'])
      [{'name': '...', 'url': '...', 'snippet': '...'}, ...]
      """
  
      def __init__(self, subscription_key: str = "", cx: str = "", timeout: int = 10):
          """
          Initializes the GoogleCustomSearchOperator with necessary credentials.
  
          Parameters
          ----------
          subscription_key : str
              Your Google Custom Search API subscription key.
          cx : str
              The Custom Search Engine ID to scope the search.
          timeout : int, optional
              The request timeout in seconds (default is 10).
          """
          compute_func = google_custom_search_compute
          super().__init__(compute_func)
          if subscription_key == "":
              subscription_key = os.getenv("GOOGLE_CUSTOM_SEARCH_API_KEY", "")
          if cx == "":
              cx = os.getenv("GOOGLE_CUSTOM_SEARCH_ENGINE_ID", "")
          self.subscription_key = subscription_key
          self.cx = cx
          self.timeout = timeout
  
      def __call__(
          self, query: str, resources: Optional[List[Resource]] = None, **kwargs: Any
      ) -> Value:
          """
          Executes the Google Custom Search with the given query.
  
          Parameters
          ----------
          query : str
              The search query string.
          resources : Optional[List[Resource]], optional
              A list of Resource objects that this operator depends on.
          **kwargs : Any
              Additional keyword arguments.
  
          Returns
          -------
          Value
              The search result containing contexts.
          """
          return super().__call__(
              query=query,
              subscription_key=self.subscription_key,
              cx=self.cx,
              timeout=self.timeout,
              resources=resources,
              **kwargs,
          )
  
      @staticmethod
      def get_function_schema() -> Dict[str, Any]:
          return GoogleSearchOpSchema.get_function_schema()
</file>
<file path="./services/AgentService/flexagent/python/flexagent/py.typed">
</file>
<file path="./services/AgentService/flexagent/python/flexagent/utils/__init__.py">
  # Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
  
  from .json_func import extract_json
  from .kvstore import KVStore
  
  
  __all__ = ["extract_json", "KVStore"]
</file>
<file path="./services/AgentService/flexagent/python/flexagent/utils/json_func.py">
  # Copyright (c) 2024, NVIDIA CORPORATION.  All rights reserved.
  
  import json
  import re
  
  
  def find_json_objects(text):
      """
      Finds all substrings in the text that are potential JSON objects.
  
      Parameters:
      text (str): The input text to search for JSON objects.
  
      Returns:
      list: A list of strings, each a candidate JSON substring.
      """
      potential_jsons = []
      brace_stack = []
      start_idx = None
  
      for idx, char in enumerate(text):
          if char == "{":
              if not brace_stack:
                  start_idx = idx
              brace_stack.append("{")
          elif char == "}":
              if brace_stack:
                  brace_stack.pop()
                  if not brace_stack and start_idx is not None:
                      end_idx = idx + 1
                      json_str = text[start_idx:end_idx]
                      potential_jsons.append(json_str)
  
      return potential_jsons
  
  
  def find_json_arrays(text):
      """
      Finds all substrings in the text that are potential JSON arrays.
  
      Parameters:
      text (str): The input text to search for JSON arrays.
  
      Returns:
      list: A list of strings, each a candidate JSON array substring.
      """
      potential_jsons = []
      bracket_stack = []
      start_idx = None
  
      for idx, char in enumerate(text):
          if char == "[":
              if not bracket_stack:
                  start_idx = idx
              bracket_stack.append("[")
          elif char == "]":
              if bracket_stack:
                  bracket_stack.pop()
                  if not bracket_stack and start_idx is not None:
                      end_idx = idx + 1
                      json_str = text[start_idx:end_idx]
                      potential_jsons.append(json_str)
  
      return potential_jsons
  
  
  def extract_json(text):
      """
      Extracts the first valid JSON string found in the input text.
  
      The function handles JSON data that may be:
      - Enclosed in ```json ... ``` code blocks
      - Enclosed in ``` ... ``` code blocks
      - Directly present in the text without any enclosing
  
      Parameters:
      text (str): The input text containing JSON data.
  
      Returns:
      str: The JSON string if found and valid, otherwise None.
      """
      # Pattern to match code blocks enclosed in triple backticks, with or without 'json' qualifier
      code_block_pattern = r"```(?:json)?\s*(.*?)\s*```"
      code_blocks = re.findall(code_block_pattern, text, re.DOTALL)
  
      # Try to find JSON from code blocks first
      for block in code_blocks:
          json_str = block.strip()
  
          # Remove any leading lines that are 'json' or similar language identifiers
          lines = json_str.splitlines()
          while lines and lines[0].strip().lower() in ['json', 'javascript']:
              lines.pop(0)
          json_str = '\n'.join(lines).strip()
  
          try:
              # Validate JSON string without parsing it into an object
              json.loads(json_str)
              return json_str
          except json.JSONDecodeError:
              continue
  
      # If no valid JSON in code blocks, search for JSON arrays and then JSON objects in the entire text
      potential_jsons = find_json_arrays(text) + find_json_objects(text)
      for json_str in potential_jsons:
          try:
              # Validate JSON string
              json.loads(json_str)
              return json_str
          except json.JSONDecodeError:
              continue
  
      # Return None if no valid JSON string is found
      return None
</file>
<file path="./services/AgentService/flexagent/python/flexagent/utils/kvstore.py">
  # Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
  
  """
  Module for a thread-safe key-value store backed by SQLite.
  """
  
  import atexit
  import queue
  import sqlite3
  import threading
  from dataclasses import dataclass
  from enum import auto, Enum
  from typing import Any, Optional
  
  
  class ChangeType(Enum):
      """Enumeration of possible change types in the key-value store."""
  
      INSERT = auto()
      DELETE = auto()
      UPDATE = auto()
  
  
  @dataclass
  class Change:
      """
      Represents a change in the key-value store.
  
      Attributes:
          change_type (ChangeType): The type of change (INSERT, DELETE, UPDATE).
          key (str): The key affected by the change.
          value (Optional[Any]): The new value associated with the key, if applicable.
      """
  
      change_type: ChangeType
      key: str
      value: Optional[Any] = None  # None for delete
  
  
  class KVStore:
      """
      Singleton class for a thread-safe key-value store backed by SQLite.
  
      This class provides methods to set, get, and delete key-value pairs.
      Changes are queued and processed by a background thread to synchronize
      with the SQLite database.
  
      Attributes:
          store (dict): In-memory store for key-value pairs.
          lock (threading.Lock): Lock to ensure thread-safe operations on the store.
          change_queue (queue.Queue): Queue to hold pending changes to be written to the database.
          db_path (str): Path to the SQLite database file.
          stop_event (threading.Event): Event to signal the worker thread to stop.
          worker_thread (threading.Thread): Background thread that processes the change queue.
      """
  
      _instances = {}
      _lock = threading.Lock()
  
      def __new__(cls, db_path: str, singleton=True):
          if not singleton:
              instance = super().__new__(cls)
              instance._initialized = False
              return instance
  
          with cls._lock:
              if db_path not in cls._instances:
                  instance = super().__new__(cls)
                  instance._initialized = False
                  cls._instances[db_path] = instance
              return cls._instances[db_path]
  
      def __init__(self, db_path: str, singleton=True):
          if singleton and self._initialized:
              return
          self.store = {}
          self.lock = threading.Lock()
          self.change_queue = queue.Queue()
          self.db_path = db_path
          self.stop_event = threading.Event()
          self.worker_thread = threading.Thread(target=self._worker, daemon=True)
          self._initialize_db()
          self._load_from_db()
          self.worker_thread.start()
          atexit.register(self.close)
          self._initialized = True
  
      def _initialize_db(self):
          """
          Initialize the SQLite database and create the kv_store table if it doesn't exist.
  
          This method sets up the necessary table structure in the database to store key-value pairs.
          """
          conn = sqlite3.connect(self.db_path)
          cursor = conn.cursor()
          cursor.execute(
              """
              CREATE TABLE IF NOT EXISTS kv_store (
                  key TEXT PRIMARY KEY,
                  value BLOB
              )
          """
          )
          conn.commit()
          conn.close()
  
      def _load_from_db(self):
          """
          Load existing key-value pairs from SQLite into the in-memory dictionary.
  
          This method reads all key-value pairs from the database and populates the in-memory store.
          """
          conn = sqlite3.connect(self.db_path)
          cursor = conn.cursor()
          cursor.execute("SELECT key, value FROM kv_store")
          rows = cursor.fetchall()
          with self.lock:
              for key, value in rows:
                  self.store[key] = value
          conn.close()
  
      def set(self, key: str, value: Any):
          """
          Set the value for a given key.
  
          If the key already exists, its value is updated; otherwise, a new key-value pair is inserted.
  
          Args:
              key (str): The key to set.
              value (Any): The value to associate with the key.
          """
          with self.lock:
              if key in self.store:
                  change_type = ChangeType.UPDATE
              else:
                  change_type = ChangeType.INSERT
              self.store[key] = value
              change = Change(change_type=change_type, key=key, value=value)
              self.change_queue.put(change)
  
      def get(self, key: str) -> Optional[Any]:
          """
          Retrieve the value for a given key.
  
          Args:
              key (str): The key to retrieve.
  
          Returns:
              Optional[Any]: The value associated with the key, or None if the key does not exist.
          """
          with self.lock:
              return self.store.get(key, None)
  
      def delete(self, key: str):
          """
          Delete a key-value pair.
  
          If the key exists, it is removed from the store and queued for deletion from the database.
  
          Args:
              key (str): The key to delete.
          """
          with self.lock:
              if key in self.store:
                  del self.store[key]
                  change = Change(change_type=ChangeType.DELETE, key=key)
                  self.change_queue.put(change)
  
      def _worker(self):
          """
          Background thread that processes changes and syncs them to SQLite.
  
          This method runs in a separate thread, continuously processing changes from the queue
          and applying them to the database. It ensures that all changes are persisted.
          """
          conn = sqlite3.connect(self.db_path)
          cursor = conn.cursor()
          while not self.stop_event.is_set() or not self.change_queue.empty():
              try:
                  # Wait for a change for a short timeout to allow graceful shutdown
                  change = self.change_queue.get(timeout=0.5)
                  if change.change_type == ChangeType.INSERT:
                      cursor.execute(
                          """
                          INSERT INTO kv_store (key, value) VALUES (?, ?)
                      """,
                          (change.key, change.value),
                      )
                  elif change.change_type == ChangeType.UPDATE:
                      cursor.execute(
                          """
                          UPDATE kv_store SET value = ? WHERE key = ?
                      """,
                          (change.value, change.key),
                      )
                  elif change.change_type == ChangeType.DELETE:
                      cursor.execute(
                          """
                          DELETE FROM kv_store WHERE key = ?
                      """,
                          (change.key,),
                      )
                  conn.commit()
                  self.change_queue.task_done()
              except queue.Empty:
                  continue
              except Exception as e:
                  print(f"Error processing change {change}: {e}")
          conn.close()
  
      def flush(self):
          """
          Wait for all pending changes to be written to SQLite.
  
          This method blocks until all changes in the queue have been processed and persisted.
          """
          self.change_queue.join()
  
      def close(self):
          """
          Stop the background worker and flush all changes.
  
          This method signals the worker thread to stop, waits for it to finish,
          and ensures all pending changes are written to the database.
          """
          self.stop_event.set()
          self.worker_thread.join()
          self.flush()
</file>
<file path="./services/AgentService/flexagent/python/pyproject.toml">
  [build-system]
  requires = ["hatchling"]
  build-backend = "hatchling.build"
  
  [tool.hatch.version]
  path = "flexagent/__init__.py"
  
  [project]
  name = "flexagent"
  dynamic = ["version"]
  dependencies = ["jinja2==3.1.4", "python-dotenv", "openai", "pydantic", "dill", "requests"]
  requires-python = ">= 3.9"
  authors = [
      {name = "Bing Xu", email = "bixu@nvidia.com"}
  ]
  maintainers = [
      {name = "Bing Xu", email = "bixu@nvidia.com"}
  ]
  
  [project.optional-dependencies]
  all = ["docling==1.13.0"]
  
  [tool.setuptools.package-data]
  "flexagent" = ["py.typed"]
</file>
<file path="./services/AgentService/main.py">
  from fastapi import FastAPI, BackgroundTasks, HTTPException
  from shared.shared_types import ServiceType, JobStatus, Conversation
  from shared.storage import StorageManager
  from shared.job import JobStatusManager
  import flexagent as fa
  from flexagent.backend import BackendConfig
  from flexagent.engine import Value
  from pydantic import BaseModel
  from pathlib import Path
  from dataclasses import dataclass
  from typing import List, Dict, Optional, Any
  import json
  import os
  import logging
  import time
  from prompts import (
      RAW_OUTLINE_PROMPT,
      OUTLINE_PROMPT,
      SEGMENT_TRANSCRIPT_PROMPT,
      DEEP_DIVE_PROMPT,
      RAW_PODCAST_DIALOGUE_PROMPT_v2,
      FUSE_OUTLINE_PROMPT,
      REVISE_PROMPT,
      PODCAST_DIALOGUE_PROMPT,
  )
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  
  # Data Models
  
  
  class PodcastSegment(BaseModel):
      section: str
      descriptions: List[str]
      duration: int
  
  
  class PodcastOutline(BaseModel):
      title: str
      segments: List[PodcastSegment]
  
  
  class TranscriptionRequest(BaseModel):
      markdown: str
      duration: int = 20
      speaker_1_name: str = "Bob"
      speaker_2_name: str = "Kate"
      model: str = "meta/llama-3.1-405b-instruct"
      job_id: str
  
  
  @dataclass
  class ModelConfig:
      name: str
      api_base: str
      backend_type: str = "nim"
  
      @classmethod
      def from_dict(cls, data: Dict[str, Any]) -> "ModelConfig":
          return cls(
              name=data["name"],
              api_base=data["api_base"],
              backend_type=data.get("backend_type", "nim"),
          )
  
  
  class LLMManager:
      DEFAULT_CONFIGS = {
          "reasoning": {
              "name": "meta/llama-3.1-405b-instruct",
              "api_base": "https://integrate.api.nvidia.com/v1",
              "backend_type": "nim",
          },
          "subsegments": {
              "name": "meta/llama-3.1-405b-instruct",
              "api_base": "https://integrate.api.nvidia.com/v1",
              "backend_type": "nim",
          },
          "json": {
              "name": "meta/llama-3.1-70b-instruct",
              "api_base": "https://nim-pc8kmx5ae.brevlab.com/v1",
              "backend_type": "nim",
          },
      }
  
      def __init__(self, api_key: str, config_path: Optional[str] = None):
          self.api_key = api_key
          self._llm_cache: Dict[str, fa.ops.LLM] = {}
          self.model_configs = self._load_configurations(config_path)
  
      def _load_configurations(
          self, config_path: Optional[str]
      ) -> Dict[str, ModelConfig]:
          """Load model configurations from JSON file if provided, otherwise use defaults"""
          configs = self.DEFAULT_CONFIGS.copy()
  
          if config_path:
              try:
                  config_path = Path(config_path)
                  if config_path.exists():
                      with config_path.open() as f:
                          custom_configs = json.load(f)
                      configs.update(custom_configs)
                  else:
                      logger.warning(
                          f"Config file {config_path} not found, using default configurations"
                      )
              except Exception as e:
                  logger.error(f"Error loading config file: {e}")
                  logger.warning("Using default configurations")
  
          return {key: ModelConfig.from_dict(config) for key, config in configs.items()}
  
      def get_llm(self, model_key: str) -> fa.ops.LLM:
          """Get or create an LLM instance for the specified model key"""
          if model_key not in self.model_configs:
              raise ValueError(f"Unknown model key: {model_key}")
  
          if model_key not in self._llm_cache:
              config = self.model_configs[model_key]
              backend = BackendConfig(
                  backend_type=config.backend_type,
                  model_name=config.name,
                  api_key=self.api_key,
                  api_base=config.api_base,
              )
              self._llm_cache[model_key] = fa.ops.LLM().to(backend)
  
          return self._llm_cache[model_key]
  
      def query(
          self,
          model_key: str,
          messages: List[Dict[str, str]],
          json_schema: Optional[Dict] = None,
          sync: bool = True,
          retries: int = 5,
      ) -> Any:
          """Send a query to the specified model with retry logic"""
          llm = self.get_llm(model_key)
  
          for attempt in range(retries):
              try:
                  extra_body = (
                      {"nvext": {"guided_json": json_schema}} if json_schema else None
                  )
                  response = llm(messages, extra_body=extra_body)
                  return response.get() if sync else response
  
              except Exception as e:
                  logger.error(f"Attempt {attempt + 1}/{retries} failed: {str(e)}")
                  if attempt == retries - 1:
                      raise Exception(
                          f"Failed to get response after {retries} attempts"
                      ) from e
                  time.sleep(3)
  
  
  class PromptTracker:
      """Track prompts and responses and save them to storage"""
  
      def __init__(self, job_id: str):
          self.job_id = job_id
          self.steps = []
  
      def track(self, step_name: str, prompt: str, response: str, model: str):
          self.steps.append(
              {
                  "step_name": step_name,
                  "prompt": prompt,
                  "response": response,
                  "model": model,
                  "timestamp": time.time(),
              }
          )
          logger.info(f"Tracked step {step_name} for {self.job_id}")
  
      def save(self, storage_manager: StorageManager):
          storage_manager.store_file(
              self.job_id,
              json.dumps({"steps": self.steps}).encode(),
              f"{self.job_id}_prompt_tracker.json",
              "application/json",
          )
          logger.info(
              f"Stored prompt tracker for {self.job_id} in minio. Length: {len(self.steps)}"
          )
  
  
  # FastAPI Application
  app = FastAPI(debug=True)
  job_manager = JobStatusManager(ServiceType.AGENT)
  storage_manager = StorageManager()
  
  
  def process_transcription(job_id: str, request: TranscriptionRequest):
      try:
          llm_manager = LLMManager(
              api_key=os.getenv("NIM_KEY"),
              config_path=os.getenv("MODEL_CONFIG_PATH"),
          )
  
          prompt_tracker = PromptTracker(job_id)
  
          # Initialize processing
          job_manager.update_status(
              job_id, JobStatus.PROCESSING, "Initializing processing"
          )
          schema = PodcastOutline.model_json_schema()
  
          # Generate initial outline
          job_manager.update_status(
              job_id, JobStatus.PROCESSING, "Generating initial outline"
          )
          prompt = RAW_OUTLINE_PROMPT.render(
              text=request.markdown, duration=request.duration
          )
          raw_outline = llm_manager.query(
              "reasoning", [{"role": "user", "content": prompt}]
          )
          prompt_tracker.track(
              "raw_outline",
              prompt,
              raw_outline,
              llm_manager.model_configs["reasoning"].name,
          )
  
          # Convert to structured format
          job_manager.update_status(
              job_id, JobStatus.PROCESSING, "Converting raw outline to structured format"
          )
          prompt = OUTLINE_PROMPT.render(
              text=raw_outline, schema=json.dumps(schema, indent=2)
          )
          outline = llm_manager.query(
              "json", [{"role": "user", "content": prompt}], json_schema=schema
          )
          prompt_tracker.track(
              "outline", prompt, outline, llm_manager.model_configs["json"].name
          )
          outline_json = json.loads(outline)
  
          # Process segments
          longest_segment_idx = max(
              range(len(outline_json["segments"])),
              key=lambda i: outline_json["segments"][i]["duration"],
          )
  
          segments = []
          sub_outline = {}
          for idx, segment in enumerate(outline_json["segments"]):
              job_manager.update_status(
                  job_id,
                  JobStatus.PROCESSING,
                  f"Processing segment {idx + 1}/{len(outline_json['segments'])}: {segment['section']}",
              )
  
              if idx == longest_segment_idx:
                  ret = deep_dive_segment(
                      job_id,
                      request.markdown,
                      segment,
                      llm_manager,
                      schema,
                      prompt_tracker,
                  )
                  segments.append(ret[0])
                  sub_outline = ret[1]
              else:
                  prompt = SEGMENT_TRANSCRIPT_PROMPT.render(
                      text=request.markdown,
                      duration=segment["duration"],
                      topic=segment["section"],
                      angles="\n".join(segment["descriptions"]),
                  )
                  seg_response = llm_manager.query(
                      "reasoning", [{"role": "user", "content": prompt}], sync=False
                  )
                  segments.append(seg_response)
                  prompt_tracker.track(
                      f"segment_transcript_{idx}",
                      prompt,
                      seg_response.get(),
                      llm_manager.model_configs["reasoning"].name,
                  )
  
          # Generate dialogue
          segment_transcripts = []
          for idx, segment in enumerate(outline_json["segments"]):
              job_manager.update_status(
                  job_id,
                  JobStatus.PROCESSING,
                  f"Converting segment {idx + 1}/{len(outline_json['segments'])} to dialogue",
              )
              prompt = RAW_PODCAST_DIALOGUE_PROMPT_v2.render(
                  text=segments[idx].get(),
                  duration=segment["duration"],
                  descriptions=segment["descriptions"],
                  speaker_1_name=request.speaker_1_name,
                  speaker_2_name=request.speaker_2_name,
              )
              seg_response = llm_manager.query(
                  "reasoning", [{"role": "user", "content": prompt}], sync=False
              )
              segment_transcripts.append(seg_response)
  
          # Combine transcripts
          job_manager.update_status(job_id, JobStatus.PROCESSING, "Combining segments")
          full_transcript = "\n".join([segment.get() for segment in segments])
          conversation = "\n".join([segment.get() for segment in segment_transcripts])
  
          # Track each segment transcript
          for idx, segment in enumerate(segments):
              prompt_tracker.track(
                  f"raw_podcast_dialogue_v2_segment_{idx}",
                  prompt,
                  segment.get(),
                  llm_manager.model_configs["reasoning"].name,
              )
  
          # Fuse outline
          job_manager.update_status(job_id, JobStatus.PROCESSING, "Fusing outline")
          prompt = FUSE_OUTLINE_PROMPT.render(
              overall_outline=outline, sub_outline=sub_outline
          )
          full_outline = llm_manager.query(
              "reasoning", [{"role": "user", "content": prompt}]
          )
          prompt_tracker.track(
              "fuse_outline",
              prompt,
              full_outline,
              llm_manager.model_configs["reasoning"].name,
          )
  
          # Revise dialogue
          job_manager.update_status(job_id, JobStatus.PROCESSING, "Revising dialogue")
          prompt = REVISE_PROMPT.render(
              raw_transcript=full_transcript,
              dialogue_transcript=conversation,
              outline=full_outline,
          )
          conversation = llm_manager.query(
              "reasoning", [{"role": "user", "content": prompt}]
          )
          prompt_tracker.track(
              "revise_dialogue",
              prompt,
              conversation,
              llm_manager.model_configs["reasoning"].name,
          )
  
          # Convert to final JSON format
          schema = Conversation.model_json_schema()
          job_manager.update_status(
              job_id, JobStatus.PROCESSING, "Converting to final format"
          )
          prompt = PODCAST_DIALOGUE_PROMPT.render(
              text=conversation,
              schema=json.dumps(schema, indent=2),
              speaker_1_name=request.speaker_1_name,
              speaker_2_name=request.speaker_2_name,
          )
          final_conversation = llm_manager.query(
              "json", [{"role": "user", "content": prompt}], json_schema=schema
          )
          prompt_tracker.track(
              "final_conversation",
              prompt,
              final_conversation,
              llm_manager.model_configs["json"].name,
          )
  
          # Store result
          result = json.loads(final_conversation)
          # Expire the result after 2 minutes
          job_manager.set_result_with_expiration(
              job_id, json.dumps(result).encode(), ex=120
          )
  
          prompt_tracker.save(storage_manager)
  
          job_manager.update_status(
              job_id, JobStatus.COMPLETED, "Transcription completed successfully"
          )
  
      except Exception as e:
          logger.error(f"Error processing job {job_id}: {str(e)}")
          job_manager.update_status(job_id, JobStatus.FAILED, str(e))
          raise
  
  
  def deep_dive_segment(
      job_id: str,
      text: str,
      segment: Dict[str, str],
      llm_manager: LLMManager,
      schema: Dict,
      prompt_tracker: PromptTracker,
  ) -> tuple[Value, Dict]:
      status_msg = f"Performing deep dive analysis of segment: {segment['section']}"
      job_manager.update_status(job_id, JobStatus.PROCESSING, status_msg)
      logger.info(f"Job {job_id}: {status_msg}")
  
      prompt = DEEP_DIVE_PROMPT.render(
          text=text, topic=segment["descriptions"], duration=segment["duration"]
      )
      outline = llm_manager.query("reasoning", [{"role": "user", "content": prompt}])
      prompt_tracker.track(
          "deep_dive_outline",
          prompt,
          outline,
          llm_manager.model_configs["reasoning"].name,
      )
  
      prompt = OUTLINE_PROMPT.render(text=outline, schema=json.dumps(schema, indent=2))
      outline_response = llm_manager.query(
          "json", [{"role": "user", "content": prompt}], json_schema=schema
      )
      prompt_tracker.track(
          "deep_dive_outline_json",
          prompt,
          outline_response,
          llm_manager.model_configs["json"].name,
      )
      outline_json = json.loads(outline_response)
  
      segments = []
      for subsegment in outline_json["segments"]:
          job_manager.update_status(
              job_id,
              JobStatus.PROCESSING,
              f"Processing subsegment: {subsegment['section']}",
          )
          prompt = SEGMENT_TRANSCRIPT_PROMPT.render(
              text=text,
              duration=subsegment["duration"],
              topic=subsegment["section"],
              angles="\n".join(subsegment["descriptions"]),
          )
          seg_response = llm_manager.query(
              "subsegments", [{"role": "user", "content": prompt}], sync=False
          )
          segments.append(seg_response)
          prompt_tracker.track(
              f"deep_dive_segment_transcript_{subsegment['section'].replace(' ', '_')}",
              prompt,
              seg_response.get(),
              llm_manager.model_configs["subsegments"].name,
          )
  
      texts = [segment.get() for segment in segments]
      return (Value("\n".join(texts)), outline_json)
  
  
  # API Endpoints
  @app.post("/transcribe", status_code=202)
  def transcribe(request: TranscriptionRequest, background_tasks: BackgroundTasks):
      job_manager.create_job(request.job_id)
      background_tasks.add_task(process_transcription, request.job_id, request)
      return {"job_id": request.job_id}
  
  
  @app.get("/status/{job_id}")
  def get_status(job_id: str):
      return job_manager.get_status(job_id)
  
  
  @app.get("/output/{job_id}")
  def get_output(job_id: str):
      result = job_manager.get_result(job_id)
      if result is None:
          raise HTTPException(status_code=404, detail="Result not found")
      return json.loads(result.decode())
  
  
  @app.get("/transcribe/health")
  def health():
      return {
          "status": "healthy",
          "version": fa.__version__,
      }
  
  
  if __name__ == "__main__":
      import uvicorn
  
      uvicorn.run(app, host="0.0.0.0", port=8964)
</file>
<file path="./services/AgentService/prompts.py">
  import jinja2
  
  RAW_OUTLINE_PROMPT = jinja2.Template("""
  I want to make the following paper into a podcast transcript for {{ duration }} minutes, to help audience understand background, innovation, impact and future work. 
  
  Come up the structure of the podcast.
                                   
  {{ text }}
                                       
  Innovation should be the focus of the podcast, and the most important part of the podcast, with enough details.
  """)
  
  OUTLINE_PROMPT = jinja2.Template("""
  Given the free form outline, convert in into a structured outline without losing any information.                                 
  
  {{ text }}
                                                             
  The result must conform to the following JSON schema:\n{{ schema }}\n\n
  """)
  
  SEGMENT_TRANSCRIPT_PROMPT = jinja2.Template("""
  Make a transcript given the text:
  
  {{ text }}
                                              
  The transcript is about {{ duration }} minutes, approximately {{ (duration * 180) | int }} words.
  The transcript's subject is {{ topic }}, and should focus on the following topics: {{ angles }}
                                              
  Explain all concepts clearly, assuming no prior knowledge
  Use analogies, stories, or examples to illustrate points
  Address potential questions or counterarguments
  Provide context and background information throughout
  Make sure the details, numbers are accurate and comprehensive
                                              
  Dive deep into each topic, and provide enough details given the time budget, don't leave any stone unturned.
  """)
  
  DEEP_DIVE_PROMPT = jinja2.Template("""
  You will be given some content, short ideas or thoughts about the content.
  
  Your task is to expand the content into a detailed and comprehensive explanation, with enough details and examples.
  
  Here is the content
  
  {{text}}
                                     
  The topic will be around
                                     
  {{topic}}
                                     
  Dive deep into each topic, come up with an outline with topics and subtopics to help fully understand the content.
  Expand the topics, don't add any other topics. Allocate time budget for each topic. Total time budget should be {{ duration }} minutes.
  Focus on the most important topics and ideas, and allocate more time budget to them.
  Avoid introduction and conclusion in the outline, focus on expanding into subtopics.
  """)
  
  TRANSCRIPT_PROMPT = jinja2.Template("""
  Given the transcript of different segments,combine and optimize the transcript to make the flow more natural.
  The content should be strictly following the transcript, and only optimize the flow. Keep all the details, and storytelling contents.
  For each segment, there is also a time budget for reference.
  
  {% for segment, duration in segments %}
  
  Time budget: {{ duration }} minutes, approximately {{ (duration * 180) | int }} words.
  {{ segment }}                                    
  
  {% endfor %}
                                      
  Only return the full transcript, no need to include any other information like time budget or segment name.
  """)
  
  RAW_PODCAST_DIALOGUE_PROMPT_v2 = jinja2.Template("""
  Your task is to transform the provided input transcript into a lively, engaging, and informative podcast dialogue. 
  
  There are two speakers, speaker-1 and speaker-2.
  speaker-1's name is {{ speaker_1_name }}, and speaker-2's name is {{ speaker_2_name }}.
  
  Given the following conversation, make introductions for both speakers at beginning of the conversation.
  During the conversation, occasionally mention the speaker's name to refer to them, to make the conversation more natural.
  Incorporate natural speech patterns, including occasional verbal fillers (e.g., "um," "well," "you know")
  Use casual language and ensure the dialogue flows smoothly, reflecting a real-life conversation
  The fillers should be used naturally, not in every sentence, and not in a robotic way but related to topic and conversation context.
                                            
  Maintain a lively pace with a mix of serious discussion and lighter moments
  Use rhetorical questions or hypotheticals to involve the listener
  Create natural moments of reflection or emphasis
       
  Allow for natural interruptions and back-and-forth between host and guest
  Ensure the guest's responses are substantiated by the input text, avoiding unsupported claims                                   
  Avoid long sentences from either speaker, break them into conversations between two speakers.
  Throughout the script, strive for authenticity in the conversation. Include:
     - Moments of genuine curiosity or surprise from the host
     - Instances where the guest might briefly struggle to articulate a complex idea
     - Light-hearted moments or humor when appropriate
     - Brief personal anecdotes or examples that relate to the topic (within the bounds of the input text)
                   
  Don't lose any information or details in the transcript. It is only format conversion, so strictly follow the transcript.
                                                   
  This segment is about {{ duration }} minutes, approximately {{ (duration * 180) | int }} words.
  The topic is {{ descriptions }}
                                            
  You should keep all analogies, stories, examples, and quotes from the transcript.
  
  Here is the transcript:
  {{text}}
                                            
  Only return the full dialogue transcript, no need to include any other information like time budget or segment name.
  Don't add introduction and ending to the dialogue unless it is provided in the transcript.
  """)
  
  FUSE_OUTLINE_PROMPT = jinja2.Template("""
  You are given two outlines, one is overall outline, another is sub-outline for one section in the overall outline.
  You need to fuse the two outlines into a new outline, to represent the whole podcast without losing any descriptions in sub sections.
  Ignore the time budget in the sub-outline, and use the time budget in the overall outline.
  Overall outline:
  {{ overall_outline }}
  
  Sub-outline:
  {{ sub_outline }}
  
  Output the new outline with the tree structure.
  """)
  
  REVISE_PROMPT = jinja2.Template("""
  You are given a podcast dialogue transcript, and a raw transcript of the podcast.
  You are only allowed to copy information from the raw dialogue transcript to make the conversation more natural and engaging, but exactly follow the outline.
                                  
  Outline:
  {{ outline}}
  
  Here is the dialogue transcript:
  {{ dialogue_transcript }}
  
  You need also to break long sentences from either speaker into conversations between two speakers, by inserting more dialogue entries and verbal fillers (e.g., "um")
  Don't let a single speaker talk more than 2 sentences, and break the conversation into multiple exchanges between two speakers.
                                  
  Don't make any explict transition between sections, this is one podcast, and the sections are connected.
  Don't use words like "Welcome back" or "Now we are going to talk about" etc.
  Don't make introductions in the middle of the conversation.
  Merge related topics according to outline and don't repeat same things in different place.
                                  
  Don't lose any information or details from the raw transcript, only make the conversation flow more natural.
  """)
  
  PODCAST_DIALOGUE_PROMPT = jinja2.Template("""
  Given a podcast transcript between two speakers, convert it into a structured JSON format.
  - Only do conversion
  - Don't miss any information in the transcript
  
  There are two speakers, speaker-1 and speaker-2.
  speaker-1's name is {{ speaker_1_name }}, and speaker-2's name is {{ speaker_2_name }}.
                                            
  Here is the original transcript:
  {{ text }}
                                            
  The result must conform to the following JSON schema:\n{{ schema }}\n\n
  """)
</file>
<file path="./services/AgentService/sample.md">
  ## Training Deep Nets with Sublinear Memory Cost
  
  Tianqi Chen $^{1}$, Bing Xu $^{2}$, Chiyuan Zhang $^{3}$, and Carlos Guestrin 1
  
  1 Unveristy of Washington 2 Dato. Inc 3 Massachusetts Institute of Technology
  
  ## Abstract
  
  We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O ( √ n ) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O (log n ) with as little as O ( n log n ) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences.
  
  ## 1 Introduction
  
  In this paper, we propose a systematic approach to reduce the memory consumption of deep neural network training. We mainly focus on reducing the memory cost to store intermediate results (feature maps) and gradients, as the size of the parameters are relatively small comparing to the size of the intermediate feature maps in many common deep architectures. We use a computation graph analysis to do automatic in-place operation and memory sharing optimizations. More importantly, we propose a novel method to trade computation for memory. As a result, we give a practical algorithm that cost O ( √ n ) memory for feature maps to train a n layer network with only double the forward pass computational cost. Interestingly, we also show that in the extreme case, it is possible to use as little as O (log n ) memory for the features maps to train a n layer network.
  
  We have recently witnessed the success of deep neural networks in many domains [8], such as computer vision, speech recognition, natural language processing and reinforcement learning. Many of the success are brought by innovations in new architectures of deep neural networks. Convolutional neural networks [15, 14, 13, 10] model the spatial patterns and give the state of art results in computer vision tasks. Recurrent neural networks, such as long short-term memory [12], show inspiring results in sequence modeling and structure prediction. One common trend in those new models is to use deeper architectures [18, 14, 13, 10] to capture the complex patterns in a large amount of training data. Since the cost of storing feature maps and their gradients scales linearly with the depth of network, our capability of exploring deeper models is limited by the device (usually a GPU) memory. For example, we already run out of memories in one of the current state-of-art models as described in [11]. In the long run, an ideal machine learning system should be able to continuously learn from an increasing amount of training data. Since the optimal model size and complexity often grows with more training data, it is very important to have memory-efficient training algorithms.
  
  Reducing memory consumption not only allows us to train bigger models. It also enables larger batch size for better device utilization and stablity of batchwise operators such as batch normalization [13]. For memory limited devices, it helps improve memory locality and potentially leads to better memory access patterns. It also enables us to switch from model parallelism to data parallelism for training deep convolutional neural networks, which can be beneficial in certain circumstances. Our solution enables us to train deeper convolutional neural networks, as well as recurrent neural networks with longer unrolling steps. We provide guidelines for deep learning frameworks to incorporate the memory optimization techniques proposed in this paper. We will also make our implementation of memory optimization algorithm publicly available.
  
  ## 2 Related Works
  
  We can trace the idea of computational graph and liveness analysis back to the literatures of compiler optimizations [3]. Analogy between optimizing a computer program and optimizing a deep neural network computational graph can be found. For example, memory allocation in deep networks is similar to register allocation in a compiler. The formal analysis of computational graph allows us save memory in a principled way. Theano [5, 4] is a pioneering framework to bring the computation graph to deep learning, which is joined by recently introduced frameworks such as CNTK [2], Tensorflow [1] and MXNet [6]. Theano and Tensorflow use reference count based recycling and runtime garbage collection to manage memory during training, while MXNet uses a static memory allocation strategy prior to the actual computation. However, most of the existing framework focus on graph analysis to optimize computation after the gradient graph is constructed, but do not discuss the computation and memory trade-off.
  
  The trade-off between memory and computation has been a long standing topic in systems research. Although not widely known, the idea of dropping intermediate results is also known as gradient checkpointing technique in automatic differentiation literature [9]. We bring this idea to neural network gradient graph construction for general deep neural networks. Through the discussion with our colleagues [19], we know that the idea of dropping computation has been applied in some limited specific use-cases. In this paper, we propose a general methodology that works for general deep neural networks, including both convolutional and recurrent neural networks. Our results show that it is possible to train a general deep neural network with sublinear memory cost. More importantly, we propose an automatic planning algorithm to provide a good memory plan for real use-cases. The proposed gradient graph optimization algorithm can be readily combined with all the existing memory optimizations in the computational graph to further reduce the memory consumption of deep learning frameworks.
  
  There are other ways to train big models, such as swapping of CPU/GPU memory and use of model parallel training [7, 16]. These are orthogonal approaches and can be used together with our algorithm to train even bigger models with fewer resources. Moreover, our algorithm does not need additional communication over PCI-E and can save the bandwidth for model/data parallel training.
  
  ## 3 Memory Optimization with Computation Graph
  
  We start by reviewing the concept of computation graph and the memory optimization techniques. Some of these techniques are already used by existing frameworks such as Theano [5, 4], Tensorflow [1] and MXNet [6]. A computation graph consists of operational nodes and edges that represent the dependencies between the operations. Fig. 1 gives an example of the computation graph of a two-layer fully connected neural network. Here we use coarse grained forward and backward operations to make the graph simpler. We further simplify the graph by hiding the weight nodes and gradients of the weights. A computation graph used in practice can be more complicated and contains mixture of fine/coarse grained operations. The analysis presented in this paper can be directly used in those more general cases.
  
  Once the network configuration (forward graph) is given, we can construct the corresponding backward pathway for gradient calculation. A backward pathway can be constructed by traversing
  
  Figure 1: Computation graph and possible memory allocation plan of a two layer fully connected neural network training procedure. Each node represents an operation and each edge represents a dependency between the operations. The nodes with the same color share the memory to store output or back-propagated gradient in each operator. To make the graph more clearly, we omit the weights and their output gradient nodes from the graph and assume that the gradient of weights are also calculated during backward operations. We also annotate two places where the in-place and sharing strategies are used.
  
  <!-- image -->
  
  the configuration in reverse topological order, and apply the backward operators as in normal backpropagation algorithm. The backward pathway in Fig. 1 represents the gradient calculation steps explicitly , so that the gradient calculation step in training is simplified to just a forward pass on the entire computation graph (including the gradient calculation pathway). Explicit gradient path also offers some other benefits (e.g. being able to calculate higher order gradients), which is beyond our scope and will not be covered in this paper.
  
  When training a deep convolutional/recurrent network, a great proportion of the memory is usually used to store the intermediate outputs and gradients. Each of these intermediate results corresponds to a node in the graph. A smart allocation algorithm is able to assign the least amount of memory to these nodes by sharing memory when possible. Fig. 1 shows a possible allocation plan of the example two-layer neural network. Two types of memory optimizations can be used
  
  - · Inplace operation : Directly store the output values to memory of a input value.
  - · Memory sharing : Memory used by intermediate results that are no longer needed can be recycled and used in another node.
  
  Allocation plan in Fig. 1 contains examples of both cases. The first sigmoid transformation is carried out using inplace operation to save memory, which is then reused by its backward operation. The storage of the softmax gradient is shared with the gradient by the first fully connected layer. Ad hoc application of these optimizations can leads to errors. For example, if the input of an operation is still needed by another operation, applying inplace operation on the input will lead to a wrong result.
  
  We can only share memory between the nodes whose lifetime do not overlap. There are multiple ways to solve this problem. One option is to construct the conflicting graph of with each variable as node and edges between variables with overlapping lifespan and then run a graph-coloring algorithm. This will cost O ( n $^{2}$) computation time. We adopt a simpler heuristic with only O ( n ) time. The algorithm is demonstrated in Fig. 2. It traverses the graph in topological order, and uses a counter to indicate the liveness of each record. An inplace operation can happen when there is no other pending operations that depend on its input. Memory sharing happens when a recycled tag is used by another node. This can also serve as a dynamic runtime algorithm that traverses the graph, and use a garbage collector to recycle the outdated memory. We use this as a static memory allocation algorithm, to allocate the memory to each node before the execution starts, in order to avoid the overhead of garbage collection during runtime.
  
  Guidelines for Deep Learning Frameworks As we can see from the algorithm demonstration graph in Fig. 2. The data dependency causes longer lifespan of each output and increases the memory
  
  Figure 2: Memory allocation algorithm on computation graph. Each node associated with a liveness counter to count on operations to be full-filled. A temporal tag is used to indicate memory sharing. Inplace operation can be carried out when the current operations is the only one left (input of counter equals 1). The tag of a node can be recycled when the node's counter goes to zero.
  
  <!-- image -->
  
  consumption of big network. It is important for deep learning frameworks to
  
  - · Declare the dependency requirements of gradient operators in minimum manner.
  - · Apply liveness analysis on the dependency information and enable memory sharing.
  
  It is important to declare minimum dependencies. For example, the allocation plan in Fig. 1 won't be possible if sigmoid-backward also depend on the output of the first fullc-forward . The dependency analysis can usually reduce the memory footprint of deep network prediction of a n layer network from O ( n ) to nearly O (1) because sharing can be done between each intermediate results. The technique also helps to reduce the memory footprint of training, although only up to a constant factor.
  
  ## 4 Trade Computation for Memory
  
  ## 4.1 General Methodology
  
  The techniques introduced in Sec. 3 can reduce the memory footprint for both training and prediction of deep neural networks. However, due to the fact that most gradient operators will depend on the intermediate results of the forward pass, we still need O ( n ) memory for intermediate results to train a n layer convolutional network or a recurrent neural networks with a sequence of length n . In order to further reduce the memory, we propose to drop some of the intermediate results , and recover them from an extra forward computation when needed.
  
  More specifically, during the backpropagation phase, we can re-compute the dropped intermediate results by running forward from the closest recorded results. To present the idea more clearly, we show a simplified algorithm for a linear chain feed-forward neural network in Alg. 1. Specifically, the neural network is divided into several segments. The algorithm only remembers the output of each segment and drops all the intermediate results within each segment. The dropped results are recomputed at the segment level during back-propagation. As a result, we only need to pay the memory cost to store the outputs of each segment plus the maximum memory cost to do backpropagation on each segment.
  
  Alg. 1 can also be generalized to common computation graphs as long as we can divide the graph into segments. However, there are two drawbacks on directly applying Alg. 1: 1) users have to manually divide the graph and write customized training loop; 2) we cannot benefit from other memory optimizations presented in Sec 3. We solve this problem by introducing a general gradient graph construction algorithm that uses essentially the same idea. The algorithm is given in Alg. 2. In this algorithm, the user specify a function m : V → $_{N}$on the nodes of a computation graph
  
  Algorithm 1: Backpropagation with Data Dropping in a Linear Chain Network
  
  ```
  v ← input for k = 1 to length ( segments ) do temp [ k ] ← v for i = segments [ k ] .begin to segments [ k ] .end - 1 do v ← layer [ i ] .forward ( v ) end end g ← gradient ( v, label ) for k = length ( segments ) to 1 do v ← temp [ k ] localtemp ← empty hashtable for i = segments [ k ] .begin to segments [ k ] .end - 1 do localtemp [ i ] ← v v ← layer [ i ] .forward ( v ) end for i = segments [ k ] .end - 1 to segments [ k ] .begin do g ← layer [ i ] .backward ( g, localtemp [ i ]) end end
  ```
  
  to indicate how many times a result can be recomputed. We call m the mirror count function as the re-computation is essentially duplicating (mirroring) the nodes. When all the mirror counts are set to 0 , the algorithm degenerates to normal gradient graph. To specify re-computation pattern in Alg. 2, the user only needs to set the m ( v ) = 1 for nodes within each segment and m ( v ) = 0 for the output node of each segment. The mirror count can also be larger than 1, which leads to a recursive generalization to be discussed in Sec 4.4. Fig. 3 shows an example of memory optimized gradient graph. Importantly, Alg. 2 also outputs a traversal order for the computation, so the memory usage can be optimized. Moreover, this traversal order can help introduce control flow dependencies for frameworks that depend on runtime allocation.
  
  ## 4.2 Drop the Results of Low Cost Operations
  
  One quick application of the general methodology is to drop the results of low cost operations and keep the results that are time consuming to compute. This is usually useful in a Conv-BatchNorm-Activation pipeline in convolutional neural networks. We can always keep the result of convolution, but drop the result of the batch normalization, activation function and pooling. In practice this will translate to a memory saving with little computation overhead, as the computation for both batch normalization and activation functions are cheap.
  
  ## 4.3 An O ( √ n ) Memory Cost Algorithm
  
  Alg. 2 provides a general way to trade computation for memory. It remains to ask which intermediate result we should keep and which ones to re-compute. Assume we divide the n network into k segments the memory cost to train this network is given as follows.
  
  cost-total = max i =1 $_{,...,k}$cost-of-segment ( i ) + O ( k ) = O ( n k$^{)}$ + O ( k ) (1)
  
  The first part of the equation is the memory cost to run back-propagation on each of the segment. Given that the segment is equally divided, this translates into O ( n/k ) cost. The second part of equation is the cost to store the intermediate outputs between segments. Setting k = √ n , we get the cost of O (2 √ n ) . This algorithm only requires an additional forward pass during training, but
  
  Figure 3: Memory optimized gradient graph generation example. The forward path is mirrored to represent the re-computation happened at gradient calculation. User specifies the mirror factor to control whether a result should be dropped or kept.
  
  <!-- image -->
  
  ## Algorithm 2: Memory Optimized Gradient Graph Construction
  
  Input : G = ( V, pred ) , input computation graph, the pred [ v ] gives the predecessors array of node v .
  
  Input : gradient(succ grads, output, inputs), symbolic gradient function that creates a gradient node given successor gradients and output and inputs
  
  Input : m : V →$_{N}$ $^{+}$, m ( v ) gives how many time node v should be duplicated, m ( v ) = 0 means do no drop output of node v .
  
  a [ v ] ← v for v ∈ V for k = 1 to max$_{v}$$_{∈}$$_{V}$ m ( v ) do for v in topological-order ( V ) do if k ≤ m ( v ) then a [ v ] ← new node, same operator as v pred [ a [ v ]] ← ∪$_{u}$$_{∈}$ pred [ v $_{]}${ a [ u ] }
  
  end end end V ' ← topological-order ( V ) for v in reverse-topological-order ( V ) do g [ v ] ← gradient ([ g [ v ] for v in successor ( v )] , a [ v ] , [ a [ v ] for v in pred [ v ]]) V ' ← append ( V $^{'}$, topological-order ( acenstors ( g [ v ])) - V $^{'}$) end
  
  Output : G ' = ( V $^{'}$, pred ) the new graph, the order in V ' gives the logical execution order.
  
  reduces the memory cost to be sub-linear . Since the backward operation is nearly twice as time consuming as the forward one, it only slows down the computation by a small amount.
  
  In the most general case, the memory cost of each layer is not the same, so we cannot simply set k = √ n . However, the trade-off between the intermediate outputs and the cost of each stage still holds. In this case, we use Alg. 3 to do a greedy allocation with a given budget for the memory cost within each segment as a single parameter B . Varying B gives us various allocation plans that either assign more memory to the intermediate outputs, or to computation within each stage. When we do static memory allocation, we can get the exact memory cost given each allocation plan. We can use this information to do a heuristic search over B to find optimal memory plan that balances the cost of the two. The details of the searching step is presented in the supplementary material. We find this approach works well in practice. We can also generalize this algorithm by considering the cost to run each operation to try to keep time consuming operations when possible.
  
  Algorithm 3: Memory Planning with Budget
  
  Input : G = ( V, pred ) , input computation graph.
  
  Input : C ⊂ V , candidate stage splitting points, we will search splitting points over v ⊂ C Input : B , approximate memory budget. We can search over B to optimize the memory allocation. temp ← 0 ,x ← 0 ,y ← 0 for v in topological-order ( V ) do temp ← temp + size-of-output ( v ) if v ∈ C and temp > B then x ← x + size-of-output ( v ) , y ← max ( y,temp ) m ( v ) = 0 , temp ← 0 else m ( v ) = 1 end
  
  end
  
  Output :
  
  x approximate cost to store inter-stage feature maps
  
  Output : y approximate memory cost for each sub stage
  
  Output :
  
  m the mirror plan to feed to Alg. 2
  
  Figure 4: Recursion view of the memory optimized allocations. The segment can be viewed as a single operator that combines all the operators within the segment. Inside each operator, a sub-graph as executed to calculate the gradient.
  
  <!-- image -->
  
  ## 4.4 More General View: Recursion and Subroutine
  
  In this section, we provide an alternative view of the memory optimization scheme described above. Specifically, we can view each segment as a bulk operator that combines all the operations inside the segment together. The idea is illustrated in Fig. 4. The combined operator calculates the gradient by executing over the sub-graph that describes its internal computation. This view allows us to treat a series of operations as subroutines. The optimization within the sub-graph does not affect the external world. As a result, we can recursively apply our memory optimization scheme to each sub-graph.
  
  Pay Even Less Memory with Recursion Let g ( n ) to be the memory cost to do forward and backward pass on a n layer neural network. Assume that we store k intermediate results in the graph and apply the same strategy recursively when doing forward and backward pass on the sub-path. We have the following recursion formula.
  
  g ( n ) = k + g ( n/ ( k + 1)) (2)
  
  Solving this recursion formula gives us
  
  g ( n ) = k log$_{k}$$_{+1}$( n ) (3)
  
  As a special case, if we set k = 1 , we get g ( n ) = log$_{2}$ n . This is interesting conclusion as all the existing implementations takes O ( n ) memory in feature map to train a n layer neural network. This will require O (log$_{2}$ n ) cost forward pass cost, so may not be used commonly. But it demonstrates how we can trade memory even further by using recursion.
  
  ## 4.5 Guideline for Deep Learning Frameworks
  
  In this section, we have shown that it is possible to trade computation for memory and combine it with the system optimizations proposed in Sec 3. It is helpful for deep learning frameworks to
  
  - · Enable option to drop result of low cost operations.
  - · Provide planning algorithms to give efficient memory plan.
  - · Enable user to set the mirror attribute in the computation graph for memory optimization.
  
  While the last option is not strictly necessary, providing such interface enables user to hack their own memory optimizers and encourages future researches on the related directions. Under this spirit, we support the customization of graph mirror plan and will make the source code publicly available.
  
  ## 5 Experiments
  
  ## 5.1 Experiment Setup
  
  We evaluate the memory cost of storing intermediate feature maps using the methods described in this paper. We our method on top of MXNet [6], which statically allocate all the intermediate feature maps before computation. This enables us to report the exact memory cost spend on feature maps. Note that the memory cost of parameters and temporal memory (e.g. required by convolution) are not part of the memory cost report. We also record the runtime total memory cost by running training steps on a Titan X GPU. Note that all the memory optimizations proposed in this paper gives equivalent weight gradient for training and can always be safely applied. We compare the following memory allocation algorithms
  
  - · no optimization , directly allocate memory to each node in the graph without any optimization.
  - · inplace , enable inplace optimization when possible.
  - · sharing , enable inplace optimization as well as sharing. This represents all the system optimizations presented at Sec. 3.
  - · drop bn-relu , apply all system optimizations, drop result of batch norm and relu, this is only shown in convolutional net benchmark.
  - · sublinear plan , apply all system optimizations, use plan search with Alg 3 to trade computation with memory.
  
  ## 5.2 Deep Convolutional Network
  
  We first evaluate the proposed method on convolutional neural network for image classification. We use deep residual network architecture [11] (ResNet), which gives the state of art result on this task. Specifically, we use 32 batch size and set input image shape as (3 , 224 , 224) . We generate different depth configuration of ResNet 1 by increasing the depth of each residual stage.
  
  We show the results in Fig. 5. We can find that the system optimizations introduced in Sec. 3 can help to reduce the memory cost by factor of two to three. However, the memory cost after optimization still exhibits a linear trend with respect to number of layers. Even with all the system optimizations, it is only possible to train a 200 layer ResNet with the best GPU we can get. On the other hand, the proposed algorithm gives a sub-linear trend in terms of number of layers. By trade computation with memory, we can train a 1000 layer ResNet using less than 7GB of GPU memory.
  
  (a) Feature map memory cost estimation
  
  <!-- image -->
  
  Figure 6: The memory cost of different memory allocation strategies on LSTM configurations. System optimization gives a lot of memory saving on the LSTM graph, which contains a lot of fine grained operations. The sub-linear plan can give more than 4x reduction over the optimized plan that do not trade computation with memory.
  
  <!-- image -->
  
  (b) Runtime total memory cost
  
  Figure 5: The memory cost of different allocation strategies on deep residual net configurations. The feature map memory cost is generated from static memory allocation plan. We also use nvidiasmi to measure the total memory cost during runtime (the missing points are due to out of memory). The figures are in log-scale, so y = αx β will translate to log( y ) = β log( x ) + log α . We can find that the graph based allocation strategy indeed help to reduce the memory cost by a factor of two to three. More importantly, the sub-linear planning algorithm indeed gives sub-linear memory trend with respect to the workload. The real runtime result also confirms that we can use our method to greatly reduce memory cost deep net training.(a) Feature map memory cost estimation
  
  <!-- image -->
  
  <!-- image -->
  
  (b) Runtime total memory cost
  
  ## 5.3 LSTM for Long Sequences
  
  We also evaluate the algorithms on a LSTM under a long sequence unrolling setting. We unrolled a four layer LSTM with 1024 hidden states equals 64 over time. The batch size is set to 64. The input of each timestamp is a continuous 50 dimension vector and the output is softmax over 5000 class. This is a typical setting for speech recognition[17], but our result can also be generalized to other recurrent networks. Using a long unrolling step can potentially help recurrent model to learn long
  
  <!-- image -->
  
  Figure 7: The runtime speed of different allocation strategy on the two settings. The speed is measured by a running 20 batches on a Titan X GPU. We can see that using sub-linear memory plan incurs roughly 30 % of additional runtime cost compared to linear memory allocation. The general trend of speed vs workload remains linear for both strategies.
  
  <!-- image -->
  
  term dependencies over time. We show the results in Fig. 6. We can find that inplace helps a lot here. This is because inplace optimization in our experiment enables direct addition of weight gradient to a single memory cell, preventing allocate space for gradient at each timestamp. The sub-linear plan gives more than 4x reduction over the optimized memory plan.
  
  ## 5.4 Impact on Training Speed
  
  We also measure the runtime cost of each strategy. The speed is benchmarked on a single Titan X GPU. The results are shown in Fig. 7. Because of the double forward cost in gradient calculation, the sublinear allocation strategy costs 30 % additional runtime compared to the normal strategy. By paying the small price, we are now able to train a much wider range of deep learning models.
  
  ## 6 Conclusion
  
  In this paper, we proposed a systematic approach to reduce the memory consumption of the intermediate feature maps when training deep neural networks. Computation graph liveness analysis is used to enable memory sharing between feature maps. We also showed that we can trade the computation with the memory. By combining the techniques, we can train a n layer deep neural network with only O ( √ n ) memory cost, by paying nothing more than one extra forward computation per mini-batch.
  
  ## Acknowledgement
  
  We thank the helpful feedbacks from the MXNet community and developers. We thank Ian Goodfellow and Yu Zhang on helpful discussions on computation memory tradeoffs. We would like to thank David Warde-Farley for pointing out the relation to gradient checkpointing. We would like to thank Nvidia for the hardware support. This work was supported in part by ONR (PECASE) N000141010672, NSF IIS 1258741 and the TerraSwarm Research Center sponsored by MARCO and DARPA. Chiyuan Zhang acknowledges the support of a Nuance Foundation Grant.
  
  ## References
  
  - [1] Mart'ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man'e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi'egas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.
  - [2] Amit Agarwal, Eldar Akchurin, Chris Basoglu, Guoguo Chen, Scott Cyphers, Jasha Droppo, Adam Eversole, Brian Guenter, Mark Hillebrand, Ryan Hoens, Xuedong Huang, Zhiheng Huang, Vladimir Ivanov, Alexey Kamenev, Philipp Kranen, Oleksii Kuchaiev, Wolfgang Manousek, Avner May, Bhaskar Mitra, Olivier Nano, Gaizka Navarro, Alexey Orlov, Marko Padmilac, Hari Parthasarathi, Baolin Peng, Alexey Reznichenko, Frank Seide, Michael L. Seltzer, Malcolm Slaney, Andreas Stolcke, Yongqiang Wang, Huaming Wang, Kaisheng Yao, Dong Yu, Yu Zhang, and Geoffrey Zweig. An introduction to computational networks and the computational network toolkit. Technical Report MSR-TR-2014-112, August 2014.
  - [3] Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. Compilers: Principles, Techniques, and Tools . Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1986.
  - [4] Fr'ed'eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
  - [5] James Bergstra, Olivier Breuleux, Fr'ed'eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy) , June 2010. Oral Presentation.
  - [6] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, , and Zheng Zhang. MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems. In Neural Information Processing Systems, Workshop on Machine Learning Systems (LearningSys'15) , 2015.
  - [7] Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, MarcAurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In NIPS , 2012.
  - [8] Ian Goodfellow, Yoshua Bengio, , and Aaron Courville. Deep learning. Book in preparation for MIT Press, 2016.
  - [9] Andreas Griewank and Andrea Walther. Algorithm 799: Revolve: An implementation of checkpointing for the reverse or adjoint mode of computational differentiation. ACM Trans. Math. Softw. , 26(1):19-45, March 2000.
  - [10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385 , 2015.
  - [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. arXiv preprint arXiv:1603.05027 , 2016.
  - [12] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput. , 9(8):1735-1780, November 1997.
  
  - [13] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the 32th International Conference on Machine Learning (ICML'15) , 2015.
  - [14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25 , pages 1097-1105. 2012.
  - [15] Yann LeCun, L'eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In S. Haykin and B. Kosko, editors, Intelligent Signal Processing , pages 306-351. IEEE Press, 2001.
  - [16] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W Keckler. Virtualizing deep neural networks for memory-efficient neural network design. arXiv preprint arXiv:1602.08124 , 2016.
  - [17] Hasim Sak, Andrew W. Senior, and Franc¸oise Beaufays. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association, Singapore, September 14-18, 2014 , pages 338-342, 2014.
  - [18] Rupesh Kumar Srivastava, Klaus Greff, and Jurgen Schmidhuber. Training very deep networks. arXiv preprint arXiv:1507.06228 , 2015.
  - [19] Yu Zhang, Guoguo Chen, Dong Yu, Kaisheng Yao, Sanjeev Khudanpur, and James Glass. Highway long short-term memory rnns for distant speech recognition. arXiv preprint arXiv:1510.08983 , 2015.
  
  ## A Search over Budget B
  
  Alg. 3 allows us to generate an optimized memory plan given a single parameter B . This algorithm relies on approximate memory estimation for faster speed. After we get the plan, we can use the static allocation algorithm to calculate the exact memory cost. We can then do a grid search over B to find a good memory plan.
  
  To get the setting of the grid, we first run the allocation algorithm with B = 0 , then run the allocation algorithm again with B = √ xy . Here x and y are the outputs from Alg. 3 in the first run. Here x is the approximate cost to store inter-stage feature maps and y is the approximate cost to run each stage. B = √ xy an estimation of each stage's memory cost. This can already give a good memory plan. We then set grid around B = √ xy to further refine the solution. √ √
  
  In practice, we find that using a size 6 grid on [ B/ 2 , 2 B ] can already give good memory plans in the experiments. We implemented the allocation algorithm in python without any attempt to optimize for speed. Our code costs a few seconds to get the plans needed in the experiments.</file>
<file path="./services/AgentService/test_api.py">
  import requests
  import json
  import os
  
  
  def test_transcribe_api():
      # API endpoint
      AGENT_SERVICE_URL = os.getenv(
          "AGENT_SERVICE_URL", "http://localhost:8964/transcribe"
      )
  
      # Load markdown content from file
      with open("sample.md", "r") as file:
          markdown_content = file.read()
  
      # Prepare the payload
      payload = {
          "markdown": markdown_content,
          "duration": 5,
          "speaker_1_name": "Kate",
          "speaker_2_name": "Bob",
          "model": "meta/llama-3.1-405b-instruct",
          "job_id": "123",
      }
  
      # Send POST request
      response = requests.post(AGENT_SERVICE_URL, json=payload)
  
      # Check if the request was successful
      assert (
          response.status_code == 202
      ), f"Expected status code 200, but got {response.status_code}"
  
      # Parse the JSON response
      try:
          transcription = response.json()
      except json.JSONDecodeError:
          assert False, "Response is not valid JSON"
  
      print(transcription)
  
  
  if __name__ == "__main__":
      test_transcribe_api()
      print("All tests passed!")
</file>
<file path="./services/PDFService/Dockerfile">
  FROM python:3.11-slim
  
  WORKDIR /app
  
  # Install only the necessary Python packages
  RUN pip install --no-cache-dir \
      fastapi \
      uvicorn \
      python-multipart \
      httpx \
      redis \
      asyncio
  
  # Copy shared package
  COPY shared /shared
  RUN pip install /shared
  
  # Copy the service code
  COPY services/PDFService/main.py ./
  
  EXPOSE 8003
  
  CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8003"]</file>
<file path="./services/PDFService/PDFModelService/Dockerfile.api">
  FROM pytorch/pytorch:2.4.0-cuda12.4-cudnn9-devel
  
  # Install system dependencies
  RUN apt-get update \
      && apt-get install -y \
          curl \
          git \
          procps \
      && apt-get clean \
      && rm -rf /var/lib/apt/lists/*
  
  WORKDIR /app
  
  # Install Python dependencies - only API requirements
  COPY requirements.api.txt /app/
  RUN pip install -r requirements.api.txt
  
  # Copy application files
  COPY main.py tasks.py /app/
  
  # Create directory for temporary files
  RUN mkdir -p /tmp/pdf_conversions
  
  EXPOSE 8003
  
  CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8003"]</file>
<file path="./services/PDFService/PDFModelService/Dockerfile.worker">
  FROM pytorch/pytorch:2.4.0-cuda12.4-cudnn9-devel
  
  # Install system dependencies
  RUN apt-get update \
      && apt-get install -y \
          libgl1 \
          libglib2.0-0 \
          curl \
          wget \
          git \
          procps \
      && apt-get clean \
      && rm -rf /var/lib/apt/lists/*
  
  WORKDIR /app
  
  # Install Python dependencies - worker requirements with ML dependencies
  COPY requirements.worker.txt /app/
  RUN pip install -r requirements.worker.txt
  
  # Download required models
  RUN echo 'from deepsearch_glm.utils.load_pretrained_models import load_pretrained_nlp_models\nload_pretrained_nlp_models(verbose=True)' > download_models.py
  RUN python download_models.py
  
  # Copy application files
  COPY tasks.py /app/
  
  # Create directory for temporary files
  RUN mkdir -p /tmp/pdf_conversions
  
  CMD ["celery", "-A", "tasks", "worker", "--loglevel=info"]</file>
<file path="./services/PDFService/PDFModelService/docker-compose-remote.yml">
  version: '3.8'
  
  services:
    pdf-api:
      image: nvcr.io/pfteb4cqjzrs/playground/pdf-model-api:1.0
      ports:
        - "8003:8003"
      environment:
        - CELERY_BROKER_URL=redis://redis:6379/0
        - CELERY_RESULT_BACKEND=redis://redis:6379/0
        - REDIS_HOST=redis
        - REDIS_PORT=6379
        - TEMP_FILE_DIR=/tmp/pdf_conversions
      volumes:
        - pdf_temp:/tmp/pdf_conversions
      depends_on:
        - redis
        - celery-worker
      restart: unless-stopped
  
    celery-worker:
      image: nvcr.io/pfteb4cqjzrs/playground/pdf-model-worker:1.0
      environment:
        - CELERY_BROKER_URL=redis://redis:6379/0
        - CELERY_RESULT_BACKEND=redis://redis:6379/0
        - TEMP_FILE_DIR=/tmp/pdf_conversions
      volumes:
        - pdf_temp:/tmp/pdf_conversions
      depends_on:
        - redis
      restart: unless-stopped
  
    redis:
      image: redis:alpine
      ports:
        - "6379:6379"
      volumes:
        - redis_data:/data
      command: redis-server --appendonly yes
      restart: unless-stopped
  
  volumes:
    redis_data:
    pdf_temp:</file>
<file path="./services/PDFService/PDFModelService/docker-compose.yml">
  version: '3.8'
  
  services:
    pdf-api:
      build:
        context: .
        dockerfile: Dockerfile.api
      ports:
        - "8003:8003"
      environment:
        - CELERY_BROKER_URL=redis://redis:6379/0
        - CELERY_RESULT_BACKEND=redis://redis:6379/0
        - REDIS_HOST=redis
        - REDIS_PORT=6379
        - TEMP_FILE_DIR=/tmp/pdf_conversions
      volumes:
        - pdf_temp:/tmp/pdf_conversions
      depends_on:
        - redis
        - celery-worker
      restart: unless-stopped
  
    celery-worker:
      build:
        context: .
        dockerfile: Dockerfile.worker
      environment:
        - CELERY_BROKER_URL=redis://redis:6379/0
        - CELERY_RESULT_BACKEND=redis://redis:6379/0
        - TEMP_FILE_DIR=/tmp/pdf_conversions
      volumes:
        - pdf_temp:/tmp/pdf_conversions
      depends_on:
        - redis
      restart: unless-stopped
  
    redis:
      image: redis:alpine
      ports:
        - "6379:6379"
      volumes:
        - redis_data:/data
      command: redis-server --appendonly yes
      restart: unless-stopped
  
  volumes:
    redis_data:
    pdf_temp:</file>
<file path="./services/PDFService/PDFModelService/main.py">
  from fastapi import FastAPI, File, UploadFile, HTTPException
  from celery.result import AsyncResult
  import os
  import logging
  from typing import Dict
  import uuid
  from fastapi.responses import JSONResponse
  
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  app = FastAPI(debug=True)
  
  
  def get_celery_task():
      """Lazy import of Celery task to avoid immediate docling import"""
      from tasks import convert_pdf_task
  
      return convert_pdf_task
  
  
  @app.post("/convert")
  async def convert_pdf(file: UploadFile = File(...)) -> Dict[str, str]:
      """
      Start an asynchronous PDF conversion task
      """
      if not file.filename.lower().endswith(".pdf"):
          raise HTTPException(status_code=400, detail="File must be a PDF")
  
      try:
          # Save file with unique name
          file_id = str(uuid.uuid4())
          temp_dir = os.getenv("TEMP_FILE_DIR", "/tmp/pdf_conversions")
          os.makedirs(temp_dir, exist_ok=True)
          temp_file_path = os.path.join(temp_dir, f"{file_id}.pdf")
  
          content = await file.read()
          with open(temp_file_path, "wb") as temp_file:
              temp_file.write(content)
  
          # Get celery task and start conversion
          convert_pdf_task = get_celery_task()
          task = convert_pdf_task.delay(temp_file_path)
  
          return {
              "task_id": task.id,
              "status": "processing",
              "status_url": f"/status/{task.id}",
          }
  
      except Exception as e:
          logger.error(f"Error starting conversion: {str(e)}")
          # Clean up file if task creation fails
          if "temp_file_path" in locals() and os.path.exists(temp_file_path):
              os.unlink(temp_file_path)
          raise HTTPException(status_code=500, detail=str(e))
  
  
  @app.get("/status/{task_id}")
  async def get_conversion_status(task_id: str):
      """
      Check the status of a PDF conversion task
      Returns:
      - 200: Task completed successfully
      - 425: Task is still processing
      - 500: Task failed
      """
      try:
          task_result = AsyncResult(task_id)
  
          if task_result.ready():
              if task_result.successful():
                  result = task_result.get()
                  if result:
                      return JSONResponse(
                          content={"status": "completed", "result": result},
                          status_code=200,
                      )
                  else:
                      return JSONResponse(
                          content={
                              "status": "failed",
                              "error": "Task completed but no result was returned",
                          },
                          status_code=500,
                      )
              else:
                  error = str(task_result.result)
                  return JSONResponse(
                      content={"status": "failed", "error": error}, status_code=500
                  )
          else:
              return JSONResponse(
                  content={
                      "status": "processing",
                      "message": "Your PDF is still being converted",
                  },
                  status_code=425,
              )
  
      except Exception as e:
          logger.error(f"Error checking status: {str(e)}")
          return JSONResponse(
              content={"status": "failed", "error": str(e)}, status_code=500
          )
  
  
  @app.get("/health")
  async def health():
      """
      Health check endpoint
      """
      try:
          return {"status": "healthy", "service": "pdf-model-api"}
      except Exception as e:
          return {"status": "unhealthy", "error": str(e)}
  
  
  if __name__ == "__main__":
      import uvicorn
  
      uvicorn.run(app, host="0.0.0.0", port=8003)
</file>
<file path="./services/PDFService/PDFModelService/requirements.api.txt">
  fastapi
  python-multipart
  uvicorn
  celery[redis]
  redis
  docling==2.2.0</file>
<file path="./services/PDFService/PDFModelService/requirements.worker.txt">
  celery[redis]
  redis
  opencv-python==4.8.0.74
  opencv-contrib-python==4.8.0.74
  torch
  torchvision
  docling==2.2.0</file>
<file path="./services/PDFService/PDFModelService/tasks.py">
  from celery import Celery
  import os
  from docling.document_converter import DocumentConverter
  import logging
  
  logger = logging.getLogger(__name__)
  
  celery_app = Celery(
      "pdf_converter",
      broker=os.getenv("CELERY_BROKER_URL", "redis://redis:6379/0"),
      backend=os.getenv("CELERY_RESULT_BACKEND", "redis://redis:6379/0"),
  )
  
  celery_app.conf.update(
      task_serializer="json",
      accept_content=["json"],
      result_serializer="json",
      timezone="UTC",
      enable_utc=True,
      task_track_started=True,
      task_time_limit=3600,  # 1 hour max runtime
      task_soft_time_limit=3300,  # 55 minutes soft limit
  )
  
  
  @celery_app.task(bind=True, max_retries=3)
  def convert_pdf_task(self, file_path: str) -> str:  # Change return type hint
      try:
          converter = DocumentConverter()
          result = converter.convert(file_path)
          markdown = result.document.export_to_markdown()
          try:
              os.unlink(file_path)
              logger.info(f"Cleaned up file: {file_path}")
          except Exception as e:
              logger.error(f"Error cleaning up file: {e}")
          return markdown
      except Exception as exc:
          logger.error(f"Error converting PDF: {exc}")
          retry_in = 5 * (2**self.request.retries)
          raise self.retry(exc=exc, countdown=retry_in)
</file>
<file path="./services/PDFService/PDFModelService/test_api.py">
  import requests
  import os
  import sys
  
  # Define the endpoint URL
  URL = "http://127.0.0.1:8003/convert"
  
  # Define the path to the PDF file
  PDF_FILE_PATH = os.path.abspath(
      os.path.join(os.path.dirname(__file__), "../../../PNP_Proof.pdf")
  )
  
  if not os.path.exists(PDF_FILE_PATH):
      sys.exit(f"File not found: {PDF_FILE_PATH}")
  
  
  def test_convert_pdf():
      # Open the PDF file
      with open(PDF_FILE_PATH, "rb") as file:
          # Create a dictionary with the file and job_id. If no job_id is provided, the server will generate one.
          files = {"file": file}
          data = {"job_id": None}
  
          # Make the request
          response = requests.post(URL, files=files, data=data)
  
          # Print the response
          print("Response Status Code:", response.status_code)
          print("Response Content:", response.json())
  
          # Handle the response based on status code
          if response.status_code == 202:
              # If the conversion is successfully started, retrieve the job ID
              job_id = response.json().get("job_id")
              print(f"Job ID: {job_id}")
  
              # To monitor the job status, you can connect to the WebSocket endpoint
              WS_URL = f"ws://127.0.0.1:8003/ws/{job_id}"
              print(f"WebSocket URL for job status: {WS_URL}")
  
              # Note: Actually using the WebSocket to get status updates might require additional code.
          else:
              print("Error:", response.json())
  
  
  if __name__ == "__main__":
      test_convert_pdf()
</file>
<file path="./services/PDFService/PDFModelService/test_pdf_api.py">
  import requests
  import sys
  import json
  import time
  from pathlib import Path
  from shared.shared_types import StatusResponse
  
  
  def test_pdf_conversion(pdf_path: str, api_url: str = "http://localhost:8003"):
      """
      Test the PDF conversion endpoint by uploading a PDF file and displaying the markdown result.
  
      Args:
          pdf_path: Path to the PDF file to convert
          api_url: Base URL of the API service
      """
      # First check if the service is healthy
      try:
          health_response = requests.get(f"{api_url}/health")
          health_response.raise_for_status()
          print("Health check response:", json.dumps(health_response.json(), indent=2))
      except requests.exceptions.RequestException as e:
          print(f"Error checking service health: {e}")
          sys.exit(1)
  
      # Check if file exists
      pdf_file = Path(pdf_path)
      if not pdf_file.exists():
          print(f"Error: File {pdf_path} does not exist")
          sys.exit(1)
  
      # Prepare the file for upload
      files = {"file": (pdf_file.name, open(pdf_file, "rb"), "application/pdf")}
  
      try:
          # Make the conversion request
          print(f"\nUploading {pdf_file.name} for conversion...")
          response = requests.post(f"{api_url}/convert", files=files)
          response.raise_for_status()
  
          # Get the task ID
          result = response.json()
          task_id = result["task_id"]
          print(f"Task ID: {task_id}")
          print("Waiting for conversion to complete...")
  
          # Poll the status endpoint until the task is complete
          while True:
              status_response = requests.get(f"{api_url}/status/{task_id}")
  
              try:
                  status_data = StatusResponse.model_validate(status_response.json())
                  print(
                      f"Status check response: Code={status_response.status_code}, Data={status_data}"
                  )
  
                  if status_response.status_code == 200:
                      # Task completed successfully
                      result = status_data.result
                      if result:
                          print(f"Successfully received markdown result: {result}")
                          return True
                      print(f"No result found in response data: {status_data}")
                      return False
                  elif status_response.status_code == 425:
                      # Task still processing
                      print("Task still processing, waiting 2 seconds...")
                      time.sleep(2)
                  else:
                      error_msg = status_data.error
                      print(f"Error response received: {error_msg}")
                      return False
              except Exception as e:
                  print(f"Error checking status: {str(e)}")
                  return False
  
      except requests.exceptions.RequestException as e:
          print(f"Error during conversion: {e}")
          if hasattr(e, "response") and e.response is not None:
              print(f"Response content: {e.response.text}")
      finally:
          # Ensure the file is closed
          files["file"][1].close()
  
  
  if __name__ == "__main__":
      if len(sys.argv) != 2:
          print("Usage: python test_pdf_api.py <path_to_pdf_file>")
          sys.exit(1)
  
      test_pdf_conversion(sys.argv[1], "http://localhost:8003")
</file>
<file path="./services/PDFService/README.md">
  # Build the Docker image
  docker build -t pdf-conversion-service .
  
  # Run the Docker container
  docker run --gpus all -p 8003:8003 pdf-conversion-service</file>
<file path="./services/PDFService/dry_run.py">
  from docling.document_converter import DocumentConverter
  
  if __name__ == "__main__":
      converter = DocumentConverter()
      result = converter.convert("/app/sample.pdf")
      print(result.document.export_to_markdown())
</file>
<file path="./services/PDFService/main.py">
  from fastapi import FastAPI, File, UploadFile, BackgroundTasks, HTTPException
  from shared.shared_types import ServiceType, JobStatus, StatusResponse
  from shared.job import JobStatusManager
  from fastapi.responses import PlainTextResponse
  import httpx
  import tempfile
  import os
  import logging
  import time
  import asyncio
  from typing import Optional
  from pydantic import BaseModel
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  app = FastAPI(debug=True)
  job_manager = JobStatusManager(ServiceType.PDF)
  
  # Configuration
  MODEL_API_URL = os.getenv("MODEL_API_URL", "https://pdf-gyrdps568.brevlab.com")
  DEFAULT_TIMEOUT = 600  # seconds
  
  
  class PDFRequest(BaseModel):
      job_id: str
  
  
  async def convert_pdf_to_markdown(pdf_path: str) -> str:
      """Convert PDF to Markdown using the external API service"""
      logger.info(f"Sending PDF to external conversion service: {pdf_path}")
  
      async with httpx.AsyncClient(timeout=DEFAULT_TIMEOUT) as client:
          try:
              # Initial conversion request
              with open(pdf_path, "rb") as pdf_file:
                  files = {"file": ("document.pdf", pdf_file, "application/pdf")}
                  logger.info(f"Sending PDF to model API: {MODEL_API_URL}")
                  response = await client.post(f"{MODEL_API_URL}/convert", files=files)
  
                  if response.status_code != 200:
                      raise HTTPException(
                          status_code=response.status_code,
                          detail=f"Model API error: {response.text}",
                      )
  
                  task_data = response.json()
                  task_id = task_data["task_id"]
  
                  # Poll the status endpoint until the task is complete
                  while True:
                      status_response = await client.get(
                          f"{MODEL_API_URL}/status/{task_id}"
                      )
                      status_data = status_response.json()
                      logger.info(
                          f"Status check response: Code={status_response.status_code}, Data={status_data}"
                      )
  
                      if status_response.status_code == 200:
                          # Task completed successfully
                          result = status_data.get("result")
                          if result:
                              logger.info("Successfully received markdown result")
                              return result
                          logger.error(f"No result found in response data: {status_data}")
                          raise HTTPException(
                              status_code=500,
                              detail="Server returned success but no result was found",
                          )
                      elif status_response.status_code == 425:
                          # Task still processing
                          logger.info("Task still processing, waiting 2 seconds...")
                          await asyncio.sleep(2)
                      else:
                          error_msg = status_data.get("error", "Unknown error")
                          logger.error(f"Error response received: {error_msg}")
                          raise HTTPException(
                              status_code=status_response.status_code,
                              detail=f"PDF conversion failed: {error_msg}",
                          )
  
          except httpx.TimeoutException:
              logger.error("Request timed out")
              raise HTTPException(status_code=504, detail="Model API request timed out")
          except httpx.RequestError as e:
              logger.error(f"Request error: {str(e)}")
              raise HTTPException(
                  status_code=502, detail=f"Error connecting to Model API: {str(e)}"
              )
  
  
  async def process_pdf(job_id: str, file_content: bytes):
      """Background task to process PDF conversion"""
      try:
          job_manager.update_status(
              job_id, JobStatus.PROCESSING, "Creating temporary file"
          )
  
          # Create temporary file
          with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
              temp_file.write(file_content)
              temp_file_path = temp_file.name
  
          try:
              job_manager.update_status(
                  job_id,
                  JobStatus.PROCESSING,
                  "Converting PDF to Markdown via external service",
              )
  
              # Convert the PDF to markdown using external service
              markdown_content = await convert_pdf_to_markdown(temp_file_path)
  
              if not isinstance(markdown_content, str):
                  markdown_content = str(markdown_content)
  
              # Store result
              job_manager.set_result(job_id, markdown_content.encode())
              job_manager.update_status(
                  job_id, JobStatus.COMPLETED, "PDF conversion completed successfully"
              )
  
          finally:
              # Clean up the temporary file
              os.unlink(temp_file_path)
              logger.info(f"Cleaned up temporary file: {temp_file_path}")
  
      except Exception as e:
          logger.error(f"Error processing PDF: {str(e)}")
          job_manager.update_status(
              job_id, JobStatus.FAILED, f"PDF conversion failed: {str(e)}"
          )
          raise
  
  
  @app.post("/convert", status_code=202)
  async def convert_pdf(
      background_tasks: BackgroundTasks,
      file: UploadFile = File(...),
      job_id: Optional[str] = None,
  ):
      if not file.filename.lower().endswith(".pdf"):
          raise HTTPException(status_code=400, detail="File must be a PDF")
  
      # Read file content
      content = await file.read()
  
      # Create job
      if not job_id:
          job_id = str(int(time.time()))
  
      job_manager.create_job(job_id)
  
      # Start processing in background
      background_tasks.add_task(process_pdf, job_id, content)
  
      return {"job_id": job_id}
  
  
  @app.get("/status/{job_id}")
  async def get_status(job_id: str) -> StatusResponse:  # Add return type annotation
      """Get status of PDF conversion job"""
      status_data = job_manager.get_status(job_id)
      return StatusResponse(**status_data)
  
  
  @app.get("/output/{job_id}", response_class=PlainTextResponse)
  async def get_output(job_id: str):
      """Get the converted markdown content"""
      result = job_manager.get_result(job_id)
      if result is None:
          raise HTTPException(status_code=404, detail="Result not found")
      return result.decode()  # Decode bytes to string for markdown content
  
  
  @app.get("/health")
  async def health():
      """Check health of the service and its connection to the model API"""
      try:
          async with httpx.AsyncClient(timeout=5) as client:
              response = await client.get(f"{MODEL_API_URL}/health")
              if response.status_code != 200:
                  return {
                      "status": "unhealthy",
                      "error": f"Model API returned status code {response.status_code}",
                  }
  
              return {
                  "status": "healthy",
                  "service": "pdf-converter",
                  "model_api": response.json(),
              }
      except Exception as e:
          return {
              "status": "unhealthy",
              "error": f"Error connecting to Model API: {str(e)}",
          }
  
  
  if __name__ == "__main__":
      import uvicorn
  
      uvicorn.run(app, host="0.0.0.0", port=8003)
</file>
<file path="./services/PDFService/test.txt">
  This is a test file.</file>
<file path="./services/PDFService/test_api.py">
  import requests
  import os
  import time
  from typing import Optional
  from shared.shared_types import StatusResponse
  
  PDF_SERVICE_URL = os.getenv("PDF_SERVICE_URL", "http://localhost:8003")
  POLL_INTERVAL = 2  # seconds
  MAX_WAIT_TIME = 3600  # seconds
  
  
  def poll_job_status(job_id: str) -> Optional[dict]:
      """Poll the job status until completion or failure"""
      start_time = time.time()
  
      while time.time() - start_time < MAX_WAIT_TIME:
          try:
              response = requests.get(f"{PDF_SERVICE_URL}/status/{job_id}")
              status_data = StatusResponse.model_validate(response.json())
              # print(f"Polling status... Response: {status_data}")
  
              # Check the job status from the response
              if status_data.status == "JobStatus.COMPLETED":
                  return status_data
              elif status_data.status == "JobStatus.FAILED":
                  print(f"Job failed: {status_data.message}")
                  return None
              elif status_data.status == "JobStatus.PROCESSING":
                  print(f"Job still processing... {status_data.message}")
                  time.sleep(POLL_INTERVAL)
                  continue
              else:
                  print(f"Unknown status: {status_data.status}")
                  time.sleep(POLL_INTERVAL)
  
          except requests.RequestException as e:
              print(f"Error polling status: {e}")
              return None
  
      print("Error: Job timed out")
      return None
  
  
  def test_convert_pdf_endpoint():
      # Path to a sample PDF file for testing
      current_dir = os.path.dirname(os.path.abspath(__file__))
      sample_pdf_path = os.path.join(current_dir, "../../PNP_Proof.pdf")
  
      # Ensure the sample PDF file exists
      if not os.path.exists(sample_pdf_path):
          print(f"Error: Sample PDF file not found at {sample_pdf_path}")
          return False
  
      # Submit the conversion job
      try:
          with open(sample_pdf_path, "rb") as pdf_file:
              files = {"file": ("sample.pdf", pdf_file, "application/pdf")}
              response = requests.post(f"{PDF_SERVICE_URL}/convert", files=files)
  
          if response.status_code != 202:
              print(f"Error: Request failed with status code {response.status_code}")
              return False
  
          job_data = response.json()
          job_id = job_data.get("job_id")
          if not job_id:
              print("Error: No job_id in response")
              return False
  
          print("Starting job polling...")
          # Poll for job completion
          status_data = poll_job_status(job_id)
          if not status_data:
              return False
  
          print(f"Job completed. Status data: {status_data}")
  
          # Get the output
          output = get_job_output(job_id)
          if not output:
              print("Failed to get output")
              return False
  
          print("Successfully retrieved output")
  
          # Validate output content
          print("Validating output content...")
          if len(output) < 10:  # Basic check for minimum content
              print("Error: Output content seems too short")
              return False
  
          print("Success: PDF conversion test passed!")
          print("Response content:")
          print(
              output[:500] + "..." if len(output) > 500 else output
          )  # Print first 500 chars
          return True
  
      except requests.RequestException as e:
          print(f"Error during request: {e}")
          return False
  
  
  def get_job_output(job_id: str) -> Optional[str]:
      """Get the markdown output for a completed job"""
      try:
          response = requests.get(f"{PDF_SERVICE_URL}/output/{job_id}")
          if response.status_code != 200:
              print(f"Error getting output: {response.status_code}")
              if response.status_code == 404:
                  print(
                      "Job result not found. This might mean the job is still processing."
                  )
              return None
          return response.text
      except requests.RequestException as e:
          print(f"Error getting output: {e}")
          return None
  
  
  def test_convert_pdf_endpoint_invalid_file():
      files = {"file": ("test.txt", b"This is not a PDF file", "text/plain")}
      try:
          response = requests.post(f"{PDF_SERVICE_URL}/convert", files=files)
  
          if response.status_code != 400:
              print(f"Error: Expected 400 status code, got {response.status_code}")
              return False
  
          print("Success: Invalid file test passed!")
          return True
  
      except requests.RequestException as e:
          print(f"Error during request: {e}")
          return False
  
  
  def test_health_endpoint():
      try:
          response = requests.get(f"{PDF_SERVICE_URL}/health")
  
          if response.status_code != 200:
              print(f"Error: Health check failed with status code {response.status_code}")
              return False
  
          health_data = response.json()
          if health_data.get("status") != "healthy":
              print(
                  f"Error: Service unhealthy: {health_data.get('error', 'Unknown error')}"
              )
              return False
  
          print("Success: Health check passed!")
          return True
  
      except requests.RequestException as e:
          print(f"Error checking health: {e}")
          return False
  
  
  if __name__ == "__main__":
      print("Running PDF Service API tests...")
  
      print("\nTest 1: Health check")
      test_health_endpoint()
  
      print("\nTest 2: Valid PDF conversion")
      test_convert_pdf_endpoint()
  
      print("\nTest 3: Invalid file handling")
      test_convert_pdf_endpoint_invalid_file()
</file>
<file path="./services/TTSService/Dockerfile">
  FROM python:3.11-slim
  
  USER root
  ARG DEBIAN_FRONTEND=noninteractive
  
  RUN set -x \
      && apt-get update \
      && apt-get -y install wget curl man git less openssl libssl-dev unzip unar build-essential aria2 tmux vim \
      && apt-get install -y openssh-server sox libsox-fmt-all libsox-fmt-mp3 libsndfile1-dev ffmpeg \
      && rm -rf /var/lib/apt/lists/* \
      && apt-get clean
  
  
  WORKDIR /workspace
  
  RUN pip install fastapi uvicorn edge-tts elevenlabs pydantic redis
  
  # Copy shared package first
  COPY shared /shared
  RUN pip install /shared
  
  # Copy service files
  COPY services/TTSService/main.py ./
  
  EXPOSE 8889
  
  CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8889"]
</file>
<file path="./services/TTSService/README.md">
  docker build -t tts-service .
  docker run --gpus all -p 8889:8889 tts-service</file>
<file path="./services/TTSService/main.py">
  from fastapi import FastAPI, BackgroundTasks, HTTPException
  from shared.shared_types import ServiceType, JobStatus
  from shared.job import JobStatusManager
  from fastapi.responses import Response
  from pydantic import BaseModel
  from typing import List, Dict, Optional
  import logging
  from elevenlabs.client import ElevenLabs
  import os
  from concurrent.futures import ThreadPoolExecutor
  import asyncio
  from functools import lru_cache
  
  # Configure logging
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  app = FastAPI(title="ElevenLabs TTS Service", debug=True)
  job_manager = JobStatusManager(ServiceType.TTS)
  MAX_CONCURRENT_REQUESTS = int(os.getenv("MAX_CONCURRENT_REQUESTS", "5"))
  DEFAULT_VOICE_1 = os.getenv("DEFAULT_VOICE_1", "iP95p4xoKVk53GoZ742B")
  DEFAULT_VOICE_2 = os.getenv("DEFAULT_VOICE_2", "9BWtsMINqrJLrRacOk9x")
  DEFAULT_VOICE_MAPPING = {"speaker-1": DEFAULT_VOICE_1, "speaker-2": DEFAULT_VOICE_2}
  
  
  class DialogueEntry(BaseModel):
      text: str
      speaker: str
      voice_id: Optional[str] = None
  
  
  class TTSRequest(BaseModel):
      dialogue: List[DialogueEntry]
      job_id: str
      scratchpad: Optional[str] = ""
      voice_mapping: Optional[Dict[str, str]] = {
          "speaker-1": DEFAULT_VOICE_1,
          "speaker-2": DEFAULT_VOICE_2,
      }
  
  
  class VoiceInfo(BaseModel):
      voice_id: str
      name: str
      description: Optional[str] = None
  
  
  @lru_cache(maxsize=1)
  def get_available_voices() -> List[VoiceInfo]:
      """Fetch available voices from ElevenLabs API"""
      try:
          client = ElevenLabs(api_key=os.getenv("ELEVENLABS_API_KEY"))
          response = client.voices.get_all()
          # Handle the response structure properly
          voices_data = response.voices  # Access the voices list directly
  
          return [
              VoiceInfo(
                  voice_id=voice.voice_id,
                  name=voice.name,
                  description=voice.description
                  if hasattr(voice, "description")
                  else None,
              )
              for voice in voices_data
          ]
      except Exception as e:
          logger.error(f"Error fetching voices: {e}")
          # Return default voices if fetch fails
          return [
              VoiceInfo(
                  voice_id=DEFAULT_VOICE_1,
                  name="Default Voice 1",
                  description="Default speaker 1 voice",
              ),
              VoiceInfo(
                  voice_id=DEFAULT_VOICE_2,
                  name="Default Voice 2",
                  description="Default speaker 2 voice",
              ),
          ]
  
  
  class TTSService:
      def __init__(self):
          self.thread_pool = ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS)
  
      async def process_job(self, job_id: str, request: TTSRequest):
          try:
              voice_mapping = request.voice_mapping
              # Validate voice mapping against available voices
              available_voices = get_available_voices()
              available_voice_ids = {voice.voice_id for voice in available_voices}
              invalid_voices = set(voice_mapping.values()) - available_voice_ids
  
              if invalid_voices:
                  logger.warning(
                      f"Using default voices. Invalid voice IDs: {invalid_voices}"
                  )
                  voice_mapping = {
                      "speaker-1": DEFAULT_VOICE_1,
                      "speaker-2": DEFAULT_VOICE_2,
                  }
  
              job_manager.update_status(
                  job_id,
                  JobStatus.PROCESSING,
                  f"Processing {len(request.dialogue)} dialogue entries",
              )
  
              combined_audio = await self._process_dialogue(
                  job_id, request.dialogue, request.voice_mapping
              )
  
              job_manager.set_result(job_id, combined_audio)
              job_manager.update_status(
                  job_id, JobStatus.COMPLETED, "Audio generation completed successfully"
              )
  
          except Exception as e:
              logger.error(f"Error processing job {job_id}: {str(e)}")
              job_manager.update_status(job_id, JobStatus.FAILED, str(e))
  
      async def _process_dialogue(
          self, job_id: str, dialogue: List[DialogueEntry], voice_mapping: Dict[str, str]
      ) -> bytes:
          combined_audio = b""
          tasks = [
              (
                  entry.text,
                  entry.voice_id
                  if entry.voice_id and entry.voice_id in voice_mapping.values()
                  else voice_mapping.get(
                      entry.speaker, DEFAULT_VOICE_MAPPING[entry.speaker]
                  ),
              )
              for entry in dialogue
          ]
  
          for i in range(0, len(tasks), MAX_CONCURRENT_REQUESTS):
              batch = tasks[i : i + MAX_CONCURRENT_REQUESTS]
              job_manager.update_status(
                  job_id,
                  JobStatus.PROCESSING,
                  f"Processing batch {i//MAX_CONCURRENT_REQUESTS + 1} of {(len(tasks)-1)//MAX_CONCURRENT_REQUESTS + 1}",
              )
  
              futures = [
                  self.thread_pool.submit(self._convert_text, text, voice_id)
                  for text, voice_id in batch
              ]
              for future in futures:
                  combined_audio += await asyncio.get_event_loop().run_in_executor(
                      None, future.result
                  )
  
          return combined_audio
  
      def _convert_text(self, text: str, voice_id: str) -> bytes:
          """Convert text to speech using ElevenLabs"""
          client = ElevenLabs(api_key=os.getenv("ELEVENLABS_API_KEY"))
          audio_stream = client.text_to_speech.convert(
              text=text,
              voice_id=voice_id,
              model_id="eleven_monolingual_v1",
              output_format="mp3_44100_128",
              voice_settings={"stability": 0.5, "similarity_boost": 0.75, "style": 0.0},
          )
          return b"".join(chunk for chunk in audio_stream)
  
  
  # Initialize service
  tts_service = TTSService()
  
  
  @app.get("/voices")
  async def list_voices() -> List[VoiceInfo]:
      """Get list of available voices"""
      voices = get_available_voices()
      return voices
  
  
  @app.post("/generate_tts", status_code=202)
  async def generate_tts(request: TTSRequest, background_tasks: BackgroundTasks):
      """Start TTS generation job"""
      job_manager.create_job(request.job_id)
      background_tasks.add_task(tts_service.process_job, request.job_id, request)
      return {"job_id": request.job_id}
  
  
  @app.get("/status/{job_id}")
  async def get_status(job_id: str):
      """Get job status"""
      return job_manager.get_status(job_id)
  
  
  @app.get("/output/{job_id}")
  async def get_output(job_id: str):
      """Get the generated audio file"""
      result = job_manager.get_result(job_id)
      if result is None:
          raise HTTPException(status_code=404, detail="Result not found")
      return Response(
          content=result,
          media_type="audio/mpeg",
          headers={"Content-Disposition": "attachment; filename=output.mp3"},
      )
  
  
  @app.post("/cleanup")
  async def cleanup_jobs():
      """Clean up old jobs"""
      removed = job_manager.cleanup_old_jobs()
      return {"message": f"Removed {removed} old jobs"}
  
  
  @app.get("/health")
  async def health():
      """Health check endpoint"""
      voices = get_available_voices()
      return {
          "status": "healthy",
          "available_voices": len(voices),
          "max_concurrent_requests": MAX_CONCURRENT_REQUESTS,
      }
</file>
<file path="./services/TTSService/test_api.py">
  import requests
  import json
  import os
  import time
  from fastapi import HTTPException
  from datetime import datetime
  
  
  def get_time():
      return datetime.now().strftime("%H:%M:%S")
  
  
  def get_output_with_retry(base_url: str, job_id: str, max_retries=10, retry_delay=2):
      """Retry getting output with exponential backoff"""
      for attempt in range(max_retries):
          try:
              response = requests.get(f"{base_url}/output/{job_id}")
              if response.status_code == 200:
                  return response.content
              elif response.status_code == 425:
                  # Result is being prepared, use shorter delay
                  wait_time = min(retry_delay * (1.5**attempt), 10)  # Cap at 10 seconds
                  print(
                      f"[{get_time()}] Result is being prepared, retrying in {wait_time:.1f}s..."
                  )
                  time.sleep(wait_time)
                  continue
              elif response.status_code == 404:
                  # Result not found or job doesn't exist
                  raise HTTPException(status_code=404, detail="Result not found")
              else:
                  print(
                      f"[{get_time()}] Unexpected status code {response.status_code}: {response.text}"
                  )
                  response.raise_for_status()
          except requests.RequestException as e:
              print(f"[{get_time()}] Error getting output: {e}")
              if attempt == max_retries - 1:
                  raise
              time.sleep(retry_delay * (2**attempt))
  
      raise TimeoutError("Failed to get output after maximum retries")
  
  
  def test_tts_api():
      # API endpoint URLs
      base_url = os.getenv("TTS_SERVICE_URL", "http://localhost:8889")
      generate_url = f"{base_url}/generate_tts"
  
      print(f"[{get_time()}] Starting TTS test...")
  
      try:
          # Load sample JSON data
          with open("sample.json", "r") as f:
              data = json.load(f)
  
          # Add job_id if not present
          if "job_id" not in data:
              data["job_id"] = str(int(time.time()))
  
          # Make initial request to generate TTS
          print(f"[{get_time()}] Submitting TTS generation request...")
          response = requests.post(generate_url, json=data)
  
          if response.status_code != 202:
              print(
                  f"[{get_time()}] Error: Unexpected status code {response.status_code}"
              )
              print(response.text)
              return
  
          # Get job ID from response
          job_data = response.json()
          job_id = job_data["job_id"]
          print(f"[{get_time()}] Job ID received: {job_id}")
  
          # Monitor job status
          last_status = None
          while True:
              status_response = requests.get(f"{base_url}/status/{job_id}")
              if status_response.status_code != 200:
                  print(f"[{get_time()}] Error checking status: {status_response.text}")
                  return
  
              status_data = status_response.json()
              current_status = status_data.get("status")
              message = status_data.get("message", "")
  
              # Only print if status has changed
              if current_status != last_status:
                  print(f"[{get_time()}] Status: {current_status} - {message}")
                  last_status = current_status
  
              if current_status in ["completed", "COMPLETED", "JobStatus.COMPLETED"]:
                  break
              elif current_status in ["failed", "FAILED", "JobStatus.FAILED"]:
                  print(f"[{get_time()}] Job failed: {message}")
                  return
  
              time.sleep(2)
  
          # Get the final output with retry logic
          print(f"[{get_time()}] Retrieving audio file...")
          audio_content = get_output_with_retry(base_url, job_id)
  
          # Save the audio file
          output_filename = f"output_{job_id}.mp3"
          with open(output_filename, "wb") as f:
              f.write(audio_content)
          print(f"[{get_time()}] Audio file saved as '{output_filename}'")
  
      except Exception as e:
          print(f"[{get_time()}] Error: {str(e)}")
  
  
  if __name__ == "__main__":
      test_tts_api()
</file>
<file path="./shared/setup.py">
  from setuptools import setup, find_packages
  
  setup(
      name="shared",
      version="0.1",
      packages=find_packages(),
      install_requires=["redis", "pydantic"],
  )
</file>
<file path="./shared/shared/__init__.py">
</file>
<file path="./shared/shared/connection.py">
  from fastapi import WebSocket, WebSocketDisconnect
  from typing import Dict, Set
  import redis
  import json
  import logging
  import time
  import asyncio
  from collections import defaultdict
  from threading import Thread
  import queue
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  
  class ConnectionManager:
      def __init__(self, redis_client: redis.Redis):
          self.active_connections: Dict[str, Set[WebSocket]] = defaultdict(set)
          self.pubsub = None
          self.message_queue = queue.Queue()
          self.redis_thread = None
          self.should_stop = False
          self.redis_client = redis_client
  
      async def connect(self, websocket: WebSocket, job_id: str):
          await websocket.accept()
          self.active_connections[job_id].add(websocket)
          logger.info(
              f"New WebSocket connection for job {job_id}. Total connections: {len(self.active_connections[job_id])}"
          )
  
          # Start Redis listener if not already running
          if self.redis_thread is None:
              self.redis_thread = Thread(target=self._redis_listener)
              self.redis_thread.daemon = True
              self.redis_thread.start()
              # Start the async message processor
              asyncio.create_task(self._process_messages())
  
      def disconnect(self, websocket: WebSocket, job_id: str):
          if job_id in self.active_connections:
              self.active_connections[job_id].remove(websocket)
              if not self.active_connections[job_id]:
                  del self.active_connections[job_id]
              logger.info(
                  f"WebSocket disconnected for job {job_id}. Remaining connections: {len(self.active_connections[job_id]) if job_id in self.active_connections else 0}"
              )
  
      def _redis_listener(self):
          """Redis subscription running in a separate thread"""
          try:
              self.pubsub = self.redis_client.pubsub(ignore_subscribe_messages=True)
              self.pubsub.subscribe("status_updates:all")
              logger.info("Successfully subscribed to Redis status_updates:all channel")
  
              while not self.should_stop:
                  message = self.pubsub.get_message()
                  if message and message["type"] == "message":
                      self.message_queue.put(message["data"])
                  time.sleep(0.01)  # Prevent tight loop
  
          except Exception as e:
              logger.error(f"Redis subscription error: {e}")
          finally:
              if self.pubsub:
                  self.pubsub.unsubscribe()
                  self.pubsub.close()
  
      async def _process_messages(self):
          """Async task to process messages from the queue and broadcast them"""
          while True:
              try:
                  # Check queue in a non-blocking way
                  while not self.message_queue.empty():
                      message = self.message_queue.get_nowait()
                      try:
                          if isinstance(message, bytes):
                              message = message.decode("utf-8")
  
                          update = json.loads(message)
                          job_id = update.get("job_id")
  
                          if job_id and job_id in self.active_connections:
                              await self.broadcast_to_job(
                                  job_id,
                                  {
                                      "service": update.get("service"),
                                      "status": update.get("status"),
                                      "message": update.get("message", ""),
                                  },
                              )
                              logger.info(
                                  f"Broadcasted update for job {job_id}: {update.get('service')} - {update.get('status')}"
                              )
                      except json.JSONDecodeError:
                          logger.error(f"Invalid JSON in Redis message: {message}")
                      except Exception as e:
                          logger.error(f"Error processing Redis message: {e}")
  
                  # Small delay before next check
                  await asyncio.sleep(0.01)
  
              except Exception as e:
                  logger.error(f"Message processing error: {e}")
                  await asyncio.sleep(1)
  
      async def broadcast_to_job(self, job_id: str, message: dict):
          """Send message to all WebSocket connections for a job"""
          if job_id in self.active_connections:
              disconnected = set()
              for connection in self.active_connections[job_id]:
                  try:
                      await connection.send_json(message)
                  except WebSocketDisconnect:
                      disconnected.add(connection)
                  except Exception as e:
                      logger.error(f"Error sending message to WebSocket: {e}")
                      disconnected.add(connection)
  
              # Clean up disconnected clients
              for connection in disconnected:
                  self.disconnect(connection, job_id)
  
      def cleanup(self):
          """Cleanup resources"""
          self.should_stop = True
          if self.redis_thread:
              self.redis_thread.join(timeout=1.0)
          if self.pubsub:
              self.pubsub.close()
</file>
<file path="./shared/shared/job.py">
  from shared.shared_types import ServiceType
  import redis
  import time
  import json
  import threading
  
  
  class JobStatusManager:
      def __init__(self, service_type: ServiceType, redis_url="redis://redis:6379"):
          self.redis = redis.Redis.from_url(redis_url, decode_responses=False)
          self.service_type = service_type
          self._lock = threading.Lock()
  
      def create_job(self, job_id: str):
          update = {
              "job_id": job_id,
              "status": "pending",
              "message": "Job created",
              "service": self.service_type,
              "timestamp": time.time(),
          }
          # Encode the update dict as JSON bytes
          self.redis.hset(
              f"status:{job_id}:{self.service_type}",
              mapping={k: str(v).encode() for k, v in update.items()},
          )
          self.redis.publish("status_updates:all", json.dumps(update).encode())
  
      def update_status(self, job_id: str, status: str, message: str):
          update = {
              "job_id": job_id,
              "status": status,
              "message": message,
              "service": self.service_type,
              "timestamp": time.time(),
          }
          # Encode the update dict as JSON bytes
          self.redis.hset(
              f"status:{job_id}:{self.service_type}",
              mapping={k: str(v).encode() for k, v in update.items()},
          )
          self.redis.publish("status_updates:all", json.dumps(update).encode())
  
      def set_result(self, job_id: str, result: bytes):
          self.redis.set(f"result:{job_id}:{self.service_type}", result)
  
      def set_result_with_expiration(self, job_id: str, result: bytes, ex: int):
          self.redis.set(f"result:{job_id}:{self.service_type}", result, ex=ex)
  
      def get_result(self, job_id: str):
          result = self.redis.get(f"result:{job_id}:{self.service_type}")
          return result if result else None
  
      def get_status(self, job_id: str):
          # Get raw bytes and decode manually
          status = self.redis.hgetall(f"status:{job_id}:{self.service_type}")
          if not status:
              raise ValueError("Job not found")
          # Decode bytes to strings for each field
          return {k.decode(): v.decode() for k, v in status.items()}
  
      def cleanup_old_jobs(self, max_age=3600):
          current_time = time.time()
          removed = 0
          pattern = f"status:*:{self.service_type}"
          for key in self.redis.scan_iter(match=pattern):
              status = self.redis.hgetall(key)
              try:
                  timestamp = float(status[b"timestamp"].decode())
                  if timestamp < current_time - max_age:
                      self.redis.delete(key)
                      job_id = key.split(b":")[1].decode()
                      self.redis.delete(f"result:{job_id}:{self.service_type}")
                      removed += 1
              except (KeyError, ValueError):
                  # Handle malformed status entries
                  continue
          return removed
</file>
<file path="./shared/shared/otel.py">
  from dataclasses import dataclass
  from typing import Optional
  
  from opentelemetry import trace
  from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
  from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
  from opentelemetry.instrumentation.redis import RedisInstrumentor
  from opentelemetry.instrumentation.requests import RequestsInstrumentor
  from opentelemetry.sdk.resources import Resource
  from opentelemetry.sdk.trace import TracerProvider
  from opentelemetry.sdk.trace.export import BatchSpanProcessor
  
  
  @dataclass
  class OpenTelemetryConfig:
      """Configuration for OpenTelemetry setup."""
      service_name: str
      otlp_endpoint: str = "http://jaeger:4317"
      enable_redis: bool = True
      enable_requests: bool = True
  
  
  class OpenTelemetryInstrumentation:
      """
      Lightweight OTEL wrapper 
      
      Example usage:
          telemetry = OpenTelemetryInstrumentation()
          app = FastAPI()
          telemetry.initialize(app, "api-service")
          
          # In code
          with telemetry.tracer.start_as_current_span("operation_name") as span:
              span.set_attribute("key", "value")
      """
  
      def __init__(self):
          self._tracer: Optional[trace.Tracer] = None
          self._config: Optional[OpenTelemetryConfig] = None
  
      @property
      def tracer(self) -> trace.Tracer:
          """Get the configured tracer instance."""
          if not self._tracer:
              raise RuntimeError("OpenTelemetry has not been initialized. Call initialize() first.")
          return self._tracer
  
      def initialize(self, app, config: OpenTelemetryConfig) -> 'OpenTelemetryInstrumentation':
          """
          Initialize OpenTelemetry instrumentation with the given configuration.
          
          Args:
              app: The FastAPI application instance
              config: OpenTelemetryConfig instance containing configuration options
              
          Returns:
              self for method chaining
          """
          self._config = config
          self._setup_tracing()
          self._instrument_app(app)
          return self
  
      def _setup_tracing(self) -> None:
          """Set up the OpenTelemetry tracer provider and processors."""
          resource = Resource.create({
              "service.name": self._config.service_name
          })
  
          provider = TracerProvider(resource=resource)
          processor = BatchSpanProcessor(
              OTLPSpanExporter(endpoint=self._config.otlp_endpoint)
          )
          
          provider.add_span_processor(processor)
          trace.set_tracer_provider(provider)
          
          self._tracer = trace.get_tracer(self._config.service_name)
  
      def _instrument_app(self, app) -> None:
          """Instrument the FastAPI application and optional components."""
          # Instrument FastAPI
          FastAPIInstrumentor.instrument_app(app)
  
          # Instrument Redis if enabled
          if self._config.enable_redis:
              RedisInstrumentor().instrument()
  
          # Instrument requests library if enabled
          if self._config.enable_requests:
              RequestsInstrumentor().instrument()
  
  
  # Example usage in your FastAPI application:
  """
  from fastapi import FastAPI
  from your_module import OpenTelemetryInstrumentation, OpenTelemetryConfig
  
  # Create FastAPI app
  app = FastAPI()
  
  # Initialize OpenTelemetry
  telemetry = OpenTelemetryInstrumentation()
  config = OpenTelemetryConfig(
      service_name="api-service",
      otlp_endpoint="http://jaeger:4317",
      enable_redis=True,
      enable_requests=True
  )
  telemetry.initialize(app, config)
  
  @app.get("/")
  async def root():
      # Use the tracer in your endpoints
      with telemetry.tracer.start_as_current_span("root_operation") as span:
          span.set_attribute("endpoint", "root")
          return {"message": "Hello World"}
  """</file>
<file path="./shared/shared/shared_types.py">
  from pydantic import BaseModel, Field
  from typing import Optional, Dict, List, Literal
  from enum import Enum
  
  
  class ServiceType(str, Enum):
      PDF = "pdf"
      AGENT = "agent"
      TTS = "tts"
  
  
  class JobStatus(str, Enum):
      PENDING = "pending"
      PROCESSING = "processing"
      COMPLETED = "completed"
      FAILED = "failed"
  
  
  class StatusUpdate(BaseModel):
      job_id: str
      status: JobStatus
      message: Optional[str] = None
      service: Optional[ServiceType] = None
      timestamp: Optional[float] = None
      data: Optional[dict] = None
  
  
  class StatusResponse(BaseModel):
      status: str
      result: Optional[str] = None
      error: Optional[str] = None
      message: Optional[str] = None
  
  
  class TranscriptionParams(BaseModel):
      name: str = Field(..., description="Name of the podcast")
      duration: int = Field(..., description="Duration in minutes")
      speaker_1_name: str = Field(..., description="Name of the first speaker")
      speaker_2_name: str = Field(..., description="Name of the second speaker")
      model: str = Field(..., description="Model name/path to use for transcription")
      voice_mapping: Dict[str, str] = Field(
          ...,
          description="Mapping of speaker IDs to voice IDs",
          example={
              "speaker-1": "iP95p4xoKVk53GoZ742B",
              "speaker-2": "9BWtsMINqrJLrRacOk9x",
          },
      )
  
  
  class SavedPodcast(BaseModel):
      job_id: str
      filename: str
      created_at: str
      size: int
      transcription_params: Optional[Dict] = {}
  
  
  class SavedPodcastWithAudio(SavedPodcast):
      audio_data: str
  
  
  # Transcript schema
  class DialogueEntry(BaseModel):
      text: str
      speaker: Literal["speaker-1", "speaker-2"]
  
  
  class Conversation(BaseModel):
      scratchpad: str
      dialogue: List[DialogueEntry]
  
  
  # Prompt tracker schema
  class ProcessingStep(BaseModel):
      step_name: str
      prompt: str
      response: str
      model: str
      timestamp: float
  
  
  class PromptTracker(BaseModel):
      steps: List[ProcessingStep]
</file>
<file path="./shared/shared/storage.py">
  import io
  import json
  import base64
  from minio import Minio
  from minio.error import S3Error
  from shared.shared_types import TranscriptionParams
  import os
  import logging
  from typing import Optional
  
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  
  # Minio config
  MINIO_ENDPOINT = os.getenv("MINIO_ENDPOINT", "minio:9000")
  MINIO_ACCESS_KEY = os.getenv("MINIO_ACCESS_KEY", "minioadmin")
  MINIO_SECRET_KEY = os.getenv("MINIO_SECRET_KEY", "minioadmin")
  MINIO_BUCKET_NAME = os.getenv("MINIO_BUCKET_NAME", "audio-results")
  
  
  # TODO: use this to wrap redis as well
  # TODO: wrap errors in StorageError
  # TODO: implement cleanup and delete as well
  class StorageManager:
      def __init__(self):
          """Initialize MinIO client and ensure bucket exists"""
          try:
              self.client = Minio(
                  os.getenv("MINIO_ENDPOINT", "minio:9000"),
                  access_key=os.getenv("MINIO_ACCESS_KEY", "minioadmin"),
                  secret_key=os.getenv("MINIO_SECRET_KEY", "minioadmin"),
                  secure=os.getenv("MINIO_SECURE", "false").lower() == "true",
              )
  
              self.bucket_name = os.getenv("MINIO_BUCKET_NAME", "audio-results")
              self._ensure_bucket_exists()
              logger.info("Successfully initialized MinIO storage")
  
          except Exception as e:
              logger.error(f"Failed to initialize MinIO client: {e}")
              raise
  
      def _ensure_bucket_exists(self):
          try:
              if not self.client.bucket_exists(self.bucket_name):
                  self.client.make_bucket(self.bucket_name)
          except Exception as e:
              logger.error(f"Failed to ensure bucket exists: {e}")
              raise
  
      def store_file(
          self,
          job_id: str,
          content: bytes,
          filename: str,
          content_type: str,
          metadata: dict = None,
      ) -> None:
          """Store any file type in MinIO with metadata"""
          try:
              self.client.put_object(
                  self.bucket_name,
                  f"{job_id}/{filename}",
                  io.BytesIO(content),
                  length=len(content),
                  content_type=content_type,
                  metadata=metadata.model_dump()
                  if hasattr(metadata, "model_dump")
                  else metadata,
              )
          except Exception as e:
              logger.error(f"Failed to store file {filename} for job {job_id}: {str(e)}")
              raise
  
      def store_audio(
          self,
          job_id: str,
          audio_content: bytes,
          filename: str,
          transcription_params: TranscriptionParams,
      ):
          """Store audio file with metadata in MinIO"""
          try:
              object_name = f"{job_id}/{filename}"
  
              # Convert transcription params to JSON string for metadata
              params_json = json.dumps(transcription_params.model_dump())
  
              # Create metadata dictionary with transcription params
              metadata = {"X-Amz-Meta-Transcription-Params": params_json}
  
              self.client.put_object(
                  self.bucket_name,
                  object_name,
                  io.BytesIO(audio_content),
                  len(audio_content),
                  content_type="audio/mpeg",
                  metadata=metadata,
              )
              logger.info(
                  f"Stored audio for {job_id} in MinIO as {object_name} with metadata"
              )
  
          except S3Error as e:
              logger.error(f"Failed to store audio in MinIO: {e}")
              raise
  
      def get_podcast_audio(self, job_id: str) -> Optional[str]:
          """Get the audio data for a specific podcast by job_id"""
          try:
              # Find the file with matching job_id
              objects = self.client.list_objects(
                  self.bucket_name, prefix=f"{job_id}/", recursive=True
              )
  
              for obj in objects:
                  if obj.object_name.endswith(".mp3"):
                      audio_data = self.client.get_object(
                          self.bucket_name, obj.object_name
                      ).read()
                      return base64.b64encode(audio_data).decode("utf-8")
  
              return None
  
          except Exception as e:
              logger.error(f"Failed to get audio for job_id {job_id}: {str(e)}")
              raise
  
      def get_file(self, job_id: str, filename: str) -> Optional[bytes]:
          """Get any file from storage by job_id and filename"""
          try:
              object_name = f"{job_id}/{filename}"
  
              try:
                  data = self.client.get_object(self.bucket_name, object_name).read()
                  return data
              except S3Error as e:
                  if e.code == "NoSuchKey":
                      return None
                  raise
  
          except Exception as e:
              logger.error(f"Failed to get file {filename} for job_id {job_id}: {str(e)}")
              raise
  
      def delete_job_files(self, job_id: str) -> bool:
          """Delete all files associated with a job_id"""
          try:
              # List all objects with the job_id prefix
              objects = self.client.list_objects(
                  self.bucket_name, prefix=f"{job_id}/", recursive=True
              )
  
              # Delete each object
              for obj in objects:
                  self.client.remove_object(self.bucket_name, obj.object_name)
                  logger.info(f"Deleted object: {obj.object_name}")
  
              return True
  
          except Exception as e:
              logger.error(f"Failed to delete files for job_id {job_id}: {str(e)}")
              return False
  
      # TODO: rework
      def list_files_metadata(self):
          """Lists metadata in the from of TranscriptionParams for an audio file which was created in store_audio"""
          try:
              objects = self.client.list_objects(self.bucket_name, recursive=True)
              files = []
  
              for obj in objects:
                  if obj.object_name.endswith("/"):
                      continue
  
                  try:
                      stat = self.client.stat_object(self.bucket_name, obj.object_name)
                      path_parts = obj.object_name.split("/")
  
                      if not path_parts[-1].endswith(".mp3"):
                          continue
  
                      job_id = path_parts[0]
  
                      file_info = {
                          "job_id": job_id,
                          "filename": path_parts[-1],
                          "size": stat.size,
                          "created_at": obj.last_modified.isoformat(),
                          "path": obj.object_name,
                          "transcription_params": {},
                      }
  
                      if stat.metadata:
                          try:
                              params = stat.metadata.get(
                                  "X-Amz-Meta-Transcription-Params"
                              )
                              if params:
                                  file_info["transcription_params"] = json.loads(params)
                          except json.JSONDecodeError:
                              logger.warning(
                                  f"Could not parse transcription params for {obj.object_name}"
                              )
  
                      files.append(file_info)
                      logger.info(
                          f"Found file: {obj.object_name}, size: {stat.size} bytes"
                      )
  
                  except Exception as e:
                      logger.error(f"Error processing object {obj.object_name}: {str(e)}")
                      continue
  
              files.sort(key=lambda x: x["created_at"], reverse=True)
              logger.info(
                  f"Successfully listed {len(files)} metadata for {len(files)} files from MinIO"
              )
              return files
  
          except Exception as e:
              logger.error(f"Failed to list files from MinIO: {str(e)}")
              raise
</file>
<file path="./test.py">
  import requests
  import os
  import json
  import time
  from datetime import datetime
  from threading import Thread, Event
  import websockets
  import asyncio
  from urllib.parse import urljoin
  
  
  class StatusMonitor:
      def __init__(self, base_url, job_id):
          self.base_url = base_url
          self.job_id = job_id
          self.ws_url = self._get_ws_url(base_url)
          self.stop_event = Event()
          self.services = {"pdf", "agent", "tts"}
          self.last_statuses = {service: None for service in self.services}
          self.tts_completed = Event()
          self.websocket = None
          self.reconnect_delay = 1.0
          self.max_reconnect_delay = 30.0
  
      def _get_ws_url(self, base_url):
          """Convert HTTP URL to WebSocket URL"""
          if base_url.startswith("https://"):
              ws_base = "wss://" + base_url[8:]
          else:
              ws_base = "ws://" + base_url[7:]
          return urljoin(ws_base, f"/ws/status/{self.job_id}")
  
      def get_time(self):
          return datetime.now().strftime("%H:%M:%S")
  
      def start(self):
          """Start the WebSocket monitoring in a separate thread"""
          self.thread = Thread(target=self._run_async_loop)
          self.thread.start()
  
      def stop(self):
          """Stop the WebSocket monitoring"""
          self.stop_event.set()
          self.thread.join()
  
      def _run_async_loop(self):
          """Run the asyncio event loop in a separate thread"""
          loop = asyncio.new_event_loop()
          asyncio.set_event_loop(loop)
          loop.run_until_complete(self._monitor_status())
  
      async def _monitor_status(self):
          """Monitor status updates via WebSocket with automatic reconnection"""
          while not self.stop_event.is_set():
              try:
                  async with websockets.connect(self.ws_url) as websocket:
                      self.websocket = websocket
                      self.reconnect_delay = 1.0  # Reset delay on successful connection
                      print(f"[{self.get_time()}] Connected to status WebSocket")
  
                      while not self.stop_event.is_set():
                          try:
                              message = await asyncio.wait_for(
                                  websocket.recv(), timeout=30
                              )
                              await self._handle_message(message)
                          except asyncio.TimeoutError:
                              # Send ping to keep connection alive
                              try:
                                  pong_waiter = await websocket.ping()
                                  await pong_waiter
                              except:  # noqa
                                  break
  
              except websockets.exceptions.ConnectionClosed:
                  if not self.stop_event.is_set():
                      print(
                          f"[{self.get_time()}] WebSocket connection closed, reconnecting..."
                      )
  
              except Exception as e:
                  if not self.stop_event.is_set():
                      print(f"[{self.get_time()}] WebSocket error: {e}, reconnecting...")
  
              if not self.stop_event.is_set():
                  # Exponential backoff for reconnection
                  await asyncio.sleep(self.reconnect_delay)
                  self.reconnect_delay = min(
                      self.reconnect_delay * 1.5, self.max_reconnect_delay
                  )
  
      async def _handle_message(self, message):
          """Handle incoming WebSocket messages"""
          try:
              data = json.loads(message)
              service = data.get("service")
              status = data.get("status")
              msg = data.get("message", "")
  
              if service in self.services:
                  current_status = f"{service}: {status} - {msg}"
                  if current_status != self.last_statuses[service]:
                      print(f"[{self.get_time()}] {current_status}")
                      self.last_statuses[service] = current_status
  
                      if status == "failed":
                          print(f"[{self.get_time()}] Job failed in {service}: {msg}")
                          self.stop_event.set()
  
                      if service == "tts" and status == "completed":
                          self.tts_completed.set()
                          self.stop_event.set()
  
          except json.JSONDecodeError:
              print(f"[{self.get_time()}] Received invalid JSON: {message}")
          except Exception as e:
              print(f"[{self.get_time()}] Error processing message: {e}")
  
  
  def get_output_with_retry(base_url: str, job_id: str, max_retries=5, retry_delay=1):
      """Retry getting output with exponential backoff"""
      for attempt in range(max_retries):
          try:
              response = requests.get(f"{base_url}/output/{job_id}")
              if response.status_code == 200:
                  return response.content
              elif response.status_code == 404:
                  wait_time = retry_delay * (2**attempt)
                  print(
                      f"[datetime.now().strftime('%H:%M:%S')] Output not ready yet, retrying in {wait_time:.1f}s..."
                  )
                  time.sleep(wait_time)
                  continue
              else:
                  response.raise_for_status()
          except requests.RequestException as e:
              print(f"[datetime.now().strftime('%H:%M:%S')] Error getting output: {e}")
              if attempt == max_retries - 1:
                  raise
              time.sleep(retry_delay * (2**attempt))
  
      raise TimeoutError("Failed to get output after maximum retries")
  
  
  def test_saved_podcasts(base_url: str, job_id: str):
      """Test the saved podcasts endpoints"""
      print(
          f"\n[{datetime.now().strftime('%H:%M:%S')}] Testing saved podcasts endpoints..."
      )
  
      # Test 1: Get all saved podcasts
      print("\nTesting list all podcasts endpoint...")
      response = requests.get(f"{base_url}/saved_podcasts")
      assert response.status_code == 200, f"Failed to get saved podcasts: {response.text}"
      podcasts = response.json()["podcasts"]
      print(f"Found {len(podcasts)} saved podcasts")
  
      # Verify our new job_id is in the list
      job_ids = [podcast["job_id"] for podcast in podcasts]
      assert (
          job_id in job_ids
      ), f"Recently created job_id {job_id} not found in saved podcasts"
      print(f"Successfully found job_id {job_id} in saved podcasts list")
  
      # Test 2: Get specific podcast metadata
      print("\nTesting individual podcast metadata endpoint...")
      response = requests.get(f"{base_url}/saved_podcast/{job_id}/metadata")
      assert (
          response.status_code == 200
      ), f"Failed to get podcast metadata: {response.text}"
      metadata = response.json()
      print(f"Retrieved metadata for podcast: {metadata.get('filename', 'unknown')}")
      print(f"Metadata: {json.dumps(metadata, indent=2)}")
  
      # Test 3: Get specific podcast audio
      print("\nTesting individual podcast audio endpoint...")
      response = requests.get(f"{base_url}/saved_podcast/{job_id}/audio")
      assert response.status_code == 200, f"Failed to get podcast audio: {response.text}"
      audio_data = response.content
      print(f"Successfully retrieved audio data, size: {len(audio_data)} bytes")
  
  
  def test_api(base_url: str):
      # Define default voice mapping
      voice_mapping = {
          "speaker-1": "iP95p4xoKVk53GoZ742B",  # Example voice ID for speaker 1
          "speaker-2": "9BWtsMINqrJLrRacOk9x",  # Example voice ID for speaker 2
      }
  
      # API endpoint
      process_url = f"{base_url}/process_pdf"
  
      # Path to a sample PDF file for testing
      current_dir = os.path.dirname(os.path.abspath(__file__))
      samples_dir = os.path.join(current_dir, "samples")
  
      # Ensure samples directory exists
      if not os.path.exists(samples_dir):
          raise FileNotFoundError(f"Samples directory not found at {samples_dir}")
  
      sample_pdf_path = os.path.join(samples_dir, "PNP_Proof.pdf")
  
      # Ensure the sample PDF file exists
      assert os.path.exists(
          sample_pdf_path
      ), f"Sample PDF file not found at {sample_pdf_path}"
  
      # Prepare the payload
      transcription_params = {
          "name": "ishan-test",
          "duration": 5,
          "speaker_1_name": "Blackwell",
          "speaker_2_name": "Hopper",
          "model": "meta/llama-3.1-405b-instruct",
          "voice_mapping": voice_mapping,  # Add voice mapping
      }
  
      # Step 1: Submit the PDF file and get job ID
      print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Submitting PDF for processing...")
      print(f"Using voices: {voice_mapping}")
  
      with open(sample_pdf_path, "rb") as pdf_file:
          files = {"file": ("sample.pdf", pdf_file, "application/pdf")}
          response = requests.post(
              process_url,
              files=files,
              data={"transcription_params": json.dumps(transcription_params)},
          )
  
      assert (
          response.status_code == 202
      ), f"Expected status code 202, but got {response.status_code}"
      job_data = response.json()
      assert "job_id" in job_data, "Response missing job_id"
      job_id = job_data["job_id"]
      print(f"[{datetime.now().strftime('%H:%M:%S')}] Job ID received: {job_id}")
  
      # Step 2: Start monitoring status via WebSocket
      monitor = StatusMonitor(base_url, job_id)
      monitor.start()
  
      try:
          # Wait for TTS completion or timeout
          max_wait = 40 * 60  # 20 minutes in seconds
          if not monitor.tts_completed.wait(timeout=max_wait):
              raise TimeoutError(f"Test timed out after {max_wait} seconds")
  
          # If we get here, TTS completed successfully
          print(
              f"\n[{datetime.now().strftime('%H:%M:%S')}] TTS processing completed, retrieving audio file..."
          )
  
          # Get the final output with retry logic
          audio_content = get_output_with_retry(base_url, job_id)
  
          # Save the audio file
          output_path = os.path.join(current_dir, "output.mp3")
          with open(output_path, "wb") as f:
              f.write(audio_content)
          print(
              f"[{datetime.now().strftime('%H:%M:%S')}] Audio file saved as '{output_path}'"
          )
  
          # Test saved podcasts endpoints with the newly created job_id
          test_saved_podcasts(base_url, job_id)
  
      finally:
          monitor.stop()
  
  
  if __name__ == "__main__":
      base_url = os.getenv("API_SERVICE_URL", "http://localhost:8002")
      print(f"{base_url=}")
      test_api(base_url)
</file>
<file path="./test_db.py">
  import os
  import io
  import time
  from minio import Minio
  from datetime import datetime
  from dataclasses import dataclass
  from typing import Dict
  
  
  # Mock the TranscriptionParams that was in main.py
  @dataclass
  class TranscriptionParams:
      name: str
      duration: int
      speaker_1_name: str
      speaker_2_name: str
      model: str
      voice_mapping: Dict[str, str]
  
  
  class StorageManager:
      def __init__(self):
          """Initialize MinIO client and ensure bucket exists"""
          try:
              self.client = Minio(
                  os.getenv("MINIO_ENDPOINT", "localhost:9000"),
                  access_key=os.getenv("MINIO_ACCESS_KEY", "minioadmin"),
                  secret_key=os.getenv("MINIO_SECRET_KEY", "minioadmin"),
                  secure=os.getenv("MINIO_SECURE", "false").lower() == "true",
              )
  
              self.bucket_name = os.getenv("MINIO_BUCKET_NAME", "audio-results")
              self._ensure_bucket_exists()
              print(f"[{self.get_time()}] Successfully initialized MinIO storage")
  
          except Exception as e:
              print(f"[{self.get_time()}] Failed to initialize MinIO client: {e}")
              raise
  
      def get_time(self):
          return datetime.now().strftime("%H:%M:%S")
  
      def _ensure_bucket_exists(self):
          try:
              if not self.client.bucket_exists(self.bucket_name):
                  self.client.make_bucket(self.bucket_name)
                  print(f"[{self.get_time()}] Created bucket: {self.bucket_name}")
          except Exception as e:
              print(f"[{self.get_time()}] Failed to ensure bucket exists: {e}")
              raise
  
      def store_audio(
          self,
          job_id: str,
          audio_content: bytes,
          filename: str,
          transcription_params: TranscriptionParams,
      ):
          try:
              object_name = f"{job_id}/{filename}"
              self.client.put_object(
                  self.bucket_name,
                  object_name,
                  io.BytesIO(audio_content),
                  len(audio_content),
                  content_type="audio/mpeg",
              )
              print(
                  f"[{self.get_time()}] Stored audio for {job_id} in MinIO as {object_name}"
              )
              return True
          except Exception as e:
              print(f"[{self.get_time()}] Failed to store audio in MinIO: {e}")
              return False
  
      def get_audio(self, job_id: str, filename: str):
          try:
              object_name = f"{job_id}/{filename}"
              result = self.client.get_object(self.bucket_name, object_name).read()
              print(
                  f"[{self.get_time()}] Retrieved audio for {job_id} from MinIO as {object_name}"
              )
              return result
          except Exception as e:
              print(f"[{self.get_time()}] Failed to get audio from MinIO: {e}")
              return None
  
  
  def test_storage_manager():
      print("\n=== Starting Storage Manager Tests ===")
  
      # Initialize test data
      test_job_id = f"test_{int(time.time())}"
      test_audio_content = b"fake audio content for testing"
      test_filename = f"{test_job_id}.mp3"
      test_transcription_params = TranscriptionParams(
          name="Test Podcast",
          duration=5,
          speaker_1_name="John",
          speaker_2_name="Jane",
          model="test-model",
          voice_mapping={"speaker-1": "voice1", "speaker-2": "voice2"},
      )
  
      # Test 1: Initialize StorageManager
      print("\nTest 1: Initializing StorageManager")
      try:
          storage_manager = StorageManager()
          print("✓ StorageManager initialized successfully")
      except Exception as e:
          print(f"✗ Failed to initialize StorageManager: {e}")
          return
  
      # Test 2: Store audio file
      print("\nTest 2: Storing audio file")
      store_success = storage_manager.store_audio(
          test_job_id, test_audio_content, test_filename, test_transcription_params
      )
      if store_success:
          print("✓ Audio stored successfully")
      else:
          print("✗ Failed to store audio")
          return
  
      # Test 3: Retrieve audio file
      print("\nTest 3: Retrieving audio file")
      retrieved_content = storage_manager.get_audio(test_job_id, test_filename)
      if retrieved_content == test_audio_content:
          print("✓ Audio retrieved successfully and content matches")
      else:
          print("✗ Retrieved audio content doesn't match or retrieval failed")
  
      # Test 4: Try to retrieve non-existent file
      print("\nTest 4: Attempting to retrieve non-existent file")
      non_existent = storage_manager.get_audio("non_existent_job", "non_existent.mp3")
      if non_existent is None:
          print("✓ Properly handled non-existent file")
      else:
          print("✗ Unexpected behavior with non-existent file")
  
      # Cleanup
      print("\nCleaning up test data...")
      try:
          storage_manager.client.remove_object(
              storage_manager.bucket_name, f"{test_job_id}/{test_filename}"
          )
          print("✓ Test data cleaned up successfully")
      except Exception as e:
          print(f"✗ Failed to clean up test data: {e}")
  
      print("\n=== Storage Manager Tests Completed ===")
  
  
  if __name__ == "__main__":
      test_storage_manager()
</file>
<file path="./test_files.py">
  import requests
  import os
  from datetime import datetime
  import json
  
  # Set your job_id here
  JOB_ID = "1730947535"
  
  
  def test_transcript():
      base_url = os.getenv("API_SERVICE_URL", "http://localhost:8002")
      print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Testing transcript endpoint...")
      print(f"Job ID: {JOB_ID}")
  
      try:
          response = requests.get(f"{base_url}/saved_podcast/{JOB_ID}/transcript")
  
          if response.status_code == 200:
              transcript = response.json()
              print(
                  f"[{datetime.now().strftime('%H:%M:%S')}] Successfully retrieved transcript"
              )
              print("\nTranscript content:")
              print(json.dumps(transcript, indent=2))
          else:
              print(
                  f"[{datetime.now().strftime('%H:%M:%S')}] Error: {response.status_code}"
              )
              print(f"Response: {response.text}")
  
      except Exception as e:
          print(f"[{datetime.now().strftime('%H:%M:%S')}] Error: {str(e)}")
  
  
  def test_prompt_tracker():
      base_url = os.getenv("API_SERVICE_URL", "http://localhost:8002")
  
      print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Testing history endpoint...")
      print(f"Job ID: {JOB_ID}")
  
      try:
          response = requests.get(f"{base_url}/saved_podcast/{JOB_ID}/history")
  
          if response.status_code == 200:
              transcript = response.json()
              print(
                  f"[{datetime.now().strftime('%H:%M:%S')}] Successfully retrieved history"
              )
              print("\nHistory content:")
              print(json.dumps(transcript, indent=2))
          else:
              print(
                  f"[{datetime.now().strftime('%H:%M:%S')}] Error: {response.status_code}"
              )
              print(f"Response: {response.text}")
  
      except Exception as e:
          print(f"[{datetime.now().strftime('%H:%M:%S')}] Error: {str(e)}")
  
  
  if __name__ == "__main__":
      test_transcript()
      test_prompt_tracker()
</file>
<file path="./test_list.py">
  import requests
  import json
  from datetime import datetime
  
  
  def format_timestamp(timestamp_str):
      """Format the ISO timestamp into a more readable format"""
      dt = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
      return dt.strftime("%B %d, %Y at %I:%M %p")
  
  
  def list_saved_podcasts():
      try:
          print("\nAttempting to connect to API...")
          response = requests.get("http://localhost:8002/saved_podcasts")
          response.raise_for_status()
  
          data = response.json()
          podcasts = data.get("podcasts", [])
  
          if not podcasts:
              print("\nNo saved podcasts found in API response.")
  
          print("\nSaved Podcasts:")
          print("-" * 80)
  
          for podcast in podcasts:
              created_at = format_timestamp(podcast["created_at"])
              print(f"Job ID: {podcast['job_id']}")
              print(f"Filename: {podcast['filename']}")
              print(f"Created: {created_at}")
              print("Transcription Parameters:")
              print(json.dumps(podcast["transcription_params"], indent=2))
              print("-" * 80)
  
          print(f"\nTotal podcasts found: {len(podcasts)}")
  
      except requests.exceptions.ConnectionError:
          print("Error: Could not connect to the API service. Is it running?")
      except requests.exceptions.HTTPError as e:
          print(f"Error: API request failed with status {e.response.status_code}")
          print(f"Details: {e.response.text}")
      except Exception as e:
          print(f"Error: {str(e)}")
  
  
  if __name__ == "__main__":
      list_saved_podcasts()
</file>
</root>
